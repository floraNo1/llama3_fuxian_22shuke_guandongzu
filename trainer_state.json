{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 1000,
  "global_step": 135886,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007359109842073503,
      "grad_norm": 2.030687093734741,
      "learning_rate": 9.999996659355323e-05,
      "loss": 9.8544,
      "step": 50
    },
    {
      "epoch": 0.0014718219684147005,
      "grad_norm": 1.3308680057525635,
      "learning_rate": 9.99998663742576e-05,
      "loss": 7.2776,
      "step": 100
    },
    {
      "epoch": 0.0022077329526220507,
      "grad_norm": 1.2270647287368774,
      "learning_rate": 9.999969934224698e-05,
      "loss": 6.2055,
      "step": 150
    },
    {
      "epoch": 0.002943643936829401,
      "grad_norm": 1.3710778951644897,
      "learning_rate": 9.999946549774459e-05,
      "loss": 5.7264,
      "step": 200
    },
    {
      "epoch": 0.0036795549210367515,
      "grad_norm": 1.563071370124817,
      "learning_rate": 9.999916484106291e-05,
      "loss": 5.5577,
      "step": 250
    },
    {
      "epoch": 0.004415465905244101,
      "grad_norm": 1.6132413148880005,
      "learning_rate": 9.99987973726037e-05,
      "loss": 5.2684,
      "step": 300
    },
    {
      "epoch": 0.005151376889451452,
      "grad_norm": 1.7367316484451294,
      "learning_rate": 9.999836309285796e-05,
      "loss": 5.1474,
      "step": 350
    },
    {
      "epoch": 0.005887287873658802,
      "grad_norm": 1.6430103778839111,
      "learning_rate": 9.999786200240604e-05,
      "loss": 5.0165,
      "step": 400
    },
    {
      "epoch": 0.0066231988578661525,
      "grad_norm": 2.0601890087127686,
      "learning_rate": 9.999729410191751e-05,
      "loss": 4.8885,
      "step": 450
    },
    {
      "epoch": 0.007359109842073503,
      "grad_norm": 1.865774154663086,
      "learning_rate": 9.999665939215124e-05,
      "loss": 4.7441,
      "step": 500
    },
    {
      "epoch": 0.008095020826280852,
      "grad_norm": 1.6333801746368408,
      "learning_rate": 9.999595787395535e-05,
      "loss": 4.7528,
      "step": 550
    },
    {
      "epoch": 0.008830931810488203,
      "grad_norm": 1.7351347208023071,
      "learning_rate": 9.999518954826724e-05,
      "loss": 4.6268,
      "step": 600
    },
    {
      "epoch": 0.009566842794695553,
      "grad_norm": 1.7398818731307983,
      "learning_rate": 9.999435441611362e-05,
      "loss": 4.5611,
      "step": 650
    },
    {
      "epoch": 0.010302753778902904,
      "grad_norm": 1.649105191230774,
      "learning_rate": 9.999345247861044e-05,
      "loss": 4.4313,
      "step": 700
    },
    {
      "epoch": 0.011038664763110254,
      "grad_norm": 1.739915132522583,
      "learning_rate": 9.999248373696291e-05,
      "loss": 4.3369,
      "step": 750
    },
    {
      "epoch": 0.011774575747317604,
      "grad_norm": 1.7428399324417114,
      "learning_rate": 9.999144819246551e-05,
      "loss": 4.339,
      "step": 800
    },
    {
      "epoch": 0.012510486731524955,
      "grad_norm": 2.1205809116363525,
      "learning_rate": 9.9990345846502e-05,
      "loss": 4.2442,
      "step": 850
    },
    {
      "epoch": 0.013246397715732305,
      "grad_norm": 1.687832236289978,
      "learning_rate": 9.998917670054541e-05,
      "loss": 4.2491,
      "step": 900
    },
    {
      "epoch": 0.013982308699939655,
      "grad_norm": 2.004915952682495,
      "learning_rate": 9.998794075615801e-05,
      "loss": 4.2105,
      "step": 950
    },
    {
      "epoch": 0.014718219684147006,
      "grad_norm": 1.7757277488708496,
      "learning_rate": 9.998663801499136e-05,
      "loss": 4.139,
      "step": 1000
    },
    {
      "epoch": 0.015454130668354356,
      "grad_norm": 1.8725981712341309,
      "learning_rate": 9.998526847878623e-05,
      "loss": 4.0303,
      "step": 1050
    },
    {
      "epoch": 0.016190041652561705,
      "grad_norm": 1.8353245258331299,
      "learning_rate": 9.998383214937265e-05,
      "loss": 3.9748,
      "step": 1100
    },
    {
      "epoch": 0.016925952636769055,
      "grad_norm": 2.0543577671051025,
      "learning_rate": 9.998232902867001e-05,
      "loss": 4.029,
      "step": 1150
    },
    {
      "epoch": 0.017661863620976406,
      "grad_norm": 1.9172115325927734,
      "learning_rate": 9.99807591186868e-05,
      "loss": 3.9525,
      "step": 1200
    },
    {
      "epoch": 0.018397774605183756,
      "grad_norm": 2.005070209503174,
      "learning_rate": 9.997912242152084e-05,
      "loss": 3.9413,
      "step": 1250
    },
    {
      "epoch": 0.019133685589391106,
      "grad_norm": 1.9291568994522095,
      "learning_rate": 9.997741893935919e-05,
      "loss": 3.9602,
      "step": 1300
    },
    {
      "epoch": 0.019869596573598457,
      "grad_norm": 1.9517691135406494,
      "learning_rate": 9.997564867447812e-05,
      "loss": 3.8509,
      "step": 1350
    },
    {
      "epoch": 0.020605507557805807,
      "grad_norm": 2.073838949203491,
      "learning_rate": 9.99738116292432e-05,
      "loss": 3.8902,
      "step": 1400
    },
    {
      "epoch": 0.021341418542013157,
      "grad_norm": 1.695328712463379,
      "learning_rate": 9.997190780610914e-05,
      "loss": 3.8362,
      "step": 1450
    },
    {
      "epoch": 0.022077329526220508,
      "grad_norm": 2.022303342819214,
      "learning_rate": 9.996993720762e-05,
      "loss": 3.7686,
      "step": 1500
    },
    {
      "epoch": 0.022813240510427858,
      "grad_norm": 1.8455067873001099,
      "learning_rate": 9.996789983640896e-05,
      "loss": 3.7967,
      "step": 1550
    },
    {
      "epoch": 0.02354915149463521,
      "grad_norm": 1.9353591203689575,
      "learning_rate": 9.996579569519849e-05,
      "loss": 3.7123,
      "step": 1600
    },
    {
      "epoch": 0.02428506247884256,
      "grad_norm": 2.0369348526000977,
      "learning_rate": 9.996362478680026e-05,
      "loss": 3.747,
      "step": 1650
    },
    {
      "epoch": 0.02502097346304991,
      "grad_norm": 2.0512027740478516,
      "learning_rate": 9.996138711411518e-05,
      "loss": 3.6537,
      "step": 1700
    },
    {
      "epoch": 0.02575688444725726,
      "grad_norm": 2.3814380168914795,
      "learning_rate": 9.995908268013334e-05,
      "loss": 3.6935,
      "step": 1750
    },
    {
      "epoch": 0.02649279543146461,
      "grad_norm": 1.9439417123794556,
      "learning_rate": 9.995671148793407e-05,
      "loss": 3.6684,
      "step": 1800
    },
    {
      "epoch": 0.02722870641567196,
      "grad_norm": 2.1986422538757324,
      "learning_rate": 9.995427354068588e-05,
      "loss": 3.6021,
      "step": 1850
    },
    {
      "epoch": 0.02796461739987931,
      "grad_norm": 2.0502536296844482,
      "learning_rate": 9.995176884164652e-05,
      "loss": 3.636,
      "step": 1900
    },
    {
      "epoch": 0.02870052838408666,
      "grad_norm": 2.0228235721588135,
      "learning_rate": 9.994919739416288e-05,
      "loss": 3.5854,
      "step": 1950
    },
    {
      "epoch": 0.02943643936829401,
      "grad_norm": 2.1669297218322754,
      "learning_rate": 9.99465592016711e-05,
      "loss": 3.6041,
      "step": 2000
    },
    {
      "epoch": 0.030172350352501362,
      "grad_norm": 2.2931501865386963,
      "learning_rate": 9.994385426769649e-05,
      "loss": 3.6503,
      "step": 2050
    },
    {
      "epoch": 0.030908261336708712,
      "grad_norm": 1.872416377067566,
      "learning_rate": 9.994108259585353e-05,
      "loss": 3.5452,
      "step": 2100
    },
    {
      "epoch": 0.03164417232091606,
      "grad_norm": 1.811305284500122,
      "learning_rate": 9.993824418984589e-05,
      "loss": 3.5862,
      "step": 2150
    },
    {
      "epoch": 0.03238008330512341,
      "grad_norm": 1.973283052444458,
      "learning_rate": 9.99353390534664e-05,
      "loss": 3.4694,
      "step": 2200
    },
    {
      "epoch": 0.033115994289330764,
      "grad_norm": 2.03804349899292,
      "learning_rate": 9.993236719059709e-05,
      "loss": 3.4863,
      "step": 2250
    },
    {
      "epoch": 0.03385190527353811,
      "grad_norm": 2.1422414779663086,
      "learning_rate": 9.992932860520912e-05,
      "loss": 3.4521,
      "step": 2300
    },
    {
      "epoch": 0.034587816257745464,
      "grad_norm": 1.856428861618042,
      "learning_rate": 9.992622330136284e-05,
      "loss": 3.4443,
      "step": 2350
    },
    {
      "epoch": 0.03532372724195281,
      "grad_norm": 2.153130531311035,
      "learning_rate": 9.992305128320774e-05,
      "loss": 3.4077,
      "step": 2400
    },
    {
      "epoch": 0.036059638226160165,
      "grad_norm": 2.165531873703003,
      "learning_rate": 9.991981255498242e-05,
      "loss": 3.4664,
      "step": 2450
    },
    {
      "epoch": 0.03679554921036751,
      "grad_norm": 2.2679831981658936,
      "learning_rate": 9.991650712101468e-05,
      "loss": 3.4037,
      "step": 2500
    },
    {
      "epoch": 0.037531460194574866,
      "grad_norm": 2.219994306564331,
      "learning_rate": 9.991313498572143e-05,
      "loss": 3.4757,
      "step": 2550
    },
    {
      "epoch": 0.03826737117878221,
      "grad_norm": 2.049311876296997,
      "learning_rate": 9.990969615360874e-05,
      "loss": 3.3607,
      "step": 2600
    },
    {
      "epoch": 0.039003282162989567,
      "grad_norm": 2.1203513145446777,
      "learning_rate": 9.990619062927173e-05,
      "loss": 3.3451,
      "step": 2650
    },
    {
      "epoch": 0.03973919314719691,
      "grad_norm": 1.8800729513168335,
      "learning_rate": 9.99026184173947e-05,
      "loss": 3.4134,
      "step": 2700
    },
    {
      "epoch": 0.04047510413140427,
      "grad_norm": 1.813874363899231,
      "learning_rate": 9.989897952275105e-05,
      "loss": 3.3676,
      "step": 2750
    },
    {
      "epoch": 0.041211015115611614,
      "grad_norm": 2.0658557415008545,
      "learning_rate": 9.989527395020328e-05,
      "loss": 3.4124,
      "step": 2800
    },
    {
      "epoch": 0.04194692609981897,
      "grad_norm": 1.931026577949524,
      "learning_rate": 9.9891501704703e-05,
      "loss": 3.3219,
      "step": 2850
    },
    {
      "epoch": 0.042682837084026315,
      "grad_norm": 2.108513116836548,
      "learning_rate": 9.988766279129089e-05,
      "loss": 3.3319,
      "step": 2900
    },
    {
      "epoch": 0.04341874806823367,
      "grad_norm": 1.9939076900482178,
      "learning_rate": 9.988375721509674e-05,
      "loss": 3.2747,
      "step": 2950
    },
    {
      "epoch": 0.044154659052441016,
      "grad_norm": 2.045078754425049,
      "learning_rate": 9.98797849813394e-05,
      "loss": 3.3012,
      "step": 3000
    },
    {
      "epoch": 0.04489057003664837,
      "grad_norm": 2.4530141353607178,
      "learning_rate": 9.98757460953268e-05,
      "loss": 3.3024,
      "step": 3050
    },
    {
      "epoch": 0.045626481020855716,
      "grad_norm": 2.1416563987731934,
      "learning_rate": 9.987164056245591e-05,
      "loss": 3.3035,
      "step": 3100
    },
    {
      "epoch": 0.04636239200506307,
      "grad_norm": 2.064828395843506,
      "learning_rate": 9.986746838821282e-05,
      "loss": 3.2931,
      "step": 3150
    },
    {
      "epoch": 0.04709830298927042,
      "grad_norm": 2.2253403663635254,
      "learning_rate": 9.986322957817261e-05,
      "loss": 3.2347,
      "step": 3200
    },
    {
      "epoch": 0.04783421397347777,
      "grad_norm": 2.1374869346618652,
      "learning_rate": 9.985892413799943e-05,
      "loss": 3.2819,
      "step": 3250
    },
    {
      "epoch": 0.04857012495768512,
      "grad_norm": 1.901450753211975,
      "learning_rate": 9.985455207344644e-05,
      "loss": 3.2046,
      "step": 3300
    },
    {
      "epoch": 0.04930603594189247,
      "grad_norm": 1.9872304201126099,
      "learning_rate": 9.985011339035588e-05,
      "loss": 3.2026,
      "step": 3350
    },
    {
      "epoch": 0.05004194692609982,
      "grad_norm": 2.616241931915283,
      "learning_rate": 9.984560809465895e-05,
      "loss": 3.239,
      "step": 3400
    },
    {
      "epoch": 0.05077785791030717,
      "grad_norm": 2.3874261379241943,
      "learning_rate": 9.98410361923759e-05,
      "loss": 3.2361,
      "step": 3450
    },
    {
      "epoch": 0.05151376889451452,
      "grad_norm": 2.1656622886657715,
      "learning_rate": 9.983639768961595e-05,
      "loss": 3.2144,
      "step": 3500
    },
    {
      "epoch": 0.052249679878721866,
      "grad_norm": 2.147057056427002,
      "learning_rate": 9.983169259257736e-05,
      "loss": 3.2925,
      "step": 3550
    },
    {
      "epoch": 0.05298559086292922,
      "grad_norm": 2.3573217391967773,
      "learning_rate": 9.982692090754732e-05,
      "loss": 3.1056,
      "step": 3600
    },
    {
      "epoch": 0.05372150184713657,
      "grad_norm": 2.5163559913635254,
      "learning_rate": 9.982208264090208e-05,
      "loss": 3.1987,
      "step": 3650
    },
    {
      "epoch": 0.05445741283134392,
      "grad_norm": 2.0788516998291016,
      "learning_rate": 9.981717779910677e-05,
      "loss": 3.1015,
      "step": 3700
    },
    {
      "epoch": 0.05519332381555127,
      "grad_norm": 1.9660533666610718,
      "learning_rate": 9.981220638871555e-05,
      "loss": 3.1662,
      "step": 3750
    },
    {
      "epoch": 0.05592923479975862,
      "grad_norm": 2.346238136291504,
      "learning_rate": 9.980716841637148e-05,
      "loss": 3.1191,
      "step": 3800
    },
    {
      "epoch": 0.05666514578396597,
      "grad_norm": 2.0285370349884033,
      "learning_rate": 9.980206388880662e-05,
      "loss": 3.1256,
      "step": 3850
    },
    {
      "epoch": 0.05740105676817332,
      "grad_norm": 1.9999542236328125,
      "learning_rate": 9.97968928128419e-05,
      "loss": 3.1462,
      "step": 3900
    },
    {
      "epoch": 0.05813696775238067,
      "grad_norm": 2.4738082885742188,
      "learning_rate": 9.979165519538725e-05,
      "loss": 3.0598,
      "step": 3950
    },
    {
      "epoch": 0.05887287873658802,
      "grad_norm": 2.084198236465454,
      "learning_rate": 9.978635104344145e-05,
      "loss": 3.1748,
      "step": 4000
    },
    {
      "epoch": 0.05960878972079537,
      "grad_norm": 2.0425496101379395,
      "learning_rate": 9.978098036409223e-05,
      "loss": 3.0954,
      "step": 4050
    },
    {
      "epoch": 0.060344700705002724,
      "grad_norm": 2.204209089279175,
      "learning_rate": 9.977554316451619e-05,
      "loss": 3.028,
      "step": 4100
    },
    {
      "epoch": 0.06108061168921007,
      "grad_norm": 1.911677598953247,
      "learning_rate": 9.977003945197885e-05,
      "loss": 3.078,
      "step": 4150
    },
    {
      "epoch": 0.061816522673417425,
      "grad_norm": 1.9878299236297607,
      "learning_rate": 9.976446923383456e-05,
      "loss": 3.0423,
      "step": 4200
    },
    {
      "epoch": 0.06255243365762478,
      "grad_norm": 1.9096354246139526,
      "learning_rate": 9.975883251752659e-05,
      "loss": 3.1137,
      "step": 4250
    },
    {
      "epoch": 0.06328834464183213,
      "grad_norm": 1.9014776945114136,
      "learning_rate": 9.975312931058704e-05,
      "loss": 3.0754,
      "step": 4300
    },
    {
      "epoch": 0.06402425562603947,
      "grad_norm": 1.9899991750717163,
      "learning_rate": 9.974735962063686e-05,
      "loss": 3.0864,
      "step": 4350
    },
    {
      "epoch": 0.06476016661024682,
      "grad_norm": 2.1520755290985107,
      "learning_rate": 9.974152345538585e-05,
      "loss": 3.0925,
      "step": 4400
    },
    {
      "epoch": 0.06549607759445418,
      "grad_norm": 2.0232841968536377,
      "learning_rate": 9.973562082263263e-05,
      "loss": 3.0476,
      "step": 4450
    },
    {
      "epoch": 0.06623198857866153,
      "grad_norm": 1.9410042762756348,
      "learning_rate": 9.972965173026465e-05,
      "loss": 3.0305,
      "step": 4500
    },
    {
      "epoch": 0.06696789956286887,
      "grad_norm": 2.072033643722534,
      "learning_rate": 9.972361618625814e-05,
      "loss": 3.1002,
      "step": 4550
    },
    {
      "epoch": 0.06770381054707622,
      "grad_norm": 1.9731203317642212,
      "learning_rate": 9.971751419867816e-05,
      "loss": 3.0229,
      "step": 4600
    },
    {
      "epoch": 0.06843972153128358,
      "grad_norm": 2.0399680137634277,
      "learning_rate": 9.971134577567852e-05,
      "loss": 3.0428,
      "step": 4650
    },
    {
      "epoch": 0.06917563251549093,
      "grad_norm": 2.243001699447632,
      "learning_rate": 9.970511092550184e-05,
      "loss": 3.0396,
      "step": 4700
    },
    {
      "epoch": 0.06991154349969828,
      "grad_norm": 2.621903657913208,
      "learning_rate": 9.969880965647947e-05,
      "loss": 3.033,
      "step": 4750
    },
    {
      "epoch": 0.07064745448390562,
      "grad_norm": 2.228790521621704,
      "learning_rate": 9.969244197703155e-05,
      "loss": 3.1335,
      "step": 4800
    },
    {
      "epoch": 0.07138336546811298,
      "grad_norm": 2.141184091567993,
      "learning_rate": 9.968600789566693e-05,
      "loss": 2.9144,
      "step": 4850
    },
    {
      "epoch": 0.07211927645232033,
      "grad_norm": 2.0902392864227295,
      "learning_rate": 9.96795074209832e-05,
      "loss": 2.9358,
      "step": 4900
    },
    {
      "epoch": 0.07285518743652768,
      "grad_norm": 2.158874988555908,
      "learning_rate": 9.967294056166667e-05,
      "loss": 3.0179,
      "step": 4950
    },
    {
      "epoch": 0.07359109842073502,
      "grad_norm": 2.2592053413391113,
      "learning_rate": 9.966630732649238e-05,
      "loss": 2.9738,
      "step": 5000
    },
    {
      "epoch": 0.07432700940494238,
      "grad_norm": 1.964558720588684,
      "learning_rate": 9.9659607724324e-05,
      "loss": 2.9138,
      "step": 5050
    },
    {
      "epoch": 0.07506292038914973,
      "grad_norm": 2.1676418781280518,
      "learning_rate": 9.965284176411398e-05,
      "loss": 2.9719,
      "step": 5100
    },
    {
      "epoch": 0.07579883137335708,
      "grad_norm": 2.349497079849243,
      "learning_rate": 9.964600945490335e-05,
      "loss": 2.9045,
      "step": 5150
    },
    {
      "epoch": 0.07653474235756443,
      "grad_norm": 2.239492893218994,
      "learning_rate": 9.963911080582185e-05,
      "loss": 2.9529,
      "step": 5200
    },
    {
      "epoch": 0.07727065334177179,
      "grad_norm": 2.3331058025360107,
      "learning_rate": 9.963214582608784e-05,
      "loss": 2.9717,
      "step": 5250
    },
    {
      "epoch": 0.07800656432597913,
      "grad_norm": 2.183743715286255,
      "learning_rate": 9.962511452500833e-05,
      "loss": 2.8712,
      "step": 5300
    },
    {
      "epoch": 0.07874247531018648,
      "grad_norm": 1.8739348649978638,
      "learning_rate": 9.961801691197899e-05,
      "loss": 2.9796,
      "step": 5350
    },
    {
      "epoch": 0.07947838629439383,
      "grad_norm": 1.981638789176941,
      "learning_rate": 9.961085299648401e-05,
      "loss": 2.974,
      "step": 5400
    },
    {
      "epoch": 0.08021429727860117,
      "grad_norm": 2.249264717102051,
      "learning_rate": 9.960362278809624e-05,
      "loss": 2.9062,
      "step": 5450
    },
    {
      "epoch": 0.08095020826280853,
      "grad_norm": 2.246137857437134,
      "learning_rate": 9.959632629647713e-05,
      "loss": 2.9397,
      "step": 5500
    },
    {
      "epoch": 0.08168611924701588,
      "grad_norm": 2.0307323932647705,
      "learning_rate": 9.958896353137666e-05,
      "loss": 2.8957,
      "step": 5550
    },
    {
      "epoch": 0.08242203023122323,
      "grad_norm": 2.1162686347961426,
      "learning_rate": 9.958153450263336e-05,
      "loss": 2.9484,
      "step": 5600
    },
    {
      "epoch": 0.08315794121543058,
      "grad_norm": 2.143101453781128,
      "learning_rate": 9.957403922017437e-05,
      "loss": 2.8575,
      "step": 5650
    },
    {
      "epoch": 0.08389385219963794,
      "grad_norm": 2.0167171955108643,
      "learning_rate": 9.956647769401529e-05,
      "loss": 2.8759,
      "step": 5700
    },
    {
      "epoch": 0.08462976318384528,
      "grad_norm": 2.2200679779052734,
      "learning_rate": 9.955884993426029e-05,
      "loss": 2.9178,
      "step": 5750
    },
    {
      "epoch": 0.08536567416805263,
      "grad_norm": 2.3862507343292236,
      "learning_rate": 9.955115595110198e-05,
      "loss": 2.8615,
      "step": 5800
    },
    {
      "epoch": 0.08610158515225998,
      "grad_norm": 2.1227731704711914,
      "learning_rate": 9.954339575482157e-05,
      "loss": 2.8569,
      "step": 5850
    },
    {
      "epoch": 0.08683749613646734,
      "grad_norm": 1.9997600317001343,
      "learning_rate": 9.953556935578863e-05,
      "loss": 2.8898,
      "step": 5900
    },
    {
      "epoch": 0.08757340712067468,
      "grad_norm": 2.1594722270965576,
      "learning_rate": 9.952767676446127e-05,
      "loss": 2.8933,
      "step": 5950
    },
    {
      "epoch": 0.08830931810488203,
      "grad_norm": 2.367262125015259,
      "learning_rate": 9.951971799138603e-05,
      "loss": 2.8861,
      "step": 6000
    },
    {
      "epoch": 0.08904522908908938,
      "grad_norm": 1.9831160306930542,
      "learning_rate": 9.951169304719787e-05,
      "loss": 2.8394,
      "step": 6050
    },
    {
      "epoch": 0.08978114007329674,
      "grad_norm": 2.1008236408233643,
      "learning_rate": 9.95036019426202e-05,
      "loss": 2.847,
      "step": 6100
    },
    {
      "epoch": 0.09051705105750409,
      "grad_norm": 2.238163709640503,
      "learning_rate": 9.949544468846481e-05,
      "loss": 2.8938,
      "step": 6150
    },
    {
      "epoch": 0.09125296204171143,
      "grad_norm": 2.0242702960968018,
      "learning_rate": 9.94872212956319e-05,
      "loss": 2.7932,
      "step": 6200
    },
    {
      "epoch": 0.09198887302591878,
      "grad_norm": 2.1242048740386963,
      "learning_rate": 9.947893177511006e-05,
      "loss": 2.8124,
      "step": 6250
    },
    {
      "epoch": 0.09272478401012614,
      "grad_norm": 2.3358404636383057,
      "learning_rate": 9.947057613797619e-05,
      "loss": 2.8172,
      "step": 6300
    },
    {
      "epoch": 0.09346069499433349,
      "grad_norm": 2.4237542152404785,
      "learning_rate": 9.946215439539562e-05,
      "loss": 2.8072,
      "step": 6350
    },
    {
      "epoch": 0.09419660597854083,
      "grad_norm": 2.1222004890441895,
      "learning_rate": 9.945366655862192e-05,
      "loss": 2.8031,
      "step": 6400
    },
    {
      "epoch": 0.09493251696274818,
      "grad_norm": 2.2314016819000244,
      "learning_rate": 9.944511263899707e-05,
      "loss": 2.8134,
      "step": 6450
    },
    {
      "epoch": 0.09566842794695554,
      "grad_norm": 2.0744128227233887,
      "learning_rate": 9.943649264795128e-05,
      "loss": 2.8257,
      "step": 6500
    },
    {
      "epoch": 0.09640433893116289,
      "grad_norm": 2.0211784839630127,
      "learning_rate": 9.942780659700312e-05,
      "loss": 2.8438,
      "step": 6550
    },
    {
      "epoch": 0.09714024991537024,
      "grad_norm": 2.2878692150115967,
      "learning_rate": 9.941905449775935e-05,
      "loss": 2.8388,
      "step": 6600
    },
    {
      "epoch": 0.09787616089957758,
      "grad_norm": 2.2476389408111572,
      "learning_rate": 9.941023636191506e-05,
      "loss": 2.7873,
      "step": 6650
    },
    {
      "epoch": 0.09861207188378494,
      "grad_norm": 2.363529682159424,
      "learning_rate": 9.940135220125356e-05,
      "loss": 2.8742,
      "step": 6700
    },
    {
      "epoch": 0.09934798286799229,
      "grad_norm": 2.429748773574829,
      "learning_rate": 9.939240202764638e-05,
      "loss": 2.7214,
      "step": 6750
    },
    {
      "epoch": 0.10008389385219964,
      "grad_norm": 2.2395758628845215,
      "learning_rate": 9.93833858530532e-05,
      "loss": 2.7956,
      "step": 6800
    },
    {
      "epoch": 0.10081980483640698,
      "grad_norm": 1.9813742637634277,
      "learning_rate": 9.937430368952203e-05,
      "loss": 2.8069,
      "step": 6850
    },
    {
      "epoch": 0.10155571582061435,
      "grad_norm": 1.902079463005066,
      "learning_rate": 9.936515554918895e-05,
      "loss": 2.8523,
      "step": 6900
    },
    {
      "epoch": 0.10229162680482169,
      "grad_norm": 2.0801773071289062,
      "learning_rate": 9.935594144427825e-05,
      "loss": 2.7575,
      "step": 6950
    },
    {
      "epoch": 0.10302753778902904,
      "grad_norm": 2.4343223571777344,
      "learning_rate": 9.934666138710232e-05,
      "loss": 2.7372,
      "step": 7000
    },
    {
      "epoch": 0.10376344877323639,
      "grad_norm": 2.248051404953003,
      "learning_rate": 9.933731539006172e-05,
      "loss": 2.7991,
      "step": 7050
    },
    {
      "epoch": 0.10449935975744373,
      "grad_norm": 2.1155319213867188,
      "learning_rate": 9.932790346564514e-05,
      "loss": 2.8185,
      "step": 7100
    },
    {
      "epoch": 0.1052352707416511,
      "grad_norm": 1.7084033489227295,
      "learning_rate": 9.931842562642931e-05,
      "loss": 2.8491,
      "step": 7150
    },
    {
      "epoch": 0.10597118172585844,
      "grad_norm": 1.8651440143585205,
      "learning_rate": 9.930888188507907e-05,
      "loss": 2.8418,
      "step": 7200
    },
    {
      "epoch": 0.10670709271006579,
      "grad_norm": 2.331019401550293,
      "learning_rate": 9.929927225434735e-05,
      "loss": 2.7906,
      "step": 7250
    },
    {
      "epoch": 0.10744300369427313,
      "grad_norm": 2.226046323776245,
      "learning_rate": 9.928959674707503e-05,
      "loss": 2.7704,
      "step": 7300
    },
    {
      "epoch": 0.1081789146784805,
      "grad_norm": 2.1756069660186768,
      "learning_rate": 9.927985537619115e-05,
      "loss": 2.7825,
      "step": 7350
    },
    {
      "epoch": 0.10891482566268784,
      "grad_norm": 2.162276029586792,
      "learning_rate": 9.927004815471266e-05,
      "loss": 2.7499,
      "step": 7400
    },
    {
      "epoch": 0.10965073664689519,
      "grad_norm": 2.237859010696411,
      "learning_rate": 9.926017509574453e-05,
      "loss": 2.7773,
      "step": 7450
    },
    {
      "epoch": 0.11038664763110254,
      "grad_norm": 1.9262945652008057,
      "learning_rate": 9.925023621247972e-05,
      "loss": 2.7525,
      "step": 7500
    },
    {
      "epoch": 0.1111225586153099,
      "grad_norm": 2.20048189163208,
      "learning_rate": 9.924023151819916e-05,
      "loss": 2.7403,
      "step": 7550
    },
    {
      "epoch": 0.11185846959951724,
      "grad_norm": 2.2050626277923584,
      "learning_rate": 9.92301610262717e-05,
      "loss": 2.7609,
      "step": 7600
    },
    {
      "epoch": 0.11259438058372459,
      "grad_norm": 2.0041754245758057,
      "learning_rate": 9.922002475015409e-05,
      "loss": 2.7773,
      "step": 7650
    },
    {
      "epoch": 0.11333029156793194,
      "grad_norm": 2.4190797805786133,
      "learning_rate": 9.920982270339101e-05,
      "loss": 2.7769,
      "step": 7700
    },
    {
      "epoch": 0.1140662025521393,
      "grad_norm": 2.2669565677642822,
      "learning_rate": 9.919955489961505e-05,
      "loss": 2.7309,
      "step": 7750
    },
    {
      "epoch": 0.11480211353634664,
      "grad_norm": 2.2925922870635986,
      "learning_rate": 9.918922135254662e-05,
      "loss": 2.7628,
      "step": 7800
    },
    {
      "epoch": 0.11553802452055399,
      "grad_norm": 2.2632391452789307,
      "learning_rate": 9.917882207599402e-05,
      "loss": 2.7348,
      "step": 7850
    },
    {
      "epoch": 0.11627393550476134,
      "grad_norm": 1.8888740539550781,
      "learning_rate": 9.916835708385334e-05,
      "loss": 2.7207,
      "step": 7900
    },
    {
      "epoch": 0.1170098464889687,
      "grad_norm": 2.167111396789551,
      "learning_rate": 9.915782639010855e-05,
      "loss": 2.7878,
      "step": 7950
    },
    {
      "epoch": 0.11774575747317605,
      "grad_norm": 2.319821834564209,
      "learning_rate": 9.914723000883134e-05,
      "loss": 2.6782,
      "step": 8000
    },
    {
      "epoch": 0.1184816684573834,
      "grad_norm": 2.148646593093872,
      "learning_rate": 9.91365679541812e-05,
      "loss": 2.7459,
      "step": 8050
    },
    {
      "epoch": 0.11921757944159074,
      "grad_norm": 2.037787914276123,
      "learning_rate": 9.912584024040543e-05,
      "loss": 2.6752,
      "step": 8100
    },
    {
      "epoch": 0.1199534904257981,
      "grad_norm": 2.4377365112304688,
      "learning_rate": 9.911504688183896e-05,
      "loss": 2.742,
      "step": 8150
    },
    {
      "epoch": 0.12068940141000545,
      "grad_norm": 2.3038251399993896,
      "learning_rate": 9.910418789290457e-05,
      "loss": 2.7454,
      "step": 8200
    },
    {
      "epoch": 0.1214253123942128,
      "grad_norm": 2.266226291656494,
      "learning_rate": 9.909326328811262e-05,
      "loss": 2.7667,
      "step": 8250
    },
    {
      "epoch": 0.12216122337842014,
      "grad_norm": 2.04105544090271,
      "learning_rate": 9.90822730820612e-05,
      "loss": 2.6748,
      "step": 8300
    },
    {
      "epoch": 0.1228971343626275,
      "grad_norm": 2.0787484645843506,
      "learning_rate": 9.907121728943607e-05,
      "loss": 2.7293,
      "step": 8350
    },
    {
      "epoch": 0.12363304534683485,
      "grad_norm": 1.960984468460083,
      "learning_rate": 9.906009592501061e-05,
      "loss": 2.6506,
      "step": 8400
    },
    {
      "epoch": 0.1243689563310422,
      "grad_norm": 2.3280441761016846,
      "learning_rate": 9.904890900364588e-05,
      "loss": 2.7351,
      "step": 8450
    },
    {
      "epoch": 0.12510486731524956,
      "grad_norm": 1.7413520812988281,
      "learning_rate": 9.903765654029043e-05,
      "loss": 2.6826,
      "step": 8500
    },
    {
      "epoch": 0.1258407782994569,
      "grad_norm": 2.228419065475464,
      "learning_rate": 9.902633854998047e-05,
      "loss": 2.7656,
      "step": 8550
    },
    {
      "epoch": 0.12657668928366425,
      "grad_norm": 2.302335023880005,
      "learning_rate": 9.901495504783977e-05,
      "loss": 2.6466,
      "step": 8600
    },
    {
      "epoch": 0.1273126002678716,
      "grad_norm": 2.012664318084717,
      "learning_rate": 9.900350604907961e-05,
      "loss": 2.777,
      "step": 8650
    },
    {
      "epoch": 0.12804851125207894,
      "grad_norm": 2.119591236114502,
      "learning_rate": 9.899199156899881e-05,
      "loss": 2.6936,
      "step": 8700
    },
    {
      "epoch": 0.1287844222362863,
      "grad_norm": 1.7936408519744873,
      "learning_rate": 9.898041162298368e-05,
      "loss": 2.7207,
      "step": 8750
    },
    {
      "epoch": 0.12952033322049364,
      "grad_norm": 2.244730234146118,
      "learning_rate": 9.896876622650802e-05,
      "loss": 2.7261,
      "step": 8800
    },
    {
      "epoch": 0.130256244204701,
      "grad_norm": 2.1436171531677246,
      "learning_rate": 9.895705539513308e-05,
      "loss": 2.6786,
      "step": 8850
    },
    {
      "epoch": 0.13099215518890836,
      "grad_norm": 2.499814510345459,
      "learning_rate": 9.894527914450755e-05,
      "loss": 2.8091,
      "step": 8900
    },
    {
      "epoch": 0.1317280661731157,
      "grad_norm": 1.927696943283081,
      "learning_rate": 9.893343749036754e-05,
      "loss": 2.6165,
      "step": 8950
    },
    {
      "epoch": 0.13246397715732305,
      "grad_norm": 2.2184500694274902,
      "learning_rate": 9.892153044853656e-05,
      "loss": 2.6641,
      "step": 9000
    },
    {
      "epoch": 0.1331998881415304,
      "grad_norm": 2.4713048934936523,
      "learning_rate": 9.890955803492546e-05,
      "loss": 2.6685,
      "step": 9050
    },
    {
      "epoch": 0.13393579912573775,
      "grad_norm": 2.138733148574829,
      "learning_rate": 9.88975202655325e-05,
      "loss": 2.6901,
      "step": 9100
    },
    {
      "epoch": 0.1346717101099451,
      "grad_norm": 2.1230857372283936,
      "learning_rate": 9.888541715644324e-05,
      "loss": 2.6226,
      "step": 9150
    },
    {
      "epoch": 0.13540762109415244,
      "grad_norm": 2.2459876537323,
      "learning_rate": 9.887324872383055e-05,
      "loss": 2.6069,
      "step": 9200
    },
    {
      "epoch": 0.1361435320783598,
      "grad_norm": 1.998685598373413,
      "learning_rate": 9.88610149839546e-05,
      "loss": 2.6512,
      "step": 9250
    },
    {
      "epoch": 0.13687944306256716,
      "grad_norm": 1.9613808393478394,
      "learning_rate": 9.88487159531628e-05,
      "loss": 2.6299,
      "step": 9300
    },
    {
      "epoch": 0.1376153540467745,
      "grad_norm": 1.9894448518753052,
      "learning_rate": 9.883635164788987e-05,
      "loss": 2.5914,
      "step": 9350
    },
    {
      "epoch": 0.13835126503098186,
      "grad_norm": 2.2852420806884766,
      "learning_rate": 9.882392208465766e-05,
      "loss": 2.6179,
      "step": 9400
    },
    {
      "epoch": 0.1390871760151892,
      "grad_norm": 1.8932706117630005,
      "learning_rate": 9.881142728007531e-05,
      "loss": 2.5908,
      "step": 9450
    },
    {
      "epoch": 0.13982308699939655,
      "grad_norm": 2.2291197776794434,
      "learning_rate": 9.879886725083907e-05,
      "loss": 2.621,
      "step": 9500
    },
    {
      "epoch": 0.1405589979836039,
      "grad_norm": 2.1482176780700684,
      "learning_rate": 9.87862420137324e-05,
      "loss": 2.6795,
      "step": 9550
    },
    {
      "epoch": 0.14129490896781124,
      "grad_norm": 1.9788559675216675,
      "learning_rate": 9.877355158562587e-05,
      "loss": 2.6689,
      "step": 9600
    },
    {
      "epoch": 0.1420308199520186,
      "grad_norm": 2.1313540935516357,
      "learning_rate": 9.876079598347715e-05,
      "loss": 2.5904,
      "step": 9650
    },
    {
      "epoch": 0.14276673093622597,
      "grad_norm": 1.968763828277588,
      "learning_rate": 9.874797522433104e-05,
      "loss": 2.6185,
      "step": 9700
    },
    {
      "epoch": 0.1435026419204333,
      "grad_norm": 2.113542318344116,
      "learning_rate": 9.873508932531934e-05,
      "loss": 2.6698,
      "step": 9750
    },
    {
      "epoch": 0.14423855290464066,
      "grad_norm": 2.1079189777374268,
      "learning_rate": 9.872213830366097e-05,
      "loss": 2.5346,
      "step": 9800
    },
    {
      "epoch": 0.144974463888848,
      "grad_norm": 2.1263439655303955,
      "learning_rate": 9.870912217666183e-05,
      "loss": 2.5961,
      "step": 9850
    },
    {
      "epoch": 0.14571037487305535,
      "grad_norm": 2.308903455734253,
      "learning_rate": 9.86960409617148e-05,
      "loss": 2.6341,
      "step": 9900
    },
    {
      "epoch": 0.14644628585726271,
      "grad_norm": 2.2841131687164307,
      "learning_rate": 9.86828946762998e-05,
      "loss": 2.6305,
      "step": 9950
    },
    {
      "epoch": 0.14718219684147005,
      "grad_norm": 1.8938578367233276,
      "learning_rate": 9.866968333798361e-05,
      "loss": 2.5686,
      "step": 10000
    },
    {
      "epoch": 0.1479181078256774,
      "grad_norm": 2.3480777740478516,
      "learning_rate": 9.865640696442001e-05,
      "loss": 2.6451,
      "step": 10050
    },
    {
      "epoch": 0.14865401880988477,
      "grad_norm": 1.7300447225570679,
      "learning_rate": 9.864306557334963e-05,
      "loss": 2.6145,
      "step": 10100
    },
    {
      "epoch": 0.1493899297940921,
      "grad_norm": 1.9362109899520874,
      "learning_rate": 9.862965918260006e-05,
      "loss": 2.6138,
      "step": 10150
    },
    {
      "epoch": 0.15012584077829946,
      "grad_norm": 2.0200788974761963,
      "learning_rate": 9.861618781008566e-05,
      "loss": 2.6597,
      "step": 10200
    },
    {
      "epoch": 0.1508617517625068,
      "grad_norm": 1.987565517425537,
      "learning_rate": 9.860265147380766e-05,
      "loss": 2.625,
      "step": 10250
    },
    {
      "epoch": 0.15159766274671416,
      "grad_norm": 2.1998281478881836,
      "learning_rate": 9.858905019185409e-05,
      "loss": 2.6081,
      "step": 10300
    },
    {
      "epoch": 0.15233357373092152,
      "grad_norm": 2.460217237472534,
      "learning_rate": 9.857538398239979e-05,
      "loss": 2.6509,
      "step": 10350
    },
    {
      "epoch": 0.15306948471512885,
      "grad_norm": 2.3080506324768066,
      "learning_rate": 9.856165286370633e-05,
      "loss": 2.5898,
      "step": 10400
    },
    {
      "epoch": 0.1538053956993362,
      "grad_norm": 2.2878057956695557,
      "learning_rate": 9.854785685412203e-05,
      "loss": 2.6455,
      "step": 10450
    },
    {
      "epoch": 0.15454130668354357,
      "grad_norm": 1.8250796794891357,
      "learning_rate": 9.853399597208191e-05,
      "loss": 2.6519,
      "step": 10500
    },
    {
      "epoch": 0.1552772176677509,
      "grad_norm": 2.1706695556640625,
      "learning_rate": 9.852007023610767e-05,
      "loss": 2.6101,
      "step": 10550
    },
    {
      "epoch": 0.15601312865195827,
      "grad_norm": 2.0135998725891113,
      "learning_rate": 9.850607966480772e-05,
      "loss": 2.5696,
      "step": 10600
    },
    {
      "epoch": 0.1567490396361656,
      "grad_norm": 2.449960708618164,
      "learning_rate": 9.849202427687702e-05,
      "loss": 2.4569,
      "step": 10650
    },
    {
      "epoch": 0.15748495062037296,
      "grad_norm": 2.7713987827301025,
      "learning_rate": 9.847790409109725e-05,
      "loss": 2.5525,
      "step": 10700
    },
    {
      "epoch": 0.15822086160458032,
      "grad_norm": 2.295839309692383,
      "learning_rate": 9.846371912633659e-05,
      "loss": 2.5836,
      "step": 10750
    },
    {
      "epoch": 0.15895677258878765,
      "grad_norm": 1.9887807369232178,
      "learning_rate": 9.84494694015498e-05,
      "loss": 2.5851,
      "step": 10800
    },
    {
      "epoch": 0.15969268357299501,
      "grad_norm": 2.093607187271118,
      "learning_rate": 9.843515493577821e-05,
      "loss": 2.5573,
      "step": 10850
    },
    {
      "epoch": 0.16042859455720235,
      "grad_norm": 2.159081220626831,
      "learning_rate": 9.842077574814962e-05,
      "loss": 2.6474,
      "step": 10900
    },
    {
      "epoch": 0.1611645055414097,
      "grad_norm": 2.1696698665618896,
      "learning_rate": 9.840633185787834e-05,
      "loss": 2.6191,
      "step": 10950
    },
    {
      "epoch": 0.16190041652561707,
      "grad_norm": 1.8598417043685913,
      "learning_rate": 9.839182328426513e-05,
      "loss": 2.6073,
      "step": 11000
    },
    {
      "epoch": 0.1626363275098244,
      "grad_norm": 2.121652841567993,
      "learning_rate": 9.83772500466972e-05,
      "loss": 2.5696,
      "step": 11050
    },
    {
      "epoch": 0.16337223849403176,
      "grad_norm": 2.2479772567749023,
      "learning_rate": 9.836261216464813e-05,
      "loss": 2.5749,
      "step": 11100
    },
    {
      "epoch": 0.16410814947823912,
      "grad_norm": 2.495469093322754,
      "learning_rate": 9.834790965767792e-05,
      "loss": 2.5817,
      "step": 11150
    },
    {
      "epoch": 0.16484406046244646,
      "grad_norm": 1.9136141538619995,
      "learning_rate": 9.83331425454329e-05,
      "loss": 2.5596,
      "step": 11200
    },
    {
      "epoch": 0.16557997144665382,
      "grad_norm": 2.1671650409698486,
      "learning_rate": 9.831831084764575e-05,
      "loss": 2.6422,
      "step": 11250
    },
    {
      "epoch": 0.16631588243086115,
      "grad_norm": 2.233119010925293,
      "learning_rate": 9.830341458413544e-05,
      "loss": 2.5418,
      "step": 11300
    },
    {
      "epoch": 0.1670517934150685,
      "grad_norm": 1.8257158994674683,
      "learning_rate": 9.828845377480722e-05,
      "loss": 2.5249,
      "step": 11350
    },
    {
      "epoch": 0.16778770439927587,
      "grad_norm": 2.240715742111206,
      "learning_rate": 9.82734284396526e-05,
      "loss": 2.6191,
      "step": 11400
    },
    {
      "epoch": 0.1685236153834832,
      "grad_norm": 2.1613028049468994,
      "learning_rate": 9.825833859874927e-05,
      "loss": 2.6259,
      "step": 11450
    },
    {
      "epoch": 0.16925952636769057,
      "grad_norm": 2.0110576152801514,
      "learning_rate": 9.824318427226117e-05,
      "loss": 2.5708,
      "step": 11500
    },
    {
      "epoch": 0.16999543735189793,
      "grad_norm": 2.158881425857544,
      "learning_rate": 9.82279654804384e-05,
      "loss": 2.5531,
      "step": 11550
    },
    {
      "epoch": 0.17073134833610526,
      "grad_norm": 2.20902943611145,
      "learning_rate": 9.821268224361717e-05,
      "loss": 2.5371,
      "step": 11600
    },
    {
      "epoch": 0.17146725932031262,
      "grad_norm": 2.2228245735168457,
      "learning_rate": 9.819733458221982e-05,
      "loss": 2.4267,
      "step": 11650
    },
    {
      "epoch": 0.17220317030451995,
      "grad_norm": 2.0783584117889404,
      "learning_rate": 9.818192251675481e-05,
      "loss": 2.5912,
      "step": 11700
    },
    {
      "epoch": 0.17293908128872731,
      "grad_norm": 2.2881600856781006,
      "learning_rate": 9.816644606781663e-05,
      "loss": 2.6078,
      "step": 11750
    },
    {
      "epoch": 0.17367499227293468,
      "grad_norm": 2.227325201034546,
      "learning_rate": 9.815090525608579e-05,
      "loss": 2.4691,
      "step": 11800
    },
    {
      "epoch": 0.174410903257142,
      "grad_norm": 2.061877489089966,
      "learning_rate": 9.813530010232883e-05,
      "loss": 2.5038,
      "step": 11850
    },
    {
      "epoch": 0.17514681424134937,
      "grad_norm": 2.2701311111450195,
      "learning_rate": 9.811963062739826e-05,
      "loss": 2.5341,
      "step": 11900
    },
    {
      "epoch": 0.17588272522555673,
      "grad_norm": 2.3481736183166504,
      "learning_rate": 9.810389685223253e-05,
      "loss": 2.5983,
      "step": 11950
    },
    {
      "epoch": 0.17661863620976406,
      "grad_norm": 2.397742986679077,
      "learning_rate": 9.808809879785602e-05,
      "loss": 2.5213,
      "step": 12000
    },
    {
      "epoch": 0.17735454719397142,
      "grad_norm": 2.047084093093872,
      "learning_rate": 9.807223648537901e-05,
      "loss": 2.5409,
      "step": 12050
    },
    {
      "epoch": 0.17809045817817876,
      "grad_norm": 2.1763134002685547,
      "learning_rate": 9.805630993599767e-05,
      "loss": 2.5032,
      "step": 12100
    },
    {
      "epoch": 0.17882636916238612,
      "grad_norm": 2.242802381515503,
      "learning_rate": 9.804031917099395e-05,
      "loss": 2.5131,
      "step": 12150
    },
    {
      "epoch": 0.17956228014659348,
      "grad_norm": 2.064882278442383,
      "learning_rate": 9.80242642117356e-05,
      "loss": 2.5651,
      "step": 12200
    },
    {
      "epoch": 0.1802981911308008,
      "grad_norm": 2.3599188327789307,
      "learning_rate": 9.800814507967625e-05,
      "loss": 2.5022,
      "step": 12250
    },
    {
      "epoch": 0.18103410211500817,
      "grad_norm": 1.9396848678588867,
      "learning_rate": 9.799196179635518e-05,
      "loss": 2.5533,
      "step": 12300
    },
    {
      "epoch": 0.18177001309921553,
      "grad_norm": 2.6899657249450684,
      "learning_rate": 9.797571438339743e-05,
      "loss": 2.5821,
      "step": 12350
    },
    {
      "epoch": 0.18250592408342287,
      "grad_norm": 2.0358164310455322,
      "learning_rate": 9.795940286251375e-05,
      "loss": 2.4618,
      "step": 12400
    },
    {
      "epoch": 0.18324183506763023,
      "grad_norm": 1.9314616918563843,
      "learning_rate": 9.794302725550053e-05,
      "loss": 2.53,
      "step": 12450
    },
    {
      "epoch": 0.18397774605183756,
      "grad_norm": 2.2856361865997314,
      "learning_rate": 9.79265875842398e-05,
      "loss": 2.5401,
      "step": 12500
    },
    {
      "epoch": 0.18471365703604492,
      "grad_norm": 2.3430120944976807,
      "learning_rate": 9.791008387069921e-05,
      "loss": 2.5092,
      "step": 12550
    },
    {
      "epoch": 0.18544956802025228,
      "grad_norm": 1.8846241235733032,
      "learning_rate": 9.789351613693196e-05,
      "loss": 2.5357,
      "step": 12600
    },
    {
      "epoch": 0.18618547900445961,
      "grad_norm": 2.4495959281921387,
      "learning_rate": 9.787688440507682e-05,
      "loss": 2.4291,
      "step": 12650
    },
    {
      "epoch": 0.18692138998866697,
      "grad_norm": 1.8934112787246704,
      "learning_rate": 9.78601886973581e-05,
      "loss": 2.5522,
      "step": 12700
    },
    {
      "epoch": 0.1876573009728743,
      "grad_norm": 1.9738987684249878,
      "learning_rate": 9.784342903608554e-05,
      "loss": 2.3951,
      "step": 12750
    },
    {
      "epoch": 0.18839321195708167,
      "grad_norm": 2.498098611831665,
      "learning_rate": 9.782660544365437e-05,
      "loss": 2.4944,
      "step": 12800
    },
    {
      "epoch": 0.18912912294128903,
      "grad_norm": 2.3959598541259766,
      "learning_rate": 9.780971794254526e-05,
      "loss": 2.4795,
      "step": 12850
    },
    {
      "epoch": 0.18986503392549636,
      "grad_norm": 1.8322434425354004,
      "learning_rate": 9.779276655532425e-05,
      "loss": 2.5363,
      "step": 12900
    },
    {
      "epoch": 0.19060094490970372,
      "grad_norm": 2.124370574951172,
      "learning_rate": 9.77757513046428e-05,
      "loss": 2.4511,
      "step": 12950
    },
    {
      "epoch": 0.19133685589391108,
      "grad_norm": 2.210029125213623,
      "learning_rate": 9.775867221323764e-05,
      "loss": 2.5573,
      "step": 13000
    },
    {
      "epoch": 0.19207276687811842,
      "grad_norm": 2.3331785202026367,
      "learning_rate": 9.774152930393083e-05,
      "loss": 2.441,
      "step": 13050
    },
    {
      "epoch": 0.19280867786232578,
      "grad_norm": 2.3219029903411865,
      "learning_rate": 9.772432259962976e-05,
      "loss": 2.5062,
      "step": 13100
    },
    {
      "epoch": 0.1935445888465331,
      "grad_norm": 3.104140043258667,
      "learning_rate": 9.770705212332701e-05,
      "loss": 2.5679,
      "step": 13150
    },
    {
      "epoch": 0.19428049983074047,
      "grad_norm": 2.4788012504577637,
      "learning_rate": 9.768971789810035e-05,
      "loss": 2.4534,
      "step": 13200
    },
    {
      "epoch": 0.19501641081494783,
      "grad_norm": 2.011667013168335,
      "learning_rate": 9.767231994711283e-05,
      "loss": 2.5075,
      "step": 13250
    },
    {
      "epoch": 0.19575232179915517,
      "grad_norm": 2.1427969932556152,
      "learning_rate": 9.765485829361256e-05,
      "loss": 2.4506,
      "step": 13300
    },
    {
      "epoch": 0.19648823278336253,
      "grad_norm": 2.1317107677459717,
      "learning_rate": 9.763733296093282e-05,
      "loss": 2.4638,
      "step": 13350
    },
    {
      "epoch": 0.1972241437675699,
      "grad_norm": 2.1159369945526123,
      "learning_rate": 9.7619743972492e-05,
      "loss": 2.556,
      "step": 13400
    },
    {
      "epoch": 0.19796005475177722,
      "grad_norm": 2.2738003730773926,
      "learning_rate": 9.760209135179349e-05,
      "loss": 2.5106,
      "step": 13450
    },
    {
      "epoch": 0.19869596573598458,
      "grad_norm": 2.023183822631836,
      "learning_rate": 9.758437512242576e-05,
      "loss": 2.5799,
      "step": 13500
    },
    {
      "epoch": 0.19943187672019191,
      "grad_norm": 2.129171848297119,
      "learning_rate": 9.756659530806227e-05,
      "loss": 2.4989,
      "step": 13550
    },
    {
      "epoch": 0.20016778770439927,
      "grad_norm": 2.31400203704834,
      "learning_rate": 9.754875193246141e-05,
      "loss": 2.5114,
      "step": 13600
    },
    {
      "epoch": 0.20090369868860664,
      "grad_norm": 2.3104798793792725,
      "learning_rate": 9.753084501946656e-05,
      "loss": 2.4572,
      "step": 13650
    },
    {
      "epoch": 0.20163960967281397,
      "grad_norm": 2.1549336910247803,
      "learning_rate": 9.751287459300595e-05,
      "loss": 2.4823,
      "step": 13700
    },
    {
      "epoch": 0.20237552065702133,
      "grad_norm": 2.17750883102417,
      "learning_rate": 9.749484067709273e-05,
      "loss": 2.4594,
      "step": 13750
    },
    {
      "epoch": 0.2031114316412287,
      "grad_norm": 2.168471336364746,
      "learning_rate": 9.747674329582483e-05,
      "loss": 2.4209,
      "step": 13800
    },
    {
      "epoch": 0.20384734262543602,
      "grad_norm": 2.3747196197509766,
      "learning_rate": 9.745858247338504e-05,
      "loss": 2.4675,
      "step": 13850
    },
    {
      "epoch": 0.20458325360964338,
      "grad_norm": 2.2613794803619385,
      "learning_rate": 9.744035823404089e-05,
      "loss": 2.4754,
      "step": 13900
    },
    {
      "epoch": 0.20531916459385072,
      "grad_norm": 1.9614430665969849,
      "learning_rate": 9.742207060214467e-05,
      "loss": 2.4688,
      "step": 13950
    },
    {
      "epoch": 0.20605507557805808,
      "grad_norm": 2.2639949321746826,
      "learning_rate": 9.740371960213338e-05,
      "loss": 2.5276,
      "step": 14000
    },
    {
      "epoch": 0.20679098656226544,
      "grad_norm": 1.9169578552246094,
      "learning_rate": 9.738530525852866e-05,
      "loss": 2.4873,
      "step": 14050
    },
    {
      "epoch": 0.20752689754647277,
      "grad_norm": 2.0531113147735596,
      "learning_rate": 9.736682759593687e-05,
      "loss": 2.5354,
      "step": 14100
    },
    {
      "epoch": 0.20826280853068013,
      "grad_norm": 2.27251935005188,
      "learning_rate": 9.734828663904889e-05,
      "loss": 2.4602,
      "step": 14150
    },
    {
      "epoch": 0.20899871951488747,
      "grad_norm": 2.095240354537964,
      "learning_rate": 9.732968241264023e-05,
      "loss": 2.4612,
      "step": 14200
    },
    {
      "epoch": 0.20973463049909483,
      "grad_norm": 2.1164491176605225,
      "learning_rate": 9.731101494157093e-05,
      "loss": 2.5038,
      "step": 14250
    },
    {
      "epoch": 0.2104705414833022,
      "grad_norm": 2.3826425075531006,
      "learning_rate": 9.729228425078556e-05,
      "loss": 2.4987,
      "step": 14300
    },
    {
      "epoch": 0.21120645246750952,
      "grad_norm": 1.8497806787490845,
      "learning_rate": 9.727349036531316e-05,
      "loss": 2.5037,
      "step": 14350
    },
    {
      "epoch": 0.21194236345171688,
      "grad_norm": 2.1769251823425293,
      "learning_rate": 9.725463331026717e-05,
      "loss": 2.4147,
      "step": 14400
    },
    {
      "epoch": 0.21267827443592424,
      "grad_norm": 1.9919432401657104,
      "learning_rate": 9.723571311084552e-05,
      "loss": 2.4785,
      "step": 14450
    },
    {
      "epoch": 0.21341418542013157,
      "grad_norm": 2.2492873668670654,
      "learning_rate": 9.721672979233043e-05,
      "loss": 2.4574,
      "step": 14500
    },
    {
      "epoch": 0.21415009640433894,
      "grad_norm": 2.0674216747283936,
      "learning_rate": 9.719768338008856e-05,
      "loss": 2.4136,
      "step": 14550
    },
    {
      "epoch": 0.21488600738854627,
      "grad_norm": 2.3409807682037354,
      "learning_rate": 9.71785738995708e-05,
      "loss": 2.4574,
      "step": 14600
    },
    {
      "epoch": 0.21562191837275363,
      "grad_norm": 2.0056378841400146,
      "learning_rate": 9.715940137631232e-05,
      "loss": 2.4152,
      "step": 14650
    },
    {
      "epoch": 0.216357829356961,
      "grad_norm": 2.3029491901397705,
      "learning_rate": 9.71401658359326e-05,
      "loss": 2.4895,
      "step": 14700
    },
    {
      "epoch": 0.21709374034116832,
      "grad_norm": 2.509934902191162,
      "learning_rate": 9.712086730413528e-05,
      "loss": 2.4741,
      "step": 14750
    },
    {
      "epoch": 0.21782965132537568,
      "grad_norm": 2.2416813373565674,
      "learning_rate": 9.710150580670814e-05,
      "loss": 2.4761,
      "step": 14800
    },
    {
      "epoch": 0.21856556230958304,
      "grad_norm": 2.204141139984131,
      "learning_rate": 9.708208136952314e-05,
      "loss": 2.4743,
      "step": 14850
    },
    {
      "epoch": 0.21930147329379038,
      "grad_norm": 2.12750506401062,
      "learning_rate": 9.706259401853637e-05,
      "loss": 2.4895,
      "step": 14900
    },
    {
      "epoch": 0.22003738427799774,
      "grad_norm": 1.9566905498504639,
      "learning_rate": 9.704304377978794e-05,
      "loss": 2.4616,
      "step": 14950
    },
    {
      "epoch": 0.22077329526220507,
      "grad_norm": 2.429940938949585,
      "learning_rate": 9.702343067940198e-05,
      "loss": 2.4508,
      "step": 15000
    },
    {
      "epoch": 0.22150920624641243,
      "grad_norm": 3.036057233810425,
      "learning_rate": 9.70037547435867e-05,
      "loss": 2.4631,
      "step": 15050
    },
    {
      "epoch": 0.2222451172306198,
      "grad_norm": 2.426251173019409,
      "learning_rate": 9.69840159986342e-05,
      "loss": 2.4464,
      "step": 15100
    },
    {
      "epoch": 0.22298102821482713,
      "grad_norm": 2.213294506072998,
      "learning_rate": 9.696421447092052e-05,
      "loss": 2.433,
      "step": 15150
    },
    {
      "epoch": 0.2237169391990345,
      "grad_norm": 2.1573476791381836,
      "learning_rate": 9.694435018690562e-05,
      "loss": 2.3879,
      "step": 15200
    },
    {
      "epoch": 0.22445285018324185,
      "grad_norm": 2.104952573776245,
      "learning_rate": 9.69244231731333e-05,
      "loss": 2.4526,
      "step": 15250
    },
    {
      "epoch": 0.22518876116744918,
      "grad_norm": 2.1775598526000977,
      "learning_rate": 9.690443345623121e-05,
      "loss": 2.4033,
      "step": 15300
    },
    {
      "epoch": 0.22592467215165654,
      "grad_norm": 2.3251075744628906,
      "learning_rate": 9.688438106291076e-05,
      "loss": 2.4471,
      "step": 15350
    },
    {
      "epoch": 0.22666058313586387,
      "grad_norm": 2.49636173248291,
      "learning_rate": 9.68642660199671e-05,
      "loss": 2.4031,
      "step": 15400
    },
    {
      "epoch": 0.22739649412007124,
      "grad_norm": 1.9022153615951538,
      "learning_rate": 9.684408835427912e-05,
      "loss": 2.4226,
      "step": 15450
    },
    {
      "epoch": 0.2281324051042786,
      "grad_norm": 2.280881643295288,
      "learning_rate": 9.682384809280939e-05,
      "loss": 2.4034,
      "step": 15500
    },
    {
      "epoch": 0.22886831608848593,
      "grad_norm": 2.1913702487945557,
      "learning_rate": 9.680354526260411e-05,
      "loss": 2.4253,
      "step": 15550
    },
    {
      "epoch": 0.2296042270726933,
      "grad_norm": 2.1636900901794434,
      "learning_rate": 9.678317989079312e-05,
      "loss": 2.4011,
      "step": 15600
    },
    {
      "epoch": 0.23034013805690065,
      "grad_norm": 2.4236035346984863,
      "learning_rate": 9.676275200458979e-05,
      "loss": 2.4197,
      "step": 15650
    },
    {
      "epoch": 0.23107604904110798,
      "grad_norm": 2.1840429306030273,
      "learning_rate": 9.674226163129104e-05,
      "loss": 2.4644,
      "step": 15700
    },
    {
      "epoch": 0.23181196002531534,
      "grad_norm": 2.2827649116516113,
      "learning_rate": 9.672170879827731e-05,
      "loss": 2.379,
      "step": 15750
    },
    {
      "epoch": 0.23254787100952268,
      "grad_norm": 2.285521984100342,
      "learning_rate": 9.670109353301247e-05,
      "loss": 2.465,
      "step": 15800
    },
    {
      "epoch": 0.23328378199373004,
      "grad_norm": 2.268141508102417,
      "learning_rate": 9.668041586304382e-05,
      "loss": 2.3507,
      "step": 15850
    },
    {
      "epoch": 0.2340196929779374,
      "grad_norm": 2.1702475547790527,
      "learning_rate": 9.665967581600209e-05,
      "loss": 2.3807,
      "step": 15900
    },
    {
      "epoch": 0.23475560396214473,
      "grad_norm": 2.3410379886627197,
      "learning_rate": 9.663887341960131e-05,
      "loss": 2.4445,
      "step": 15950
    },
    {
      "epoch": 0.2354915149463521,
      "grad_norm": 2.113358736038208,
      "learning_rate": 9.661800870163884e-05,
      "loss": 2.4577,
      "step": 16000
    },
    {
      "epoch": 0.23622742593055943,
      "grad_norm": 2.419673442840576,
      "learning_rate": 9.659708168999535e-05,
      "loss": 2.3736,
      "step": 16050
    },
    {
      "epoch": 0.2369633369147668,
      "grad_norm": 2.2275846004486084,
      "learning_rate": 9.65760924126347e-05,
      "loss": 2.3515,
      "step": 16100
    },
    {
      "epoch": 0.23769924789897415,
      "grad_norm": 2.0551211833953857,
      "learning_rate": 9.655504089760399e-05,
      "loss": 2.3575,
      "step": 16150
    },
    {
      "epoch": 0.23843515888318148,
      "grad_norm": 1.752110481262207,
      "learning_rate": 9.653392717303346e-05,
      "loss": 2.3544,
      "step": 16200
    },
    {
      "epoch": 0.23917106986738884,
      "grad_norm": 2.029726266860962,
      "learning_rate": 9.651275126713651e-05,
      "loss": 2.4309,
      "step": 16250
    },
    {
      "epoch": 0.2399069808515962,
      "grad_norm": 2.0296695232391357,
      "learning_rate": 9.64915132082096e-05,
      "loss": 2.3757,
      "step": 16300
    },
    {
      "epoch": 0.24064289183580354,
      "grad_norm": 1.9766769409179688,
      "learning_rate": 9.647021302463224e-05,
      "loss": 2.3599,
      "step": 16350
    },
    {
      "epoch": 0.2413788028200109,
      "grad_norm": 2.2368650436401367,
      "learning_rate": 9.644885074486699e-05,
      "loss": 2.3761,
      "step": 16400
    },
    {
      "epoch": 0.24211471380421823,
      "grad_norm": 2.1200296878814697,
      "learning_rate": 9.642742639745936e-05,
      "loss": 2.3868,
      "step": 16450
    },
    {
      "epoch": 0.2428506247884256,
      "grad_norm": 1.905089259147644,
      "learning_rate": 9.640594001103778e-05,
      "loss": 2.4538,
      "step": 16500
    },
    {
      "epoch": 0.24358653577263295,
      "grad_norm": 2.148160219192505,
      "learning_rate": 9.638439161431364e-05,
      "loss": 2.3912,
      "step": 16550
    },
    {
      "epoch": 0.24432244675684028,
      "grad_norm": 2.1921656131744385,
      "learning_rate": 9.636278123608114e-05,
      "loss": 2.4389,
      "step": 16600
    },
    {
      "epoch": 0.24505835774104764,
      "grad_norm": 1.824918270111084,
      "learning_rate": 9.634110890521729e-05,
      "loss": 2.3989,
      "step": 16650
    },
    {
      "epoch": 0.245794268725255,
      "grad_norm": 2.1810739040374756,
      "learning_rate": 9.631937465068195e-05,
      "loss": 2.3632,
      "step": 16700
    },
    {
      "epoch": 0.24653017970946234,
      "grad_norm": 2.5107860565185547,
      "learning_rate": 9.629757850151769e-05,
      "loss": 2.4037,
      "step": 16750
    },
    {
      "epoch": 0.2472660906936697,
      "grad_norm": 2.2340126037597656,
      "learning_rate": 9.627572048684974e-05,
      "loss": 2.3689,
      "step": 16800
    },
    {
      "epoch": 0.24800200167787703,
      "grad_norm": 2.277407169342041,
      "learning_rate": 9.62538006358861e-05,
      "loss": 2.3434,
      "step": 16850
    },
    {
      "epoch": 0.2487379126620844,
      "grad_norm": 2.262761354446411,
      "learning_rate": 9.623181897791731e-05,
      "loss": 2.3849,
      "step": 16900
    },
    {
      "epoch": 0.24947382364629175,
      "grad_norm": 2.0463287830352783,
      "learning_rate": 9.620977554231654e-05,
      "loss": 2.3686,
      "step": 16950
    },
    {
      "epoch": 0.2502097346304991,
      "grad_norm": 2.1355459690093994,
      "learning_rate": 9.61876703585395e-05,
      "loss": 2.3872,
      "step": 17000
    },
    {
      "epoch": 0.25094564561470645,
      "grad_norm": 2.1607933044433594,
      "learning_rate": 9.616550345612444e-05,
      "loss": 2.3417,
      "step": 17050
    },
    {
      "epoch": 0.2516815565989138,
      "grad_norm": 2.0415961742401123,
      "learning_rate": 9.614327486469203e-05,
      "loss": 2.2982,
      "step": 17100
    },
    {
      "epoch": 0.25241746758312117,
      "grad_norm": 2.199702262878418,
      "learning_rate": 9.612098461394541e-05,
      "loss": 2.3979,
      "step": 17150
    },
    {
      "epoch": 0.2531533785673285,
      "grad_norm": 2.012286901473999,
      "learning_rate": 9.60986327336701e-05,
      "loss": 2.4631,
      "step": 17200
    },
    {
      "epoch": 0.25388928955153583,
      "grad_norm": 2.3552894592285156,
      "learning_rate": 9.6076219253734e-05,
      "loss": 2.4167,
      "step": 17250
    },
    {
      "epoch": 0.2546252005357432,
      "grad_norm": 2.2238152027130127,
      "learning_rate": 9.605374420408725e-05,
      "loss": 2.3749,
      "step": 17300
    },
    {
      "epoch": 0.25536111151995056,
      "grad_norm": 2.1188251972198486,
      "learning_rate": 9.603120761476237e-05,
      "loss": 2.353,
      "step": 17350
    },
    {
      "epoch": 0.2560970225041579,
      "grad_norm": 1.9240044355392456,
      "learning_rate": 9.6008609515874e-05,
      "loss": 2.3686,
      "step": 17400
    },
    {
      "epoch": 0.2568329334883652,
      "grad_norm": 2.1943230628967285,
      "learning_rate": 9.598594993761905e-05,
      "loss": 2.3775,
      "step": 17450
    },
    {
      "epoch": 0.2575688444725726,
      "grad_norm": 2.3161067962646484,
      "learning_rate": 9.596322891027659e-05,
      "loss": 2.4157,
      "step": 17500
    },
    {
      "epoch": 0.25830475545677994,
      "grad_norm": 2.103386640548706,
      "learning_rate": 9.594044646420772e-05,
      "loss": 2.3214,
      "step": 17550
    },
    {
      "epoch": 0.2590406664409873,
      "grad_norm": 2.0754106044769287,
      "learning_rate": 9.59176026298557e-05,
      "loss": 2.3766,
      "step": 17600
    },
    {
      "epoch": 0.25977657742519467,
      "grad_norm": 2.1913022994995117,
      "learning_rate": 9.589469743774575e-05,
      "loss": 2.3167,
      "step": 17650
    },
    {
      "epoch": 0.260512488409402,
      "grad_norm": 2.1758313179016113,
      "learning_rate": 9.587173091848516e-05,
      "loss": 2.3278,
      "step": 17700
    },
    {
      "epoch": 0.26124839939360933,
      "grad_norm": 2.570582628250122,
      "learning_rate": 9.584870310276308e-05,
      "loss": 2.4008,
      "step": 17750
    },
    {
      "epoch": 0.2619843103778167,
      "grad_norm": 2.4913370609283447,
      "learning_rate": 9.582561402135062e-05,
      "loss": 2.3363,
      "step": 17800
    },
    {
      "epoch": 0.26272022136202405,
      "grad_norm": 2.1713900566101074,
      "learning_rate": 9.580246370510078e-05,
      "loss": 2.3471,
      "step": 17850
    },
    {
      "epoch": 0.2634561323462314,
      "grad_norm": 2.2555248737335205,
      "learning_rate": 9.57792521849483e-05,
      "loss": 2.3931,
      "step": 17900
    },
    {
      "epoch": 0.2641920433304388,
      "grad_norm": 2.0723800659179688,
      "learning_rate": 9.575597949190977e-05,
      "loss": 2.348,
      "step": 17950
    },
    {
      "epoch": 0.2649279543146461,
      "grad_norm": 2.388047456741333,
      "learning_rate": 9.573264565708355e-05,
      "loss": 2.4134,
      "step": 18000
    },
    {
      "epoch": 0.26566386529885344,
      "grad_norm": 2.2720816135406494,
      "learning_rate": 9.570925071164962e-05,
      "loss": 2.3999,
      "step": 18050
    },
    {
      "epoch": 0.2663997762830608,
      "grad_norm": 2.274787664413452,
      "learning_rate": 9.568579468686967e-05,
      "loss": 2.3363,
      "step": 18100
    },
    {
      "epoch": 0.26713568726726816,
      "grad_norm": 2.218883991241455,
      "learning_rate": 9.5662277614087e-05,
      "loss": 2.3503,
      "step": 18150
    },
    {
      "epoch": 0.2678715982514755,
      "grad_norm": 2.0276427268981934,
      "learning_rate": 9.563869952472648e-05,
      "loss": 2.3347,
      "step": 18200
    },
    {
      "epoch": 0.26860750923568283,
      "grad_norm": 2.0249879360198975,
      "learning_rate": 9.561506045029451e-05,
      "loss": 2.3114,
      "step": 18250
    },
    {
      "epoch": 0.2693434202198902,
      "grad_norm": 2.100822925567627,
      "learning_rate": 9.559136042237902e-05,
      "loss": 2.2765,
      "step": 18300
    },
    {
      "epoch": 0.27007933120409755,
      "grad_norm": 2.3129003047943115,
      "learning_rate": 9.556759947264932e-05,
      "loss": 2.3364,
      "step": 18350
    },
    {
      "epoch": 0.2708152421883049,
      "grad_norm": 2.212362766265869,
      "learning_rate": 9.55437776328562e-05,
      "loss": 2.371,
      "step": 18400
    },
    {
      "epoch": 0.27155115317251227,
      "grad_norm": 2.321101188659668,
      "learning_rate": 9.551989493483174e-05,
      "loss": 2.2907,
      "step": 18450
    },
    {
      "epoch": 0.2722870641567196,
      "grad_norm": 2.4185879230499268,
      "learning_rate": 9.549595141048942e-05,
      "loss": 2.321,
      "step": 18500
    },
    {
      "epoch": 0.27302297514092694,
      "grad_norm": 2.0907442569732666,
      "learning_rate": 9.547194709182396e-05,
      "loss": 2.3561,
      "step": 18550
    },
    {
      "epoch": 0.2737588861251343,
      "grad_norm": 2.0302655696868896,
      "learning_rate": 9.544788201091131e-05,
      "loss": 2.3433,
      "step": 18600
    },
    {
      "epoch": 0.27449479710934166,
      "grad_norm": 2.039696216583252,
      "learning_rate": 9.542375619990862e-05,
      "loss": 2.346,
      "step": 18650
    },
    {
      "epoch": 0.275230708093549,
      "grad_norm": 2.2493839263916016,
      "learning_rate": 9.539956969105419e-05,
      "loss": 2.3565,
      "step": 18700
    },
    {
      "epoch": 0.2759666190777564,
      "grad_norm": 1.9399415254592896,
      "learning_rate": 9.537532251666745e-05,
      "loss": 2.3356,
      "step": 18750
    },
    {
      "epoch": 0.2767025300619637,
      "grad_norm": 1.7913950681686401,
      "learning_rate": 9.535101470914887e-05,
      "loss": 2.3426,
      "step": 18800
    },
    {
      "epoch": 0.27743844104617105,
      "grad_norm": 1.8750901222229004,
      "learning_rate": 9.532664630097995e-05,
      "loss": 2.3232,
      "step": 18850
    },
    {
      "epoch": 0.2781743520303784,
      "grad_norm": 2.2687857151031494,
      "learning_rate": 9.530221732472316e-05,
      "loss": 2.2932,
      "step": 18900
    },
    {
      "epoch": 0.27891026301458577,
      "grad_norm": 2.0776662826538086,
      "learning_rate": 9.527772781302192e-05,
      "loss": 2.3614,
      "step": 18950
    },
    {
      "epoch": 0.2796461739987931,
      "grad_norm": 2.0158650875091553,
      "learning_rate": 9.525317779860054e-05,
      "loss": 2.3582,
      "step": 19000
    },
    {
      "epoch": 0.28038208498300043,
      "grad_norm": 1.8154505491256714,
      "learning_rate": 9.522856731426415e-05,
      "loss": 2.2865,
      "step": 19050
    },
    {
      "epoch": 0.2811179959672078,
      "grad_norm": 2.13153338432312,
      "learning_rate": 9.520389639289871e-05,
      "loss": 2.4237,
      "step": 19100
    },
    {
      "epoch": 0.28185390695141516,
      "grad_norm": 2.6521759033203125,
      "learning_rate": 9.517916506747095e-05,
      "loss": 2.317,
      "step": 19150
    },
    {
      "epoch": 0.2825898179356225,
      "grad_norm": 2.0866074562072754,
      "learning_rate": 9.515437337102829e-05,
      "loss": 2.3333,
      "step": 19200
    },
    {
      "epoch": 0.2833257289198299,
      "grad_norm": 2.208364725112915,
      "learning_rate": 9.51295213366988e-05,
      "loss": 2.2892,
      "step": 19250
    },
    {
      "epoch": 0.2840616399040372,
      "grad_norm": 2.122164011001587,
      "learning_rate": 9.510460899769124e-05,
      "loss": 2.3083,
      "step": 19300
    },
    {
      "epoch": 0.28479755088824454,
      "grad_norm": 1.7132470607757568,
      "learning_rate": 9.507963638729491e-05,
      "loss": 2.29,
      "step": 19350
    },
    {
      "epoch": 0.28553346187245193,
      "grad_norm": 2.3828089237213135,
      "learning_rate": 9.505460353887967e-05,
      "loss": 2.3287,
      "step": 19400
    },
    {
      "epoch": 0.28626937285665927,
      "grad_norm": 2.122231960296631,
      "learning_rate": 9.502951048589584e-05,
      "loss": 2.2766,
      "step": 19450
    },
    {
      "epoch": 0.2870052838408666,
      "grad_norm": 1.9513579607009888,
      "learning_rate": 9.50043572618742e-05,
      "loss": 2.3138,
      "step": 19500
    },
    {
      "epoch": 0.28774119482507393,
      "grad_norm": 2.288209915161133,
      "learning_rate": 9.4979143900426e-05,
      "loss": 2.2865,
      "step": 19550
    },
    {
      "epoch": 0.2884771058092813,
      "grad_norm": 2.17467999458313,
      "learning_rate": 9.49538704352427e-05,
      "loss": 2.3747,
      "step": 19600
    },
    {
      "epoch": 0.28921301679348865,
      "grad_norm": 1.9796286821365356,
      "learning_rate": 9.492853690009626e-05,
      "loss": 2.3057,
      "step": 19650
    },
    {
      "epoch": 0.289948927777696,
      "grad_norm": 2.141854763031006,
      "learning_rate": 9.490314332883878e-05,
      "loss": 2.3077,
      "step": 19700
    },
    {
      "epoch": 0.2906848387619034,
      "grad_norm": 2.3682637214660645,
      "learning_rate": 9.48776897554026e-05,
      "loss": 2.2727,
      "step": 19750
    },
    {
      "epoch": 0.2914207497461107,
      "grad_norm": 2.021087646484375,
      "learning_rate": 9.485217621380027e-05,
      "loss": 2.3055,
      "step": 19800
    },
    {
      "epoch": 0.29215666073031804,
      "grad_norm": 2.278716564178467,
      "learning_rate": 9.482660273812447e-05,
      "loss": 2.4021,
      "step": 19850
    },
    {
      "epoch": 0.29289257171452543,
      "grad_norm": 1.9995622634887695,
      "learning_rate": 9.480096936254795e-05,
      "loss": 2.314,
      "step": 19900
    },
    {
      "epoch": 0.29362848269873276,
      "grad_norm": 2.0324435234069824,
      "learning_rate": 9.477527612132352e-05,
      "loss": 2.3303,
      "step": 19950
    },
    {
      "epoch": 0.2943643936829401,
      "grad_norm": 1.9223114252090454,
      "learning_rate": 9.474952304878397e-05,
      "loss": 2.2786,
      "step": 20000
    },
    {
      "epoch": 0.2951003046671475,
      "grad_norm": 2.2994649410247803,
      "learning_rate": 9.472371017934203e-05,
      "loss": 2.3162,
      "step": 20050
    },
    {
      "epoch": 0.2958362156513548,
      "grad_norm": 1.8621258735656738,
      "learning_rate": 9.469783754749038e-05,
      "loss": 2.299,
      "step": 20100
    },
    {
      "epoch": 0.29657212663556215,
      "grad_norm": 1.8668580055236816,
      "learning_rate": 9.467190518780149e-05,
      "loss": 2.4068,
      "step": 20150
    },
    {
      "epoch": 0.29730803761976954,
      "grad_norm": 1.8674606084823608,
      "learning_rate": 9.464591313492772e-05,
      "loss": 2.3301,
      "step": 20200
    },
    {
      "epoch": 0.29804394860397687,
      "grad_norm": 2.0244340896606445,
      "learning_rate": 9.461986142360113e-05,
      "loss": 2.3053,
      "step": 20250
    },
    {
      "epoch": 0.2987798595881842,
      "grad_norm": 1.9127219915390015,
      "learning_rate": 9.459375008863353e-05,
      "loss": 2.3276,
      "step": 20300
    },
    {
      "epoch": 0.29951577057239154,
      "grad_norm": 2.1249630451202393,
      "learning_rate": 9.456757916491642e-05,
      "loss": 2.312,
      "step": 20350
    },
    {
      "epoch": 0.3002516815565989,
      "grad_norm": 2.3850913047790527,
      "learning_rate": 9.454134868742084e-05,
      "loss": 2.2934,
      "step": 20400
    },
    {
      "epoch": 0.30098759254080626,
      "grad_norm": 2.1305315494537354,
      "learning_rate": 9.451505869119753e-05,
      "loss": 2.2874,
      "step": 20450
    },
    {
      "epoch": 0.3017235035250136,
      "grad_norm": 2.080056667327881,
      "learning_rate": 9.448870921137669e-05,
      "loss": 2.3008,
      "step": 20500
    },
    {
      "epoch": 0.302459414509221,
      "grad_norm": 1.7412214279174805,
      "learning_rate": 9.446230028316803e-05,
      "loss": 2.3011,
      "step": 20550
    },
    {
      "epoch": 0.3031953254934283,
      "grad_norm": 2.2679991722106934,
      "learning_rate": 9.443583194186066e-05,
      "loss": 2.2838,
      "step": 20600
    },
    {
      "epoch": 0.30393123647763565,
      "grad_norm": 2.3315114974975586,
      "learning_rate": 9.440930422282313e-05,
      "loss": 2.2182,
      "step": 20650
    },
    {
      "epoch": 0.30466714746184304,
      "grad_norm": 2.0215606689453125,
      "learning_rate": 9.438271716150328e-05,
      "loss": 2.3323,
      "step": 20700
    },
    {
      "epoch": 0.30540305844605037,
      "grad_norm": 2.2671046257019043,
      "learning_rate": 9.435607079342835e-05,
      "loss": 2.2732,
      "step": 20750
    },
    {
      "epoch": 0.3061389694302577,
      "grad_norm": 1.9943598508834839,
      "learning_rate": 9.43293651542047e-05,
      "loss": 2.2712,
      "step": 20800
    },
    {
      "epoch": 0.3068748804144651,
      "grad_norm": 2.004096746444702,
      "learning_rate": 9.430260027951797e-05,
      "loss": 2.3151,
      "step": 20850
    },
    {
      "epoch": 0.3076107913986724,
      "grad_norm": 2.083873987197876,
      "learning_rate": 9.427577620513293e-05,
      "loss": 2.358,
      "step": 20900
    },
    {
      "epoch": 0.30834670238287976,
      "grad_norm": 2.332894802093506,
      "learning_rate": 9.424889296689346e-05,
      "loss": 2.2882,
      "step": 20950
    },
    {
      "epoch": 0.30908261336708714,
      "grad_norm": 1.8516570329666138,
      "learning_rate": 9.422195060072249e-05,
      "loss": 2.3177,
      "step": 21000
    },
    {
      "epoch": 0.3098185243512945,
      "grad_norm": 2.4541876316070557,
      "learning_rate": 9.419494914262199e-05,
      "loss": 2.2598,
      "step": 21050
    },
    {
      "epoch": 0.3105544353355018,
      "grad_norm": 2.013791561126709,
      "learning_rate": 9.416788862867288e-05,
      "loss": 2.3047,
      "step": 21100
    },
    {
      "epoch": 0.31129034631970914,
      "grad_norm": 2.2065067291259766,
      "learning_rate": 9.414076909503494e-05,
      "loss": 2.2103,
      "step": 21150
    },
    {
      "epoch": 0.31202625730391653,
      "grad_norm": 2.0272226333618164,
      "learning_rate": 9.411359057794688e-05,
      "loss": 2.3669,
      "step": 21200
    },
    {
      "epoch": 0.31276216828812387,
      "grad_norm": 2.292856454849243,
      "learning_rate": 9.408635311372623e-05,
      "loss": 2.3144,
      "step": 21250
    },
    {
      "epoch": 0.3134980792723312,
      "grad_norm": 1.9745320081710815,
      "learning_rate": 9.405905673876924e-05,
      "loss": 2.2831,
      "step": 21300
    },
    {
      "epoch": 0.3142339902565386,
      "grad_norm": 2.0479469299316406,
      "learning_rate": 9.403170148955094e-05,
      "loss": 2.2977,
      "step": 21350
    },
    {
      "epoch": 0.3149699012407459,
      "grad_norm": 2.0878753662109375,
      "learning_rate": 9.400428740262495e-05,
      "loss": 2.2504,
      "step": 21400
    },
    {
      "epoch": 0.31570581222495325,
      "grad_norm": 2.134528636932373,
      "learning_rate": 9.397681451462358e-05,
      "loss": 2.3453,
      "step": 21450
    },
    {
      "epoch": 0.31644172320916064,
      "grad_norm": 2.483980178833008,
      "learning_rate": 9.394928286225771e-05,
      "loss": 2.2951,
      "step": 21500
    },
    {
      "epoch": 0.317177634193368,
      "grad_norm": 2.0920519828796387,
      "learning_rate": 9.392169248231668e-05,
      "loss": 2.3344,
      "step": 21550
    },
    {
      "epoch": 0.3179135451775753,
      "grad_norm": 1.9916527271270752,
      "learning_rate": 9.38940434116684e-05,
      "loss": 2.2721,
      "step": 21600
    },
    {
      "epoch": 0.3186494561617827,
      "grad_norm": 1.9313410520553589,
      "learning_rate": 9.386633568725916e-05,
      "loss": 2.3027,
      "step": 21650
    },
    {
      "epoch": 0.31938536714599003,
      "grad_norm": 2.059896230697632,
      "learning_rate": 9.38385693461136e-05,
      "loss": 2.2503,
      "step": 21700
    },
    {
      "epoch": 0.32012127813019736,
      "grad_norm": 2.162633180618286,
      "learning_rate": 9.38107444253347e-05,
      "loss": 2.2639,
      "step": 21750
    },
    {
      "epoch": 0.3208571891144047,
      "grad_norm": 2.003955364227295,
      "learning_rate": 9.378286096210375e-05,
      "loss": 2.2653,
      "step": 21800
    },
    {
      "epoch": 0.3215931000986121,
      "grad_norm": 2.3845133781433105,
      "learning_rate": 9.375491899368028e-05,
      "loss": 2.2713,
      "step": 21850
    },
    {
      "epoch": 0.3223290110828194,
      "grad_norm": 1.8514690399169922,
      "learning_rate": 9.37269185574019e-05,
      "loss": 2.2638,
      "step": 21900
    },
    {
      "epoch": 0.32306492206702675,
      "grad_norm": 1.876551628112793,
      "learning_rate": 9.369885969068444e-05,
      "loss": 2.2455,
      "step": 21950
    },
    {
      "epoch": 0.32380083305123414,
      "grad_norm": 2.129519462585449,
      "learning_rate": 9.36707424310218e-05,
      "loss": 2.3154,
      "step": 22000
    },
    {
      "epoch": 0.32453674403544147,
      "grad_norm": 2.1937806606292725,
      "learning_rate": 9.364256681598587e-05,
      "loss": 2.2658,
      "step": 22050
    },
    {
      "epoch": 0.3252726550196488,
      "grad_norm": 2.017517328262329,
      "learning_rate": 9.361433288322655e-05,
      "loss": 2.2781,
      "step": 22100
    },
    {
      "epoch": 0.3260085660038562,
      "grad_norm": 2.0477638244628906,
      "learning_rate": 9.358604067047164e-05,
      "loss": 2.2171,
      "step": 22150
    },
    {
      "epoch": 0.3267444769880635,
      "grad_norm": 2.2709178924560547,
      "learning_rate": 9.355769021552682e-05,
      "loss": 2.2414,
      "step": 22200
    },
    {
      "epoch": 0.32748038797227086,
      "grad_norm": 1.990504264831543,
      "learning_rate": 9.352928155627565e-05,
      "loss": 2.2425,
      "step": 22250
    },
    {
      "epoch": 0.32821629895647825,
      "grad_norm": 2.22339129447937,
      "learning_rate": 9.350081473067942e-05,
      "loss": 2.2413,
      "step": 22300
    },
    {
      "epoch": 0.3289522099406856,
      "grad_norm": 2.23840594291687,
      "learning_rate": 9.34722897767771e-05,
      "loss": 2.2412,
      "step": 22350
    },
    {
      "epoch": 0.3296881209248929,
      "grad_norm": 1.9463518857955933,
      "learning_rate": 9.344370673268543e-05,
      "loss": 2.267,
      "step": 22400
    },
    {
      "epoch": 0.3304240319091003,
      "grad_norm": 2.2689003944396973,
      "learning_rate": 9.341506563659871e-05,
      "loss": 2.27,
      "step": 22450
    },
    {
      "epoch": 0.33115994289330764,
      "grad_norm": 2.0893795490264893,
      "learning_rate": 9.338636652678882e-05,
      "loss": 2.3037,
      "step": 22500
    },
    {
      "epoch": 0.33189585387751497,
      "grad_norm": 1.7837141752243042,
      "learning_rate": 9.335760944160521e-05,
      "loss": 2.2661,
      "step": 22550
    },
    {
      "epoch": 0.3326317648617223,
      "grad_norm": 2.333327531814575,
      "learning_rate": 9.332879441947473e-05,
      "loss": 2.243,
      "step": 22600
    },
    {
      "epoch": 0.3333676758459297,
      "grad_norm": 1.9845707416534424,
      "learning_rate": 9.329992149890168e-05,
      "loss": 2.2795,
      "step": 22650
    },
    {
      "epoch": 0.334103586830137,
      "grad_norm": 2.0898332595825195,
      "learning_rate": 9.327099071846775e-05,
      "loss": 2.2408,
      "step": 22700
    },
    {
      "epoch": 0.33483949781434436,
      "grad_norm": 2.072685480117798,
      "learning_rate": 9.324200211683188e-05,
      "loss": 2.2739,
      "step": 22750
    },
    {
      "epoch": 0.33557540879855174,
      "grad_norm": 2.132613182067871,
      "learning_rate": 9.321295573273037e-05,
      "loss": 2.2839,
      "step": 22800
    },
    {
      "epoch": 0.3363113197827591,
      "grad_norm": 2.0985190868377686,
      "learning_rate": 9.318385160497662e-05,
      "loss": 2.2418,
      "step": 22850
    },
    {
      "epoch": 0.3370472307669664,
      "grad_norm": 1.9109199047088623,
      "learning_rate": 9.315468977246131e-05,
      "loss": 2.3302,
      "step": 22900
    },
    {
      "epoch": 0.3377831417511738,
      "grad_norm": 2.021671772003174,
      "learning_rate": 9.312547027415215e-05,
      "loss": 2.3048,
      "step": 22950
    },
    {
      "epoch": 0.33851905273538113,
      "grad_norm": 1.9856067895889282,
      "learning_rate": 9.30961931490939e-05,
      "loss": 2.2622,
      "step": 23000
    },
    {
      "epoch": 0.33925496371958846,
      "grad_norm": 2.389232635498047,
      "learning_rate": 9.306685843640836e-05,
      "loss": 2.2052,
      "step": 23050
    },
    {
      "epoch": 0.33999087470379585,
      "grad_norm": 2.569343090057373,
      "learning_rate": 9.30374661752943e-05,
      "loss": 2.2277,
      "step": 23100
    },
    {
      "epoch": 0.3407267856880032,
      "grad_norm": 2.1812150478363037,
      "learning_rate": 9.300801640502731e-05,
      "loss": 2.2472,
      "step": 23150
    },
    {
      "epoch": 0.3414626966722105,
      "grad_norm": 2.3427045345306396,
      "learning_rate": 9.297850916495991e-05,
      "loss": 2.305,
      "step": 23200
    },
    {
      "epoch": 0.34219860765641785,
      "grad_norm": 2.2554006576538086,
      "learning_rate": 9.294894449452139e-05,
      "loss": 2.2355,
      "step": 23250
    },
    {
      "epoch": 0.34293451864062524,
      "grad_norm": 2.1705799102783203,
      "learning_rate": 9.291932243321774e-05,
      "loss": 2.2948,
      "step": 23300
    },
    {
      "epoch": 0.3436704296248326,
      "grad_norm": 1.7825343608856201,
      "learning_rate": 9.28896430206317e-05,
      "loss": 2.252,
      "step": 23350
    },
    {
      "epoch": 0.3444063406090399,
      "grad_norm": 1.9378161430358887,
      "learning_rate": 9.285990629642261e-05,
      "loss": 2.2314,
      "step": 23400
    },
    {
      "epoch": 0.3451422515932473,
      "grad_norm": 2.033207654953003,
      "learning_rate": 9.283011230032639e-05,
      "loss": 2.2549,
      "step": 23450
    },
    {
      "epoch": 0.34587816257745463,
      "grad_norm": 1.7816447019577026,
      "learning_rate": 9.280026107215552e-05,
      "loss": 2.2495,
      "step": 23500
    },
    {
      "epoch": 0.34661407356166196,
      "grad_norm": 1.814902663230896,
      "learning_rate": 9.277035265179894e-05,
      "loss": 2.2771,
      "step": 23550
    },
    {
      "epoch": 0.34734998454586935,
      "grad_norm": 2.40749192237854,
      "learning_rate": 9.2740387079222e-05,
      "loss": 2.1894,
      "step": 23600
    },
    {
      "epoch": 0.3480858955300767,
      "grad_norm": 1.9209364652633667,
      "learning_rate": 9.271036439446644e-05,
      "loss": 2.3037,
      "step": 23650
    },
    {
      "epoch": 0.348821806514284,
      "grad_norm": 2.4775044918060303,
      "learning_rate": 9.268028463765028e-05,
      "loss": 2.2675,
      "step": 23700
    },
    {
      "epoch": 0.3495577174984914,
      "grad_norm": 1.9742401838302612,
      "learning_rate": 9.265014784896788e-05,
      "loss": 2.215,
      "step": 23750
    },
    {
      "epoch": 0.35029362848269874,
      "grad_norm": 2.0051817893981934,
      "learning_rate": 9.261995406868974e-05,
      "loss": 2.2231,
      "step": 23800
    },
    {
      "epoch": 0.35102953946690607,
      "grad_norm": 2.1605944633483887,
      "learning_rate": 9.258970333716252e-05,
      "loss": 2.2443,
      "step": 23850
    },
    {
      "epoch": 0.35176545045111346,
      "grad_norm": 2.3470258712768555,
      "learning_rate": 9.255939569480902e-05,
      "loss": 2.1862,
      "step": 23900
    },
    {
      "epoch": 0.3525013614353208,
      "grad_norm": 1.9757652282714844,
      "learning_rate": 9.252903118212805e-05,
      "loss": 2.2586,
      "step": 23950
    },
    {
      "epoch": 0.3532372724195281,
      "grad_norm": 2.7820072174072266,
      "learning_rate": 9.249860983969444e-05,
      "loss": 2.1747,
      "step": 24000
    },
    {
      "epoch": 0.35397318340373546,
      "grad_norm": 2.1009387969970703,
      "learning_rate": 9.246813170815898e-05,
      "loss": 2.2437,
      "step": 24050
    },
    {
      "epoch": 0.35470909438794285,
      "grad_norm": 2.291661500930786,
      "learning_rate": 9.243759682824824e-05,
      "loss": 2.2241,
      "step": 24100
    },
    {
      "epoch": 0.3554450053721502,
      "grad_norm": 2.252145528793335,
      "learning_rate": 9.240700524076474e-05,
      "loss": 2.2464,
      "step": 24150
    },
    {
      "epoch": 0.3561809163563575,
      "grad_norm": 2.3370378017425537,
      "learning_rate": 9.237635698658673e-05,
      "loss": 2.2507,
      "step": 24200
    },
    {
      "epoch": 0.3569168273405649,
      "grad_norm": 2.0645534992218018,
      "learning_rate": 9.234565210666817e-05,
      "loss": 2.2215,
      "step": 24250
    },
    {
      "epoch": 0.35765273832477223,
      "grad_norm": 2.207469940185547,
      "learning_rate": 9.231489064203869e-05,
      "loss": 2.1881,
      "step": 24300
    },
    {
      "epoch": 0.35838864930897957,
      "grad_norm": 1.9919884204864502,
      "learning_rate": 9.228407263380355e-05,
      "loss": 2.251,
      "step": 24350
    },
    {
      "epoch": 0.35912456029318696,
      "grad_norm": 2.30875825881958,
      "learning_rate": 9.225319812314357e-05,
      "loss": 2.2416,
      "step": 24400
    },
    {
      "epoch": 0.3598604712773943,
      "grad_norm": 1.8778297901153564,
      "learning_rate": 9.222226715131503e-05,
      "loss": 2.2398,
      "step": 24450
    },
    {
      "epoch": 0.3605963822616016,
      "grad_norm": 2.0935099124908447,
      "learning_rate": 9.219127975964969e-05,
      "loss": 2.2312,
      "step": 24500
    },
    {
      "epoch": 0.361332293245809,
      "grad_norm": 2.2428886890411377,
      "learning_rate": 9.216023598955472e-05,
      "loss": 2.2382,
      "step": 24550
    },
    {
      "epoch": 0.36206820423001634,
      "grad_norm": 1.8153061866760254,
      "learning_rate": 9.212913588251258e-05,
      "loss": 2.297,
      "step": 24600
    },
    {
      "epoch": 0.3628041152142237,
      "grad_norm": 2.0361249446868896,
      "learning_rate": 9.209797948008104e-05,
      "loss": 2.287,
      "step": 24650
    },
    {
      "epoch": 0.36354002619843107,
      "grad_norm": 2.4235169887542725,
      "learning_rate": 9.206676682389309e-05,
      "loss": 2.2407,
      "step": 24700
    },
    {
      "epoch": 0.3642759371826384,
      "grad_norm": 2.0263304710388184,
      "learning_rate": 9.203549795565687e-05,
      "loss": 2.2062,
      "step": 24750
    },
    {
      "epoch": 0.36501184816684573,
      "grad_norm": 2.0583112239837646,
      "learning_rate": 9.200417291715569e-05,
      "loss": 2.2298,
      "step": 24800
    },
    {
      "epoch": 0.36574775915105306,
      "grad_norm": 2.1388983726501465,
      "learning_rate": 9.197279175024785e-05,
      "loss": 2.2211,
      "step": 24850
    },
    {
      "epoch": 0.36648367013526045,
      "grad_norm": 1.7047404050827026,
      "learning_rate": 9.194135449686667e-05,
      "loss": 2.1282,
      "step": 24900
    },
    {
      "epoch": 0.3672195811194678,
      "grad_norm": 2.088291883468628,
      "learning_rate": 9.190986119902045e-05,
      "loss": 2.2528,
      "step": 24950
    },
    {
      "epoch": 0.3679554921036751,
      "grad_norm": 2.19598388671875,
      "learning_rate": 9.187831189879235e-05,
      "loss": 2.2433,
      "step": 25000
    },
    {
      "epoch": 0.3686914030878825,
      "grad_norm": 2.210606813430786,
      "learning_rate": 9.184670663834036e-05,
      "loss": 2.206,
      "step": 25050
    },
    {
      "epoch": 0.36942731407208984,
      "grad_norm": 1.9691554307937622,
      "learning_rate": 9.181504545989728e-05,
      "loss": 2.2484,
      "step": 25100
    },
    {
      "epoch": 0.3701632250562972,
      "grad_norm": 1.9752286672592163,
      "learning_rate": 9.178332840577061e-05,
      "loss": 2.2666,
      "step": 25150
    },
    {
      "epoch": 0.37089913604050456,
      "grad_norm": 2.067173957824707,
      "learning_rate": 9.175155551834249e-05,
      "loss": 2.2462,
      "step": 25200
    },
    {
      "epoch": 0.3716350470247119,
      "grad_norm": 2.023294448852539,
      "learning_rate": 9.171972684006969e-05,
      "loss": 2.1889,
      "step": 25250
    },
    {
      "epoch": 0.37237095800891923,
      "grad_norm": 2.1349902153015137,
      "learning_rate": 9.168784241348357e-05,
      "loss": 2.1995,
      "step": 25300
    },
    {
      "epoch": 0.3731068689931266,
      "grad_norm": 2.1011674404144287,
      "learning_rate": 9.16559022811899e-05,
      "loss": 2.2448,
      "step": 25350
    },
    {
      "epoch": 0.37384277997733395,
      "grad_norm": 2.0974037647247314,
      "learning_rate": 9.162390648586897e-05,
      "loss": 2.1623,
      "step": 25400
    },
    {
      "epoch": 0.3745786909615413,
      "grad_norm": 2.1744704246520996,
      "learning_rate": 9.159185507027537e-05,
      "loss": 2.269,
      "step": 25450
    },
    {
      "epoch": 0.3753146019457486,
      "grad_norm": 2.5077755451202393,
      "learning_rate": 9.155974807723809e-05,
      "loss": 2.2082,
      "step": 25500
    },
    {
      "epoch": 0.376050512929956,
      "grad_norm": 2.4667510986328125,
      "learning_rate": 9.152758554966036e-05,
      "loss": 2.2105,
      "step": 25550
    },
    {
      "epoch": 0.37678642391416334,
      "grad_norm": 1.9539825916290283,
      "learning_rate": 9.149536753051959e-05,
      "loss": 2.2507,
      "step": 25600
    },
    {
      "epoch": 0.37752233489837067,
      "grad_norm": 1.7640643119812012,
      "learning_rate": 9.146309406286736e-05,
      "loss": 2.2486,
      "step": 25650
    },
    {
      "epoch": 0.37825824588257806,
      "grad_norm": 1.9569494724273682,
      "learning_rate": 9.143076518982936e-05,
      "loss": 2.1921,
      "step": 25700
    },
    {
      "epoch": 0.3789941568667854,
      "grad_norm": 2.0282394886016846,
      "learning_rate": 9.139838095460529e-05,
      "loss": 2.25,
      "step": 25750
    },
    {
      "epoch": 0.3797300678509927,
      "grad_norm": 2.1371195316314697,
      "learning_rate": 9.136594140046884e-05,
      "loss": 2.227,
      "step": 25800
    },
    {
      "epoch": 0.3804659788352001,
      "grad_norm": 2.2563750743865967,
      "learning_rate": 9.133344657076762e-05,
      "loss": 2.1584,
      "step": 25850
    },
    {
      "epoch": 0.38120188981940745,
      "grad_norm": 2.349041223526001,
      "learning_rate": 9.130089650892311e-05,
      "loss": 2.284,
      "step": 25900
    },
    {
      "epoch": 0.3819378008036148,
      "grad_norm": 2.4154913425445557,
      "learning_rate": 9.126829125843058e-05,
      "loss": 2.2007,
      "step": 25950
    },
    {
      "epoch": 0.38267371178782217,
      "grad_norm": 1.9608532190322876,
      "learning_rate": 9.123563086285904e-05,
      "loss": 2.1981,
      "step": 26000
    },
    {
      "epoch": 0.3834096227720295,
      "grad_norm": 2.098923921585083,
      "learning_rate": 9.120291536585124e-05,
      "loss": 2.249,
      "step": 26050
    },
    {
      "epoch": 0.38414553375623683,
      "grad_norm": 2.377398729324341,
      "learning_rate": 9.117014481112347e-05,
      "loss": 2.2811,
      "step": 26100
    },
    {
      "epoch": 0.3848814447404442,
      "grad_norm": 2.0152504444122314,
      "learning_rate": 9.113731924246567e-05,
      "loss": 2.2458,
      "step": 26150
    },
    {
      "epoch": 0.38561735572465156,
      "grad_norm": 2.1423943042755127,
      "learning_rate": 9.110443870374126e-05,
      "loss": 2.1782,
      "step": 26200
    },
    {
      "epoch": 0.3863532667088589,
      "grad_norm": 2.003206968307495,
      "learning_rate": 9.107150323888714e-05,
      "loss": 2.239,
      "step": 26250
    },
    {
      "epoch": 0.3870891776930662,
      "grad_norm": 2.1856625080108643,
      "learning_rate": 9.103851289191354e-05,
      "loss": 2.185,
      "step": 26300
    },
    {
      "epoch": 0.3878250886772736,
      "grad_norm": 2.1581602096557617,
      "learning_rate": 9.10054677069041e-05,
      "loss": 2.2581,
      "step": 26350
    },
    {
      "epoch": 0.38856099966148094,
      "grad_norm": 2.2691991329193115,
      "learning_rate": 9.097236772801569e-05,
      "loss": 2.2497,
      "step": 26400
    },
    {
      "epoch": 0.3892969106456883,
      "grad_norm": 1.7650847434997559,
      "learning_rate": 9.093921299947845e-05,
      "loss": 2.2035,
      "step": 26450
    },
    {
      "epoch": 0.39003282162989567,
      "grad_norm": 2.054873466491699,
      "learning_rate": 9.090600356559563e-05,
      "loss": 2.2431,
      "step": 26500
    },
    {
      "epoch": 0.390768732614103,
      "grad_norm": 1.9138387441635132,
      "learning_rate": 9.087273947074357e-05,
      "loss": 2.1654,
      "step": 26550
    },
    {
      "epoch": 0.39150464359831033,
      "grad_norm": 2.376908779144287,
      "learning_rate": 9.083942075937172e-05,
      "loss": 2.2072,
      "step": 26600
    },
    {
      "epoch": 0.3922405545825177,
      "grad_norm": 2.164238929748535,
      "learning_rate": 9.080604747600244e-05,
      "loss": 2.2122,
      "step": 26650
    },
    {
      "epoch": 0.39297646556672505,
      "grad_norm": 2.0409247875213623,
      "learning_rate": 9.077261966523108e-05,
      "loss": 2.2078,
      "step": 26700
    },
    {
      "epoch": 0.3937123765509324,
      "grad_norm": 2.3029751777648926,
      "learning_rate": 9.073913737172576e-05,
      "loss": 2.2427,
      "step": 26750
    },
    {
      "epoch": 0.3944482875351398,
      "grad_norm": 1.9332245588302612,
      "learning_rate": 9.070560064022749e-05,
      "loss": 2.1671,
      "step": 26800
    },
    {
      "epoch": 0.3951841985193471,
      "grad_norm": 2.218604326248169,
      "learning_rate": 9.067200951555001e-05,
      "loss": 2.1892,
      "step": 26850
    },
    {
      "epoch": 0.39592010950355444,
      "grad_norm": 2.4321401119232178,
      "learning_rate": 9.06383640425797e-05,
      "loss": 2.2016,
      "step": 26900
    },
    {
      "epoch": 0.3966560204877618,
      "grad_norm": 2.2820937633514404,
      "learning_rate": 9.060466426627559e-05,
      "loss": 2.227,
      "step": 26950
    },
    {
      "epoch": 0.39739193147196916,
      "grad_norm": 2.6003787517547607,
      "learning_rate": 9.057091023166928e-05,
      "loss": 2.1981,
      "step": 27000
    },
    {
      "epoch": 0.3981278424561765,
      "grad_norm": 2.403224468231201,
      "learning_rate": 9.053710198386485e-05,
      "loss": 2.264,
      "step": 27050
    },
    {
      "epoch": 0.39886375344038383,
      "grad_norm": 2.22759747505188,
      "learning_rate": 9.050323956803884e-05,
      "loss": 2.1788,
      "step": 27100
    },
    {
      "epoch": 0.3995996644245912,
      "grad_norm": 1.9465980529785156,
      "learning_rate": 9.046932302944018e-05,
      "loss": 2.2032,
      "step": 27150
    },
    {
      "epoch": 0.40033557540879855,
      "grad_norm": 1.9770467281341553,
      "learning_rate": 9.043535241339012e-05,
      "loss": 2.1614,
      "step": 27200
    },
    {
      "epoch": 0.4010714863930059,
      "grad_norm": 1.8155477046966553,
      "learning_rate": 9.040132776528214e-05,
      "loss": 2.1318,
      "step": 27250
    },
    {
      "epoch": 0.40180739737721327,
      "grad_norm": 2.0355663299560547,
      "learning_rate": 9.036724913058194e-05,
      "loss": 2.2189,
      "step": 27300
    },
    {
      "epoch": 0.4025433083614206,
      "grad_norm": 2.0527801513671875,
      "learning_rate": 9.03331165548274e-05,
      "loss": 2.2176,
      "step": 27350
    },
    {
      "epoch": 0.40327921934562794,
      "grad_norm": 2.1048474311828613,
      "learning_rate": 9.029893008362842e-05,
      "loss": 2.2129,
      "step": 27400
    },
    {
      "epoch": 0.4040151303298353,
      "grad_norm": 2.055809736251831,
      "learning_rate": 9.026468976266693e-05,
      "loss": 2.1071,
      "step": 27450
    },
    {
      "epoch": 0.40475104131404266,
      "grad_norm": 2.1983630657196045,
      "learning_rate": 9.023039563769683e-05,
      "loss": 2.2109,
      "step": 27500
    },
    {
      "epoch": 0.40548695229825,
      "grad_norm": 2.1028950214385986,
      "learning_rate": 9.019604775454396e-05,
      "loss": 2.2142,
      "step": 27550
    },
    {
      "epoch": 0.4062228632824574,
      "grad_norm": 2.0235049724578857,
      "learning_rate": 9.01616461591059e-05,
      "loss": 2.1891,
      "step": 27600
    },
    {
      "epoch": 0.4069587742666647,
      "grad_norm": 1.9378324747085571,
      "learning_rate": 9.012719089735206e-05,
      "loss": 2.1916,
      "step": 27650
    },
    {
      "epoch": 0.40769468525087205,
      "grad_norm": 2.6311309337615967,
      "learning_rate": 9.009268201532356e-05,
      "loss": 2.1272,
      "step": 27700
    },
    {
      "epoch": 0.4084305962350794,
      "grad_norm": 1.9121609926223755,
      "learning_rate": 9.005811955913317e-05,
      "loss": 2.202,
      "step": 27750
    },
    {
      "epoch": 0.40916650721928677,
      "grad_norm": 2.058168411254883,
      "learning_rate": 9.002350357496524e-05,
      "loss": 2.1786,
      "step": 27800
    },
    {
      "epoch": 0.4099024182034941,
      "grad_norm": 2.2722280025482178,
      "learning_rate": 8.998883410907565e-05,
      "loss": 2.1373,
      "step": 27850
    },
    {
      "epoch": 0.41063832918770143,
      "grad_norm": 2.6041009426116943,
      "learning_rate": 8.995411120779175e-05,
      "loss": 2.1969,
      "step": 27900
    },
    {
      "epoch": 0.4113742401719088,
      "grad_norm": 2.233905076980591,
      "learning_rate": 8.99193349175123e-05,
      "loss": 2.1893,
      "step": 27950
    },
    {
      "epoch": 0.41211015115611616,
      "grad_norm": 1.9736270904541016,
      "learning_rate": 8.988450528470735e-05,
      "loss": 2.1707,
      "step": 28000
    },
    {
      "epoch": 0.4128460621403235,
      "grad_norm": 1.9319082498550415,
      "learning_rate": 8.984962235591833e-05,
      "loss": 2.1669,
      "step": 28050
    },
    {
      "epoch": 0.4135819731245309,
      "grad_norm": 2.227762222290039,
      "learning_rate": 8.981468617775779e-05,
      "loss": 2.2067,
      "step": 28100
    },
    {
      "epoch": 0.4143178841087382,
      "grad_norm": 2.0830914974212646,
      "learning_rate": 8.97796967969095e-05,
      "loss": 2.2047,
      "step": 28150
    },
    {
      "epoch": 0.41505379509294554,
      "grad_norm": 2.035947561264038,
      "learning_rate": 8.974465426012825e-05,
      "loss": 2.1717,
      "step": 28200
    },
    {
      "epoch": 0.41578970607715293,
      "grad_norm": 2.3488736152648926,
      "learning_rate": 8.970955861423995e-05,
      "loss": 2.1622,
      "step": 28250
    },
    {
      "epoch": 0.41652561706136026,
      "grad_norm": 2.122798442840576,
      "learning_rate": 8.967440990614141e-05,
      "loss": 2.1397,
      "step": 28300
    },
    {
      "epoch": 0.4172615280455676,
      "grad_norm": 2.567375421524048,
      "learning_rate": 8.963920818280039e-05,
      "loss": 2.159,
      "step": 28350
    },
    {
      "epoch": 0.41799743902977493,
      "grad_norm": 2.210273504257202,
      "learning_rate": 8.960395349125544e-05,
      "loss": 2.182,
      "step": 28400
    },
    {
      "epoch": 0.4187333500139823,
      "grad_norm": 2.070699691772461,
      "learning_rate": 8.956864587861594e-05,
      "loss": 2.1676,
      "step": 28450
    },
    {
      "epoch": 0.41946926099818965,
      "grad_norm": 2.1614022254943848,
      "learning_rate": 8.953328539206194e-05,
      "loss": 2.1719,
      "step": 28500
    },
    {
      "epoch": 0.420205171982397,
      "grad_norm": 1.961956262588501,
      "learning_rate": 8.94978720788442e-05,
      "loss": 2.1679,
      "step": 28550
    },
    {
      "epoch": 0.4209410829666044,
      "grad_norm": 2.052229642868042,
      "learning_rate": 8.946240598628402e-05,
      "loss": 2.1915,
      "step": 28600
    },
    {
      "epoch": 0.4216769939508117,
      "grad_norm": 1.954079508781433,
      "learning_rate": 8.942688716177325e-05,
      "loss": 2.1604,
      "step": 28650
    },
    {
      "epoch": 0.42241290493501904,
      "grad_norm": 2.462967872619629,
      "learning_rate": 8.939131565277422e-05,
      "loss": 2.1895,
      "step": 28700
    },
    {
      "epoch": 0.42314881591922643,
      "grad_norm": 1.812936782836914,
      "learning_rate": 8.935569150681961e-05,
      "loss": 2.1495,
      "step": 28750
    },
    {
      "epoch": 0.42388472690343376,
      "grad_norm": 2.2551043033599854,
      "learning_rate": 8.932001477151246e-05,
      "loss": 2.1346,
      "step": 28800
    },
    {
      "epoch": 0.4246206378876411,
      "grad_norm": 2.331747055053711,
      "learning_rate": 8.928428549452613e-05,
      "loss": 2.1942,
      "step": 28850
    },
    {
      "epoch": 0.4253565488718485,
      "grad_norm": 1.9211674928665161,
      "learning_rate": 8.92485037236041e-05,
      "loss": 2.1294,
      "step": 28900
    },
    {
      "epoch": 0.4260924598560558,
      "grad_norm": 1.906463623046875,
      "learning_rate": 8.921266950656007e-05,
      "loss": 2.1823,
      "step": 28950
    },
    {
      "epoch": 0.42682837084026315,
      "grad_norm": 2.2019801139831543,
      "learning_rate": 8.917678289127778e-05,
      "loss": 2.1942,
      "step": 29000
    },
    {
      "epoch": 0.42756428182447054,
      "grad_norm": 2.179785966873169,
      "learning_rate": 8.9140843925711e-05,
      "loss": 2.1692,
      "step": 29050
    },
    {
      "epoch": 0.42830019280867787,
      "grad_norm": 2.312124729156494,
      "learning_rate": 8.910485265788349e-05,
      "loss": 2.1326,
      "step": 29100
    },
    {
      "epoch": 0.4290361037928852,
      "grad_norm": 2.1604912281036377,
      "learning_rate": 8.906880913588882e-05,
      "loss": 2.09,
      "step": 29150
    },
    {
      "epoch": 0.42977201477709254,
      "grad_norm": 2.3887476921081543,
      "learning_rate": 8.903271340789047e-05,
      "loss": 2.1778,
      "step": 29200
    },
    {
      "epoch": 0.4305079257612999,
      "grad_norm": 2.3067009449005127,
      "learning_rate": 8.899656552212162e-05,
      "loss": 2.2009,
      "step": 29250
    },
    {
      "epoch": 0.43124383674550726,
      "grad_norm": 2.015838384628296,
      "learning_rate": 8.896036552688514e-05,
      "loss": 2.1657,
      "step": 29300
    },
    {
      "epoch": 0.4319797477297146,
      "grad_norm": 2.0537426471710205,
      "learning_rate": 8.892411347055361e-05,
      "loss": 2.1947,
      "step": 29350
    },
    {
      "epoch": 0.432715658713922,
      "grad_norm": 2.5637550354003906,
      "learning_rate": 8.888780940156909e-05,
      "loss": 2.1685,
      "step": 29400
    },
    {
      "epoch": 0.4334515696981293,
      "grad_norm": 2.0522172451019287,
      "learning_rate": 8.88514533684432e-05,
      "loss": 2.139,
      "step": 29450
    },
    {
      "epoch": 0.43418748068233665,
      "grad_norm": 2.3139874935150146,
      "learning_rate": 8.881504541975695e-05,
      "loss": 2.1268,
      "step": 29500
    },
    {
      "epoch": 0.43492339166654403,
      "grad_norm": 2.2748074531555176,
      "learning_rate": 8.87785856041608e-05,
      "loss": 2.1392,
      "step": 29550
    },
    {
      "epoch": 0.43565930265075137,
      "grad_norm": 2.228641986846924,
      "learning_rate": 8.87420739703744e-05,
      "loss": 2.1753,
      "step": 29600
    },
    {
      "epoch": 0.4363952136349587,
      "grad_norm": 2.2028989791870117,
      "learning_rate": 8.870551056718673e-05,
      "loss": 2.1453,
      "step": 29650
    },
    {
      "epoch": 0.4371311246191661,
      "grad_norm": 2.0355224609375,
      "learning_rate": 8.866889544345594e-05,
      "loss": 2.0932,
      "step": 29700
    },
    {
      "epoch": 0.4378670356033734,
      "grad_norm": 2.0021250247955322,
      "learning_rate": 8.863222864810928e-05,
      "loss": 2.1305,
      "step": 29750
    },
    {
      "epoch": 0.43860294658758076,
      "grad_norm": 2.3766427040100098,
      "learning_rate": 8.859551023014301e-05,
      "loss": 2.1357,
      "step": 29800
    },
    {
      "epoch": 0.43933885757178814,
      "grad_norm": 2.2551889419555664,
      "learning_rate": 8.855874023862245e-05,
      "loss": 2.1367,
      "step": 29850
    },
    {
      "epoch": 0.4400747685559955,
      "grad_norm": 2.1025102138519287,
      "learning_rate": 8.852191872268175e-05,
      "loss": 2.1123,
      "step": 29900
    },
    {
      "epoch": 0.4408106795402028,
      "grad_norm": 1.9039511680603027,
      "learning_rate": 8.848504573152399e-05,
      "loss": 2.2138,
      "step": 29950
    },
    {
      "epoch": 0.44154659052441014,
      "grad_norm": 2.0255465507507324,
      "learning_rate": 8.844812131442096e-05,
      "loss": 2.1934,
      "step": 30000
    },
    {
      "epoch": 0.44228250150861753,
      "grad_norm": 2.1081833839416504,
      "learning_rate": 8.841114552071321e-05,
      "loss": 2.1237,
      "step": 30050
    },
    {
      "epoch": 0.44301841249282486,
      "grad_norm": 2.371795415878296,
      "learning_rate": 8.837411839980995e-05,
      "loss": 2.1329,
      "step": 30100
    },
    {
      "epoch": 0.4437543234770322,
      "grad_norm": 2.148668050765991,
      "learning_rate": 8.833704000118897e-05,
      "loss": 2.1787,
      "step": 30150
    },
    {
      "epoch": 0.4444902344612396,
      "grad_norm": 2.260970115661621,
      "learning_rate": 8.829991037439653e-05,
      "loss": 2.0893,
      "step": 30200
    },
    {
      "epoch": 0.4452261454454469,
      "grad_norm": 2.0189125537872314,
      "learning_rate": 8.826272956904744e-05,
      "loss": 2.1699,
      "step": 30250
    },
    {
      "epoch": 0.44596205642965425,
      "grad_norm": 2.1248254776000977,
      "learning_rate": 8.822549763482479e-05,
      "loss": 2.1372,
      "step": 30300
    },
    {
      "epoch": 0.44669796741386164,
      "grad_norm": 2.808443069458008,
      "learning_rate": 8.818821462148008e-05,
      "loss": 2.1696,
      "step": 30350
    },
    {
      "epoch": 0.447433878398069,
      "grad_norm": 2.065110445022583,
      "learning_rate": 8.815088057883302e-05,
      "loss": 2.1994,
      "step": 30400
    },
    {
      "epoch": 0.4481697893822763,
      "grad_norm": 2.2882895469665527,
      "learning_rate": 8.811349555677154e-05,
      "loss": 2.1167,
      "step": 30450
    },
    {
      "epoch": 0.4489057003664837,
      "grad_norm": 2.152765989303589,
      "learning_rate": 8.807605960525162e-05,
      "loss": 2.1701,
      "step": 30500
    },
    {
      "epoch": 0.44964161135069103,
      "grad_norm": 2.182830810546875,
      "learning_rate": 8.803857277429738e-05,
      "loss": 2.1642,
      "step": 30550
    },
    {
      "epoch": 0.45037752233489836,
      "grad_norm": 1.9579511880874634,
      "learning_rate": 8.800103511400089e-05,
      "loss": 2.087,
      "step": 30600
    },
    {
      "epoch": 0.4511134333191057,
      "grad_norm": 1.9936952590942383,
      "learning_rate": 8.796344667452214e-05,
      "loss": 2.1163,
      "step": 30650
    },
    {
      "epoch": 0.4518493443033131,
      "grad_norm": 2.1936371326446533,
      "learning_rate": 8.792580750608898e-05,
      "loss": 2.1193,
      "step": 30700
    },
    {
      "epoch": 0.4525852552875204,
      "grad_norm": 2.519608497619629,
      "learning_rate": 8.788811765899704e-05,
      "loss": 2.2177,
      "step": 30750
    },
    {
      "epoch": 0.45332116627172775,
      "grad_norm": 2.2622578144073486,
      "learning_rate": 8.785037718360968e-05,
      "loss": 2.0889,
      "step": 30800
    },
    {
      "epoch": 0.45405707725593514,
      "grad_norm": 2.4669601917266846,
      "learning_rate": 8.781258613035792e-05,
      "loss": 2.1653,
      "step": 30850
    },
    {
      "epoch": 0.45479298824014247,
      "grad_norm": 2.0421276092529297,
      "learning_rate": 8.777474454974033e-05,
      "loss": 2.0873,
      "step": 30900
    },
    {
      "epoch": 0.4555288992243498,
      "grad_norm": 2.0550615787506104,
      "learning_rate": 8.773685249232302e-05,
      "loss": 2.1751,
      "step": 30950
    },
    {
      "epoch": 0.4562648102085572,
      "grad_norm": 2.5711066722869873,
      "learning_rate": 8.769891000873956e-05,
      "loss": 2.1595,
      "step": 31000
    },
    {
      "epoch": 0.4570007211927645,
      "grad_norm": 2.3787131309509277,
      "learning_rate": 8.766091714969091e-05,
      "loss": 2.1378,
      "step": 31050
    },
    {
      "epoch": 0.45773663217697186,
      "grad_norm": 1.921814203262329,
      "learning_rate": 8.762287396594529e-05,
      "loss": 2.1461,
      "step": 31100
    },
    {
      "epoch": 0.45847254316117925,
      "grad_norm": 2.229024887084961,
      "learning_rate": 8.758478050833821e-05,
      "loss": 2.2136,
      "step": 31150
    },
    {
      "epoch": 0.4592084541453866,
      "grad_norm": 1.9162590503692627,
      "learning_rate": 8.754663682777239e-05,
      "loss": 2.077,
      "step": 31200
    },
    {
      "epoch": 0.4599443651295939,
      "grad_norm": 2.190894365310669,
      "learning_rate": 8.750844297521759e-05,
      "loss": 2.135,
      "step": 31250
    },
    {
      "epoch": 0.4606802761138013,
      "grad_norm": 1.795032262802124,
      "learning_rate": 8.747019900171065e-05,
      "loss": 2.1679,
      "step": 31300
    },
    {
      "epoch": 0.46141618709800863,
      "grad_norm": 2.4830162525177,
      "learning_rate": 8.743190495835539e-05,
      "loss": 2.1068,
      "step": 31350
    },
    {
      "epoch": 0.46215209808221597,
      "grad_norm": 2.3451790809631348,
      "learning_rate": 8.73935608963225e-05,
      "loss": 2.1764,
      "step": 31400
    },
    {
      "epoch": 0.4628880090664233,
      "grad_norm": 1.9488446712493896,
      "learning_rate": 8.735516686684957e-05,
      "loss": 2.1409,
      "step": 31450
    },
    {
      "epoch": 0.4636239200506307,
      "grad_norm": 1.9788676500320435,
      "learning_rate": 8.73167229212409e-05,
      "loss": 2.1052,
      "step": 31500
    },
    {
      "epoch": 0.464359831034838,
      "grad_norm": 1.7903550863265991,
      "learning_rate": 8.727822911086754e-05,
      "loss": 2.1072,
      "step": 31550
    },
    {
      "epoch": 0.46509574201904536,
      "grad_norm": 2.2248623371124268,
      "learning_rate": 8.723968548716712e-05,
      "loss": 2.2174,
      "step": 31600
    },
    {
      "epoch": 0.46583165300325274,
      "grad_norm": 1.838417410850525,
      "learning_rate": 8.720109210164386e-05,
      "loss": 2.1442,
      "step": 31650
    },
    {
      "epoch": 0.4665675639874601,
      "grad_norm": 2.4228804111480713,
      "learning_rate": 8.716244900586851e-05,
      "loss": 2.1682,
      "step": 31700
    },
    {
      "epoch": 0.4673034749716674,
      "grad_norm": 2.2023301124572754,
      "learning_rate": 8.71237562514782e-05,
      "loss": 2.0841,
      "step": 31750
    },
    {
      "epoch": 0.4680393859558748,
      "grad_norm": 2.347646713256836,
      "learning_rate": 8.708501389017639e-05,
      "loss": 2.1043,
      "step": 31800
    },
    {
      "epoch": 0.46877529694008213,
      "grad_norm": 2.2148795127868652,
      "learning_rate": 8.704622197373289e-05,
      "loss": 2.1785,
      "step": 31850
    },
    {
      "epoch": 0.46951120792428946,
      "grad_norm": 2.5026378631591797,
      "learning_rate": 8.700738055398371e-05,
      "loss": 2.1042,
      "step": 31900
    },
    {
      "epoch": 0.47024711890849685,
      "grad_norm": 2.22243595123291,
      "learning_rate": 8.6968489682831e-05,
      "loss": 2.1272,
      "step": 31950
    },
    {
      "epoch": 0.4709830298927042,
      "grad_norm": 2.0230164527893066,
      "learning_rate": 8.692954941224299e-05,
      "loss": 2.0903,
      "step": 32000
    },
    {
      "epoch": 0.4717189408769115,
      "grad_norm": 2.1936044692993164,
      "learning_rate": 8.689055979425394e-05,
      "loss": 2.1438,
      "step": 32050
    },
    {
      "epoch": 0.47245485186111885,
      "grad_norm": 2.224708080291748,
      "learning_rate": 8.685152088096398e-05,
      "loss": 2.1338,
      "step": 32100
    },
    {
      "epoch": 0.47319076284532624,
      "grad_norm": 2.069003105163574,
      "learning_rate": 8.681243272453923e-05,
      "loss": 2.1099,
      "step": 32150
    },
    {
      "epoch": 0.4739266738295336,
      "grad_norm": 2.2921080589294434,
      "learning_rate": 8.67732953772115e-05,
      "loss": 2.1782,
      "step": 32200
    },
    {
      "epoch": 0.4746625848137409,
      "grad_norm": 1.8213506937026978,
      "learning_rate": 8.673410889127843e-05,
      "loss": 2.1,
      "step": 32250
    },
    {
      "epoch": 0.4753984957979483,
      "grad_norm": 2.041903495788574,
      "learning_rate": 8.66948733191032e-05,
      "loss": 2.1273,
      "step": 32300
    },
    {
      "epoch": 0.47613440678215563,
      "grad_norm": 2.0248711109161377,
      "learning_rate": 8.66555887131147e-05,
      "loss": 2.1285,
      "step": 32350
    },
    {
      "epoch": 0.47687031776636296,
      "grad_norm": 2.1565613746643066,
      "learning_rate": 8.661625512580729e-05,
      "loss": 2.1366,
      "step": 32400
    },
    {
      "epoch": 0.47760622875057035,
      "grad_norm": 3.205812931060791,
      "learning_rate": 8.657687260974076e-05,
      "loss": 2.1437,
      "step": 32450
    },
    {
      "epoch": 0.4783421397347777,
      "grad_norm": 2.127314329147339,
      "learning_rate": 8.653744121754036e-05,
      "loss": 2.1078,
      "step": 32500
    },
    {
      "epoch": 0.479078050718985,
      "grad_norm": 2.2287356853485107,
      "learning_rate": 8.649796100189654e-05,
      "loss": 2.1584,
      "step": 32550
    },
    {
      "epoch": 0.4798139617031924,
      "grad_norm": 2.046814203262329,
      "learning_rate": 8.645843201556508e-05,
      "loss": 2.0877,
      "step": 32600
    },
    {
      "epoch": 0.48054987268739974,
      "grad_norm": 2.1039648056030273,
      "learning_rate": 8.641885431136689e-05,
      "loss": 2.0646,
      "step": 32650
    },
    {
      "epoch": 0.48128578367160707,
      "grad_norm": 1.9940876960754395,
      "learning_rate": 8.6379227942188e-05,
      "loss": 2.1988,
      "step": 32700
    },
    {
      "epoch": 0.48202169465581446,
      "grad_norm": 2.2808706760406494,
      "learning_rate": 8.633955296097944e-05,
      "loss": 2.032,
      "step": 32750
    },
    {
      "epoch": 0.4827576056400218,
      "grad_norm": 1.9945704936981201,
      "learning_rate": 8.629982942075722e-05,
      "loss": 2.067,
      "step": 32800
    },
    {
      "epoch": 0.4834935166242291,
      "grad_norm": 1.9820374250411987,
      "learning_rate": 8.626005737460225e-05,
      "loss": 2.1376,
      "step": 32850
    },
    {
      "epoch": 0.48422942760843646,
      "grad_norm": 2.152782440185547,
      "learning_rate": 8.622023687566024e-05,
      "loss": 2.1632,
      "step": 32900
    },
    {
      "epoch": 0.48496533859264385,
      "grad_norm": 2.296482801437378,
      "learning_rate": 8.61803679771416e-05,
      "loss": 2.1352,
      "step": 32950
    },
    {
      "epoch": 0.4857012495768512,
      "grad_norm": 1.9365371465682983,
      "learning_rate": 8.614045073232151e-05,
      "loss": 2.0536,
      "step": 33000
    },
    {
      "epoch": 0.4864371605610585,
      "grad_norm": 2.0558600425720215,
      "learning_rate": 8.610048519453968e-05,
      "loss": 2.1193,
      "step": 33050
    },
    {
      "epoch": 0.4871730715452659,
      "grad_norm": 2.5881805419921875,
      "learning_rate": 8.606047141720037e-05,
      "loss": 2.0791,
      "step": 33100
    },
    {
      "epoch": 0.48790898252947323,
      "grad_norm": 1.8658931255340576,
      "learning_rate": 8.602040945377233e-05,
      "loss": 2.0426,
      "step": 33150
    },
    {
      "epoch": 0.48864489351368057,
      "grad_norm": 2.037482500076294,
      "learning_rate": 8.598029935778867e-05,
      "loss": 2.1156,
      "step": 33200
    },
    {
      "epoch": 0.48938080449788796,
      "grad_norm": 2.0157525539398193,
      "learning_rate": 8.594014118284677e-05,
      "loss": 2.1268,
      "step": 33250
    },
    {
      "epoch": 0.4901167154820953,
      "grad_norm": 2.0242886543273926,
      "learning_rate": 8.589993498260837e-05,
      "loss": 2.0916,
      "step": 33300
    },
    {
      "epoch": 0.4908526264663026,
      "grad_norm": 1.937922477722168,
      "learning_rate": 8.58596808107993e-05,
      "loss": 2.104,
      "step": 33350
    },
    {
      "epoch": 0.49158853745051,
      "grad_norm": 2.129981279373169,
      "learning_rate": 8.58193787212095e-05,
      "loss": 2.1047,
      "step": 33400
    },
    {
      "epoch": 0.49232444843471734,
      "grad_norm": 2.073364496231079,
      "learning_rate": 8.577902876769299e-05,
      "loss": 2.0597,
      "step": 33450
    },
    {
      "epoch": 0.4930603594189247,
      "grad_norm": 2.0260610580444336,
      "learning_rate": 8.573863100416767e-05,
      "loss": 2.005,
      "step": 33500
    },
    {
      "epoch": 0.49379627040313206,
      "grad_norm": 1.8965402841567993,
      "learning_rate": 8.569818548461538e-05,
      "loss": 2.0864,
      "step": 33550
    },
    {
      "epoch": 0.4945321813873394,
      "grad_norm": 2.112607955932617,
      "learning_rate": 8.56576922630818e-05,
      "loss": 2.1253,
      "step": 33600
    },
    {
      "epoch": 0.49526809237154673,
      "grad_norm": 2.060305118560791,
      "learning_rate": 8.561715139367627e-05,
      "loss": 2.1125,
      "step": 33650
    },
    {
      "epoch": 0.49600400335575406,
      "grad_norm": 2.504913091659546,
      "learning_rate": 8.557656293057188e-05,
      "loss": 2.0498,
      "step": 33700
    },
    {
      "epoch": 0.49673991433996145,
      "grad_norm": 2.1036529541015625,
      "learning_rate": 8.553592692800524e-05,
      "loss": 2.1748,
      "step": 33750
    },
    {
      "epoch": 0.4974758253241688,
      "grad_norm": 1.9860332012176514,
      "learning_rate": 8.54952434402766e-05,
      "loss": 2.1348,
      "step": 33800
    },
    {
      "epoch": 0.4982117363083761,
      "grad_norm": 1.9218943119049072,
      "learning_rate": 8.545451252174951e-05,
      "loss": 2.1255,
      "step": 33850
    },
    {
      "epoch": 0.4989476472925835,
      "grad_norm": 2.0055758953094482,
      "learning_rate": 8.541373422685103e-05,
      "loss": 2.1081,
      "step": 33900
    },
    {
      "epoch": 0.49968355827679084,
      "grad_norm": 1.9770699739456177,
      "learning_rate": 8.537290861007145e-05,
      "loss": 2.1288,
      "step": 33950
    },
    {
      "epoch": 0.5004194692609982,
      "grad_norm": 2.01031231880188,
      "learning_rate": 8.533203572596436e-05,
      "loss": 2.1543,
      "step": 34000
    },
    {
      "epoch": 0.5011553802452056,
      "grad_norm": 2.2340054512023926,
      "learning_rate": 8.529111562914644e-05,
      "loss": 2.067,
      "step": 34050
    },
    {
      "epoch": 0.5018912912294129,
      "grad_norm": 2.1644601821899414,
      "learning_rate": 8.525014837429751e-05,
      "loss": 2.1104,
      "step": 34100
    },
    {
      "epoch": 0.5026272022136202,
      "grad_norm": 2.368288040161133,
      "learning_rate": 8.520913401616036e-05,
      "loss": 2.0459,
      "step": 34150
    },
    {
      "epoch": 0.5033631131978276,
      "grad_norm": 2.0059056282043457,
      "learning_rate": 8.516807260954077e-05,
      "loss": 2.1191,
      "step": 34200
    },
    {
      "epoch": 0.5040990241820349,
      "grad_norm": 2.0404281616210938,
      "learning_rate": 8.512696420930737e-05,
      "loss": 2.175,
      "step": 34250
    },
    {
      "epoch": 0.5048349351662423,
      "grad_norm": 2.100135326385498,
      "learning_rate": 8.508580887039157e-05,
      "loss": 2.0925,
      "step": 34300
    },
    {
      "epoch": 0.5055708461504497,
      "grad_norm": 2.3065309524536133,
      "learning_rate": 8.504460664778753e-05,
      "loss": 2.1275,
      "step": 34350
    },
    {
      "epoch": 0.506306757134657,
      "grad_norm": 1.79512619972229,
      "learning_rate": 8.500335759655205e-05,
      "loss": 2.1594,
      "step": 34400
    },
    {
      "epoch": 0.5070426681188643,
      "grad_norm": 2.1063477993011475,
      "learning_rate": 8.496206177180447e-05,
      "loss": 2.1766,
      "step": 34450
    },
    {
      "epoch": 0.5077785791030717,
      "grad_norm": 1.9706140756607056,
      "learning_rate": 8.492071922872669e-05,
      "loss": 2.0465,
      "step": 34500
    },
    {
      "epoch": 0.508514490087279,
      "grad_norm": 2.10188627243042,
      "learning_rate": 8.487933002256299e-05,
      "loss": 2.0707,
      "step": 34550
    },
    {
      "epoch": 0.5092504010714864,
      "grad_norm": 2.542234182357788,
      "learning_rate": 8.483789420862e-05,
      "loss": 2.0865,
      "step": 34600
    },
    {
      "epoch": 0.5099863120556938,
      "grad_norm": 2.32682466506958,
      "learning_rate": 8.479641184226672e-05,
      "loss": 2.0696,
      "step": 34650
    },
    {
      "epoch": 0.5107222230399011,
      "grad_norm": 1.7952601909637451,
      "learning_rate": 8.475488297893424e-05,
      "loss": 2.1607,
      "step": 34700
    },
    {
      "epoch": 0.5114581340241084,
      "grad_norm": 3.765918731689453,
      "learning_rate": 8.471330767411584e-05,
      "loss": 2.1412,
      "step": 34750
    },
    {
      "epoch": 0.5121940450083158,
      "grad_norm": 2.0716164112091064,
      "learning_rate": 8.467168598336685e-05,
      "loss": 2.0976,
      "step": 34800
    },
    {
      "epoch": 0.5129299559925231,
      "grad_norm": 1.9458367824554443,
      "learning_rate": 8.463001796230456e-05,
      "loss": 2.0303,
      "step": 34850
    },
    {
      "epoch": 0.5136658669767304,
      "grad_norm": 2.059638500213623,
      "learning_rate": 8.458830366660824e-05,
      "loss": 2.1366,
      "step": 34900
    },
    {
      "epoch": 0.5144017779609379,
      "grad_norm": 2.237825393676758,
      "learning_rate": 8.454654315201891e-05,
      "loss": 2.1923,
      "step": 34950
    },
    {
      "epoch": 0.5151376889451452,
      "grad_norm": 2.01690936088562,
      "learning_rate": 8.450473647433939e-05,
      "loss": 2.0807,
      "step": 35000
    },
    {
      "epoch": 0.5158735999293526,
      "grad_norm": 2.1657581329345703,
      "learning_rate": 8.446288368943417e-05,
      "loss": 2.0917,
      "step": 35050
    },
    {
      "epoch": 0.5166095109135599,
      "grad_norm": 2.0601279735565186,
      "learning_rate": 8.44209848532294e-05,
      "loss": 2.1117,
      "step": 35100
    },
    {
      "epoch": 0.5173454218977672,
      "grad_norm": 1.9896847009658813,
      "learning_rate": 8.437904002171271e-05,
      "loss": 2.1079,
      "step": 35150
    },
    {
      "epoch": 0.5180813328819746,
      "grad_norm": 2.4042928218841553,
      "learning_rate": 8.43370492509332e-05,
      "loss": 2.0855,
      "step": 35200
    },
    {
      "epoch": 0.518817243866182,
      "grad_norm": 1.933313250541687,
      "learning_rate": 8.429501259700138e-05,
      "loss": 2.1026,
      "step": 35250
    },
    {
      "epoch": 0.5195531548503893,
      "grad_norm": 2.0565543174743652,
      "learning_rate": 8.425293011608905e-05,
      "loss": 2.1273,
      "step": 35300
    },
    {
      "epoch": 0.5202890658345967,
      "grad_norm": 2.129124641418457,
      "learning_rate": 8.421080186442929e-05,
      "loss": 2.1117,
      "step": 35350
    },
    {
      "epoch": 0.521024976818804,
      "grad_norm": 1.8502503633499146,
      "learning_rate": 8.416862789831625e-05,
      "loss": 2.0798,
      "step": 35400
    },
    {
      "epoch": 0.5217608878030113,
      "grad_norm": 2.031248092651367,
      "learning_rate": 8.412640827410528e-05,
      "loss": 2.1674,
      "step": 35450
    },
    {
      "epoch": 0.5224967987872187,
      "grad_norm": 2.1297500133514404,
      "learning_rate": 8.408414304821266e-05,
      "loss": 2.1042,
      "step": 35500
    },
    {
      "epoch": 0.523232709771426,
      "grad_norm": 2.0368423461914062,
      "learning_rate": 8.404183227711562e-05,
      "loss": 2.0631,
      "step": 35550
    },
    {
      "epoch": 0.5239686207556334,
      "grad_norm": 2.1081535816192627,
      "learning_rate": 8.399947601735228e-05,
      "loss": 2.1023,
      "step": 35600
    },
    {
      "epoch": 0.5247045317398408,
      "grad_norm": 2.131840944290161,
      "learning_rate": 8.39570743255215e-05,
      "loss": 2.0627,
      "step": 35650
    },
    {
      "epoch": 0.5254404427240481,
      "grad_norm": 2.1743714809417725,
      "learning_rate": 8.391462725828292e-05,
      "loss": 2.0895,
      "step": 35700
    },
    {
      "epoch": 0.5261763537082554,
      "grad_norm": 1.951064944267273,
      "learning_rate": 8.387213487235672e-05,
      "loss": 2.0929,
      "step": 35750
    },
    {
      "epoch": 0.5269122646924628,
      "grad_norm": 2.251067638397217,
      "learning_rate": 8.382959722452371e-05,
      "loss": 2.1204,
      "step": 35800
    },
    {
      "epoch": 0.5276481756766701,
      "grad_norm": 2.41438889503479,
      "learning_rate": 8.378701437162516e-05,
      "loss": 2.0777,
      "step": 35850
    },
    {
      "epoch": 0.5283840866608775,
      "grad_norm": 2.3709192276000977,
      "learning_rate": 8.374438637056272e-05,
      "loss": 2.1293,
      "step": 35900
    },
    {
      "epoch": 0.5291199976450849,
      "grad_norm": 1.9834994077682495,
      "learning_rate": 8.370171327829842e-05,
      "loss": 2.0578,
      "step": 35950
    },
    {
      "epoch": 0.5298559086292922,
      "grad_norm": 2.3027584552764893,
      "learning_rate": 8.36589951518545e-05,
      "loss": 2.0894,
      "step": 36000
    },
    {
      "epoch": 0.5305918196134995,
      "grad_norm": 2.339332103729248,
      "learning_rate": 8.361623204831339e-05,
      "loss": 2.0294,
      "step": 36050
    },
    {
      "epoch": 0.5313277305977069,
      "grad_norm": 1.8843270540237427,
      "learning_rate": 8.357342402481763e-05,
      "loss": 2.1117,
      "step": 36100
    },
    {
      "epoch": 0.5320636415819142,
      "grad_norm": 1.6391106843948364,
      "learning_rate": 8.353057113856977e-05,
      "loss": 2.0984,
      "step": 36150
    },
    {
      "epoch": 0.5327995525661215,
      "grad_norm": 2.1445131301879883,
      "learning_rate": 8.348767344683234e-05,
      "loss": 2.1154,
      "step": 36200
    },
    {
      "epoch": 0.533535463550329,
      "grad_norm": 1.990308165550232,
      "learning_rate": 8.344473100692767e-05,
      "loss": 2.1068,
      "step": 36250
    },
    {
      "epoch": 0.5342713745345363,
      "grad_norm": 2.260399103164673,
      "learning_rate": 8.3401743876238e-05,
      "loss": 2.0606,
      "step": 36300
    },
    {
      "epoch": 0.5350072855187437,
      "grad_norm": 2.037862777709961,
      "learning_rate": 8.335871211220517e-05,
      "loss": 2.0075,
      "step": 36350
    },
    {
      "epoch": 0.535743196502951,
      "grad_norm": 2.077313184738159,
      "learning_rate": 8.331563577233072e-05,
      "loss": 2.0872,
      "step": 36400
    },
    {
      "epoch": 0.5364791074871583,
      "grad_norm": 2.0430352687835693,
      "learning_rate": 8.327251491417578e-05,
      "loss": 2.1317,
      "step": 36450
    },
    {
      "epoch": 0.5372150184713657,
      "grad_norm": 2.046468496322632,
      "learning_rate": 8.32293495953609e-05,
      "loss": 2.0328,
      "step": 36500
    },
    {
      "epoch": 0.5379509294555731,
      "grad_norm": 2.2627506256103516,
      "learning_rate": 8.31861398735661e-05,
      "loss": 2.0778,
      "step": 36550
    },
    {
      "epoch": 0.5386868404397804,
      "grad_norm": 2.160856246948242,
      "learning_rate": 8.314288580653069e-05,
      "loss": 2.0704,
      "step": 36600
    },
    {
      "epoch": 0.5394227514239878,
      "grad_norm": 2.055311918258667,
      "learning_rate": 8.309958745205327e-05,
      "loss": 2.1014,
      "step": 36650
    },
    {
      "epoch": 0.5401586624081951,
      "grad_norm": 2.3173744678497314,
      "learning_rate": 8.305624486799162e-05,
      "loss": 2.137,
      "step": 36700
    },
    {
      "epoch": 0.5408945733924024,
      "grad_norm": 2.2702224254608154,
      "learning_rate": 8.301285811226259e-05,
      "loss": 2.097,
      "step": 36750
    },
    {
      "epoch": 0.5416304843766098,
      "grad_norm": 2.0626981258392334,
      "learning_rate": 8.296942724284207e-05,
      "loss": 2.1278,
      "step": 36800
    },
    {
      "epoch": 0.5423663953608172,
      "grad_norm": 2.023775100708008,
      "learning_rate": 8.29259523177649e-05,
      "loss": 2.0795,
      "step": 36850
    },
    {
      "epoch": 0.5431023063450245,
      "grad_norm": 1.989874005317688,
      "learning_rate": 8.288243339512481e-05,
      "loss": 2.115,
      "step": 36900
    },
    {
      "epoch": 0.5438382173292319,
      "grad_norm": 2.344761610031128,
      "learning_rate": 8.28388705330743e-05,
      "loss": 2.0592,
      "step": 36950
    },
    {
      "epoch": 0.5445741283134392,
      "grad_norm": 2.2783725261688232,
      "learning_rate": 8.279526378982459e-05,
      "loss": 2.0619,
      "step": 37000
    },
    {
      "epoch": 0.5453100392976465,
      "grad_norm": 1.9503380060195923,
      "learning_rate": 8.275161322364551e-05,
      "loss": 2.0799,
      "step": 37050
    },
    {
      "epoch": 0.5460459502818539,
      "grad_norm": 2.2084057331085205,
      "learning_rate": 8.270791889286549e-05,
      "loss": 2.0927,
      "step": 37100
    },
    {
      "epoch": 0.5467818612660612,
      "grad_norm": 1.9249097108840942,
      "learning_rate": 8.266418085587144e-05,
      "loss": 2.0321,
      "step": 37150
    },
    {
      "epoch": 0.5475177722502687,
      "grad_norm": 1.8995286226272583,
      "learning_rate": 8.26203991711086e-05,
      "loss": 2.0654,
      "step": 37200
    },
    {
      "epoch": 0.548253683234476,
      "grad_norm": 1.9875001907348633,
      "learning_rate": 8.257657389708066e-05,
      "loss": 2.0069,
      "step": 37250
    },
    {
      "epoch": 0.5489895942186833,
      "grad_norm": 2.262725830078125,
      "learning_rate": 8.253270509234944e-05,
      "loss": 2.0274,
      "step": 37300
    },
    {
      "epoch": 0.5497255052028907,
      "grad_norm": 2.035306215286255,
      "learning_rate": 8.2488792815535e-05,
      "loss": 2.0388,
      "step": 37350
    },
    {
      "epoch": 0.550461416187098,
      "grad_norm": 2.0666885375976562,
      "learning_rate": 8.244483712531545e-05,
      "loss": 2.0351,
      "step": 37400
    },
    {
      "epoch": 0.5511973271713053,
      "grad_norm": 1.7845593690872192,
      "learning_rate": 8.240083808042694e-05,
      "loss": 2.0598,
      "step": 37450
    },
    {
      "epoch": 0.5519332381555128,
      "grad_norm": 2.10385799407959,
      "learning_rate": 8.235679573966352e-05,
      "loss": 2.0553,
      "step": 37500
    },
    {
      "epoch": 0.5526691491397201,
      "grad_norm": 1.9946744441986084,
      "learning_rate": 8.231271016187713e-05,
      "loss": 2.1026,
      "step": 37550
    },
    {
      "epoch": 0.5534050601239274,
      "grad_norm": 2.1261513233184814,
      "learning_rate": 8.226858140597749e-05,
      "loss": 2.0922,
      "step": 37600
    },
    {
      "epoch": 0.5541409711081348,
      "grad_norm": 2.0882179737091064,
      "learning_rate": 8.222440953093196e-05,
      "loss": 2.0837,
      "step": 37650
    },
    {
      "epoch": 0.5548768820923421,
      "grad_norm": 1.8711928129196167,
      "learning_rate": 8.218019459576556e-05,
      "loss": 2.0768,
      "step": 37700
    },
    {
      "epoch": 0.5556127930765494,
      "grad_norm": 2.1025285720825195,
      "learning_rate": 8.213593665956088e-05,
      "loss": 2.0608,
      "step": 37750
    },
    {
      "epoch": 0.5563487040607568,
      "grad_norm": 2.111802339553833,
      "learning_rate": 8.209163578145789e-05,
      "loss": 2.0952,
      "step": 37800
    },
    {
      "epoch": 0.5570846150449642,
      "grad_norm": 2.271822214126587,
      "learning_rate": 8.204729202065402e-05,
      "loss": 2.0943,
      "step": 37850
    },
    {
      "epoch": 0.5578205260291715,
      "grad_norm": 1.997348666191101,
      "learning_rate": 8.200290543640396e-05,
      "loss": 2.0931,
      "step": 37900
    },
    {
      "epoch": 0.5585564370133789,
      "grad_norm": 2.0682895183563232,
      "learning_rate": 8.195847608801962e-05,
      "loss": 2.0616,
      "step": 37950
    },
    {
      "epoch": 0.5592923479975862,
      "grad_norm": 2.196011543273926,
      "learning_rate": 8.19140040348701e-05,
      "loss": 2.0299,
      "step": 38000
    },
    {
      "epoch": 0.5600282589817935,
      "grad_norm": 2.0057244300842285,
      "learning_rate": 8.186948933638149e-05,
      "loss": 2.0921,
      "step": 38050
    },
    {
      "epoch": 0.5607641699660009,
      "grad_norm": 1.8964534997940063,
      "learning_rate": 8.182493205203693e-05,
      "loss": 2.0936,
      "step": 38100
    },
    {
      "epoch": 0.5615000809502083,
      "grad_norm": 2.0234899520874023,
      "learning_rate": 8.178033224137643e-05,
      "loss": 2.1128,
      "step": 38150
    },
    {
      "epoch": 0.5622359919344156,
      "grad_norm": 1.914326548576355,
      "learning_rate": 8.173568996399685e-05,
      "loss": 2.036,
      "step": 38200
    },
    {
      "epoch": 0.562971902918623,
      "grad_norm": 1.8190770149230957,
      "learning_rate": 8.169100527955178e-05,
      "loss": 2.0007,
      "step": 38250
    },
    {
      "epoch": 0.5637078139028303,
      "grad_norm": 2.019622325897217,
      "learning_rate": 8.164627824775148e-05,
      "loss": 2.0394,
      "step": 38300
    },
    {
      "epoch": 0.5644437248870376,
      "grad_norm": 2.086613655090332,
      "learning_rate": 8.16015089283628e-05,
      "loss": 2.0943,
      "step": 38350
    },
    {
      "epoch": 0.565179635871245,
      "grad_norm": 1.8802893161773682,
      "learning_rate": 8.155669738120909e-05,
      "loss": 2.0455,
      "step": 38400
    },
    {
      "epoch": 0.5659155468554523,
      "grad_norm": 2.1174750328063965,
      "learning_rate": 8.151184366617014e-05,
      "loss": 2.1539,
      "step": 38450
    },
    {
      "epoch": 0.5666514578396598,
      "grad_norm": 2.401123523712158,
      "learning_rate": 8.146694784318207e-05,
      "loss": 2.0835,
      "step": 38500
    },
    {
      "epoch": 0.5673873688238671,
      "grad_norm": 2.097490072250366,
      "learning_rate": 8.142200997223729e-05,
      "loss": 2.0595,
      "step": 38550
    },
    {
      "epoch": 0.5681232798080744,
      "grad_norm": 1.671431541442871,
      "learning_rate": 8.137703011338438e-05,
      "loss": 2.0314,
      "step": 38600
    },
    {
      "epoch": 0.5688591907922818,
      "grad_norm": 2.3220882415771484,
      "learning_rate": 8.133200832672803e-05,
      "loss": 2.0669,
      "step": 38650
    },
    {
      "epoch": 0.5695951017764891,
      "grad_norm": 1.8639748096466064,
      "learning_rate": 8.128694467242894e-05,
      "loss": 2.0194,
      "step": 38700
    },
    {
      "epoch": 0.5703310127606964,
      "grad_norm": 2.604180335998535,
      "learning_rate": 8.12418392107038e-05,
      "loss": 2.0352,
      "step": 38750
    },
    {
      "epoch": 0.5710669237449039,
      "grad_norm": 2.0931594371795654,
      "learning_rate": 8.119669200182512e-05,
      "loss": 2.0965,
      "step": 38800
    },
    {
      "epoch": 0.5718028347291112,
      "grad_norm": 2.0469675064086914,
      "learning_rate": 8.115150310612121e-05,
      "loss": 2.0518,
      "step": 38850
    },
    {
      "epoch": 0.5725387457133185,
      "grad_norm": 2.3858742713928223,
      "learning_rate": 8.110627258397611e-05,
      "loss": 2.0516,
      "step": 38900
    },
    {
      "epoch": 0.5732746566975259,
      "grad_norm": 1.9333724975585938,
      "learning_rate": 8.106100049582943e-05,
      "loss": 2.0928,
      "step": 38950
    },
    {
      "epoch": 0.5740105676817332,
      "grad_norm": 2.0641682147979736,
      "learning_rate": 8.101568690217638e-05,
      "loss": 2.0592,
      "step": 39000
    },
    {
      "epoch": 0.5747464786659405,
      "grad_norm": 2.169898509979248,
      "learning_rate": 8.097033186356761e-05,
      "loss": 2.1022,
      "step": 39050
    },
    {
      "epoch": 0.5754823896501479,
      "grad_norm": 1.969730019569397,
      "learning_rate": 8.092493544060911e-05,
      "loss": 2.0304,
      "step": 39100
    },
    {
      "epoch": 0.5762183006343553,
      "grad_norm": 1.9931652545928955,
      "learning_rate": 8.087949769396226e-05,
      "loss": 1.9765,
      "step": 39150
    },
    {
      "epoch": 0.5769542116185626,
      "grad_norm": 1.986199975013733,
      "learning_rate": 8.083401868434355e-05,
      "loss": 2.0209,
      "step": 39200
    },
    {
      "epoch": 0.57769012260277,
      "grad_norm": 2.112957715988159,
      "learning_rate": 8.078849847252472e-05,
      "loss": 2.0702,
      "step": 39250
    },
    {
      "epoch": 0.5784260335869773,
      "grad_norm": 1.8727753162384033,
      "learning_rate": 8.074293711933247e-05,
      "loss": 2.0992,
      "step": 39300
    },
    {
      "epoch": 0.5791619445711846,
      "grad_norm": 2.302255630493164,
      "learning_rate": 8.069733468564853e-05,
      "loss": 2.0363,
      "step": 39350
    },
    {
      "epoch": 0.579897855555392,
      "grad_norm": 2.0045924186706543,
      "learning_rate": 8.065169123240952e-05,
      "loss": 2.0244,
      "step": 39400
    },
    {
      "epoch": 0.5806337665395994,
      "grad_norm": 2.158428907394409,
      "learning_rate": 8.060600682060684e-05,
      "loss": 2.0694,
      "step": 39450
    },
    {
      "epoch": 0.5813696775238067,
      "grad_norm": 2.566345691680908,
      "learning_rate": 8.056028151128667e-05,
      "loss": 2.0461,
      "step": 39500
    },
    {
      "epoch": 0.5821055885080141,
      "grad_norm": 2.0109312534332275,
      "learning_rate": 8.051451536554982e-05,
      "loss": 2.0491,
      "step": 39550
    },
    {
      "epoch": 0.5828414994922214,
      "grad_norm": 2.0559332370758057,
      "learning_rate": 8.046870844455163e-05,
      "loss": 2.0427,
      "step": 39600
    },
    {
      "epoch": 0.5835774104764287,
      "grad_norm": 2.1512067317962646,
      "learning_rate": 8.042286080950199e-05,
      "loss": 2.101,
      "step": 39650
    },
    {
      "epoch": 0.5843133214606361,
      "grad_norm": 1.9485636949539185,
      "learning_rate": 8.037697252166515e-05,
      "loss": 2.0458,
      "step": 39700
    },
    {
      "epoch": 0.5850492324448435,
      "grad_norm": 2.097320079803467,
      "learning_rate": 8.033104364235966e-05,
      "loss": 2.0881,
      "step": 39750
    },
    {
      "epoch": 0.5857851434290509,
      "grad_norm": 2.346127986907959,
      "learning_rate": 8.028507423295841e-05,
      "loss": 1.9969,
      "step": 39800
    },
    {
      "epoch": 0.5865210544132582,
      "grad_norm": 1.766086459159851,
      "learning_rate": 8.023906435488835e-05,
      "loss": 2.0349,
      "step": 39850
    },
    {
      "epoch": 0.5872569653974655,
      "grad_norm": 2.1696887016296387,
      "learning_rate": 8.019301406963056e-05,
      "loss": 2.0414,
      "step": 39900
    },
    {
      "epoch": 0.5879928763816729,
      "grad_norm": 1.8857725858688354,
      "learning_rate": 8.014692343872008e-05,
      "loss": 2.0528,
      "step": 39950
    },
    {
      "epoch": 0.5887287873658802,
      "grad_norm": 2.0586578845977783,
      "learning_rate": 8.010079252374588e-05,
      "loss": 2.0053,
      "step": 40000
    },
    {
      "epoch": 0.5894646983500875,
      "grad_norm": 2.1962015628814697,
      "learning_rate": 8.005462138635076e-05,
      "loss": 2.0123,
      "step": 40050
    },
    {
      "epoch": 0.590200609334295,
      "grad_norm": 2.0835518836975098,
      "learning_rate": 8.000841008823126e-05,
      "loss": 2.0761,
      "step": 40100
    },
    {
      "epoch": 0.5909365203185023,
      "grad_norm": 2.246249198913574,
      "learning_rate": 7.996215869113761e-05,
      "loss": 2.0846,
      "step": 40150
    },
    {
      "epoch": 0.5916724313027096,
      "grad_norm": 2.0813560485839844,
      "learning_rate": 7.991586725687358e-05,
      "loss": 2.0134,
      "step": 40200
    },
    {
      "epoch": 0.592408342286917,
      "grad_norm": 1.8924275636672974,
      "learning_rate": 7.98695358472965e-05,
      "loss": 2.074,
      "step": 40250
    },
    {
      "epoch": 0.5931442532711243,
      "grad_norm": 2.2290427684783936,
      "learning_rate": 7.982316452431703e-05,
      "loss": 2.0576,
      "step": 40300
    },
    {
      "epoch": 0.5938801642553316,
      "grad_norm": 2.1121983528137207,
      "learning_rate": 7.977675334989924e-05,
      "loss": 1.9783,
      "step": 40350
    },
    {
      "epoch": 0.5946160752395391,
      "grad_norm": 1.9867889881134033,
      "learning_rate": 7.973030238606044e-05,
      "loss": 2.0891,
      "step": 40400
    },
    {
      "epoch": 0.5953519862237464,
      "grad_norm": 2.171344518661499,
      "learning_rate": 7.968381169487108e-05,
      "loss": 2.0317,
      "step": 40450
    },
    {
      "epoch": 0.5960878972079537,
      "grad_norm": 2.205683708190918,
      "learning_rate": 7.963728133845471e-05,
      "loss": 2.0634,
      "step": 40500
    },
    {
      "epoch": 0.5968238081921611,
      "grad_norm": 2.0342655181884766,
      "learning_rate": 7.95907113789879e-05,
      "loss": 2.0617,
      "step": 40550
    },
    {
      "epoch": 0.5975597191763684,
      "grad_norm": 1.9181413650512695,
      "learning_rate": 7.95441018787001e-05,
      "loss": 2.0492,
      "step": 40600
    },
    {
      "epoch": 0.5982956301605757,
      "grad_norm": 2.0536460876464844,
      "learning_rate": 7.949745289987365e-05,
      "loss": 2.1022,
      "step": 40650
    },
    {
      "epoch": 0.5990315411447831,
      "grad_norm": 2.195103883743286,
      "learning_rate": 7.94507645048436e-05,
      "loss": 2.1029,
      "step": 40700
    },
    {
      "epoch": 0.5997674521289905,
      "grad_norm": 2.1964685916900635,
      "learning_rate": 7.940403675599767e-05,
      "loss": 2.0753,
      "step": 40750
    },
    {
      "epoch": 0.6005033631131979,
      "grad_norm": 2.234861135482788,
      "learning_rate": 7.935726971577624e-05,
      "loss": 1.9818,
      "step": 40800
    },
    {
      "epoch": 0.6012392740974052,
      "grad_norm": 1.785775065422058,
      "learning_rate": 7.931046344667207e-05,
      "loss": 2.0388,
      "step": 40850
    },
    {
      "epoch": 0.6019751850816125,
      "grad_norm": 2.137302875518799,
      "learning_rate": 7.926361801123044e-05,
      "loss": 2.0366,
      "step": 40900
    },
    {
      "epoch": 0.6027110960658199,
      "grad_norm": 1.7999308109283447,
      "learning_rate": 7.921673347204891e-05,
      "loss": 2.0513,
      "step": 40950
    },
    {
      "epoch": 0.6034470070500272,
      "grad_norm": 1.9132061004638672,
      "learning_rate": 7.916980989177734e-05,
      "loss": 2.0593,
      "step": 41000
    },
    {
      "epoch": 0.6041829180342346,
      "grad_norm": 1.9518481492996216,
      "learning_rate": 7.91228473331177e-05,
      "loss": 2.0334,
      "step": 41050
    },
    {
      "epoch": 0.604918829018442,
      "grad_norm": 1.8277504444122314,
      "learning_rate": 7.90758458588241e-05,
      "loss": 2.0301,
      "step": 41100
    },
    {
      "epoch": 0.6056547400026493,
      "grad_norm": 1.817176342010498,
      "learning_rate": 7.902880553170264e-05,
      "loss": 2.0496,
      "step": 41150
    },
    {
      "epoch": 0.6063906509868566,
      "grad_norm": 2.090069055557251,
      "learning_rate": 7.898172641461132e-05,
      "loss": 1.9992,
      "step": 41200
    },
    {
      "epoch": 0.607126561971064,
      "grad_norm": 2.126631498336792,
      "learning_rate": 7.893460857045998e-05,
      "loss": 2.0515,
      "step": 41250
    },
    {
      "epoch": 0.6078624729552713,
      "grad_norm": 1.8466575145721436,
      "learning_rate": 7.88874520622102e-05,
      "loss": 2.0593,
      "step": 41300
    },
    {
      "epoch": 0.6085983839394786,
      "grad_norm": 2.349217653274536,
      "learning_rate": 7.884025695287525e-05,
      "loss": 2.026,
      "step": 41350
    },
    {
      "epoch": 0.6093342949236861,
      "grad_norm": 2.0225343704223633,
      "learning_rate": 7.879302330551994e-05,
      "loss": 1.9784,
      "step": 41400
    },
    {
      "epoch": 0.6100702059078934,
      "grad_norm": 1.9760241508483887,
      "learning_rate": 7.874575118326065e-05,
      "loss": 1.9605,
      "step": 41450
    },
    {
      "epoch": 0.6108061168921007,
      "grad_norm": 2.0077288150787354,
      "learning_rate": 7.869844064926507e-05,
      "loss": 2.0299,
      "step": 41500
    },
    {
      "epoch": 0.6115420278763081,
      "grad_norm": 2.397433042526245,
      "learning_rate": 7.865109176675232e-05,
      "loss": 2.0705,
      "step": 41550
    },
    {
      "epoch": 0.6122779388605154,
      "grad_norm": 2.22696590423584,
      "learning_rate": 7.860370459899268e-05,
      "loss": 2.0774,
      "step": 41600
    },
    {
      "epoch": 0.6130138498447227,
      "grad_norm": 2.217313766479492,
      "learning_rate": 7.855627920930766e-05,
      "loss": 2.0761,
      "step": 41650
    },
    {
      "epoch": 0.6137497608289302,
      "grad_norm": 2.3424129486083984,
      "learning_rate": 7.850881566106977e-05,
      "loss": 2.0282,
      "step": 41700
    },
    {
      "epoch": 0.6144856718131375,
      "grad_norm": 1.70395827293396,
      "learning_rate": 7.846131401770261e-05,
      "loss": 2.039,
      "step": 41750
    },
    {
      "epoch": 0.6152215827973448,
      "grad_norm": 1.9562708139419556,
      "learning_rate": 7.841377434268056e-05,
      "loss": 2.0651,
      "step": 41800
    },
    {
      "epoch": 0.6159574937815522,
      "grad_norm": 2.2179901599884033,
      "learning_rate": 7.836619669952893e-05,
      "loss": 2.014,
      "step": 41850
    },
    {
      "epoch": 0.6166934047657595,
      "grad_norm": 2.0606207847595215,
      "learning_rate": 7.831858115182369e-05,
      "loss": 2.0433,
      "step": 41900
    },
    {
      "epoch": 0.6174293157499668,
      "grad_norm": 2.601572036743164,
      "learning_rate": 7.827092776319152e-05,
      "loss": 2.0941,
      "step": 41950
    },
    {
      "epoch": 0.6181652267341743,
      "grad_norm": 1.9178520441055298,
      "learning_rate": 7.82232365973096e-05,
      "loss": 2.0416,
      "step": 42000
    },
    {
      "epoch": 0.6189011377183816,
      "grad_norm": 1.834734559059143,
      "learning_rate": 7.817550771790564e-05,
      "loss": 2.0061,
      "step": 42050
    },
    {
      "epoch": 0.619637048702589,
      "grad_norm": 1.9363410472869873,
      "learning_rate": 7.812774118875777e-05,
      "loss": 2.061,
      "step": 42100
    },
    {
      "epoch": 0.6203729596867963,
      "grad_norm": 1.7272435426712036,
      "learning_rate": 7.807993707369433e-05,
      "loss": 2.0517,
      "step": 42150
    },
    {
      "epoch": 0.6211088706710036,
      "grad_norm": 2.1346845626831055,
      "learning_rate": 7.803209543659399e-05,
      "loss": 2.0872,
      "step": 42200
    },
    {
      "epoch": 0.621844781655211,
      "grad_norm": 2.278019428253174,
      "learning_rate": 7.798421634138548e-05,
      "loss": 1.961,
      "step": 42250
    },
    {
      "epoch": 0.6225806926394183,
      "grad_norm": 2.019885540008545,
      "learning_rate": 7.793629985204765e-05,
      "loss": 2.0727,
      "step": 42300
    },
    {
      "epoch": 0.6233166036236257,
      "grad_norm": 2.4832630157470703,
      "learning_rate": 7.788834603260923e-05,
      "loss": 2.0123,
      "step": 42350
    },
    {
      "epoch": 0.6240525146078331,
      "grad_norm": 2.1586506366729736,
      "learning_rate": 7.784035494714896e-05,
      "loss": 1.9804,
      "step": 42400
    },
    {
      "epoch": 0.6247884255920404,
      "grad_norm": 2.208828926086426,
      "learning_rate": 7.779232665979527e-05,
      "loss": 2.0659,
      "step": 42450
    },
    {
      "epoch": 0.6255243365762477,
      "grad_norm": 2.0062992572784424,
      "learning_rate": 7.774426123472635e-05,
      "loss": 2.0343,
      "step": 42500
    },
    {
      "epoch": 0.6262602475604551,
      "grad_norm": 1.8384901285171509,
      "learning_rate": 7.769615873616998e-05,
      "loss": 1.9973,
      "step": 42550
    },
    {
      "epoch": 0.6269961585446624,
      "grad_norm": 2.2608180046081543,
      "learning_rate": 7.764801922840351e-05,
      "loss": 2.0568,
      "step": 42600
    },
    {
      "epoch": 0.6277320695288698,
      "grad_norm": 2.3910295963287354,
      "learning_rate": 7.759984277575375e-05,
      "loss": 2.1397,
      "step": 42650
    },
    {
      "epoch": 0.6284679805130772,
      "grad_norm": 2.0281989574432373,
      "learning_rate": 7.755162944259686e-05,
      "loss": 1.9748,
      "step": 42700
    },
    {
      "epoch": 0.6292038914972845,
      "grad_norm": 2.0534114837646484,
      "learning_rate": 7.750337929335827e-05,
      "loss": 2.0514,
      "step": 42750
    },
    {
      "epoch": 0.6299398024814918,
      "grad_norm": 2.057440996170044,
      "learning_rate": 7.745509239251264e-05,
      "loss": 2.021,
      "step": 42800
    },
    {
      "epoch": 0.6306757134656992,
      "grad_norm": 2.140209913253784,
      "learning_rate": 7.740676880458371e-05,
      "loss": 2.073,
      "step": 42850
    },
    {
      "epoch": 0.6314116244499065,
      "grad_norm": 1.8360295295715332,
      "learning_rate": 7.735840859414428e-05,
      "loss": 2.1297,
      "step": 42900
    },
    {
      "epoch": 0.6321475354341138,
      "grad_norm": 1.7959191799163818,
      "learning_rate": 7.731001182581602e-05,
      "loss": 2.0243,
      "step": 42950
    },
    {
      "epoch": 0.6328834464183213,
      "grad_norm": 1.983691930770874,
      "learning_rate": 7.726157856426951e-05,
      "loss": 2.0285,
      "step": 43000
    },
    {
      "epoch": 0.6336193574025286,
      "grad_norm": 2.079737424850464,
      "learning_rate": 7.721310887422411e-05,
      "loss": 2.0899,
      "step": 43050
    },
    {
      "epoch": 0.634355268386736,
      "grad_norm": 2.244643449783325,
      "learning_rate": 7.716460282044776e-05,
      "loss": 2.0392,
      "step": 43100
    },
    {
      "epoch": 0.6350911793709433,
      "grad_norm": 2.1511716842651367,
      "learning_rate": 7.71160604677571e-05,
      "loss": 2.0027,
      "step": 43150
    },
    {
      "epoch": 0.6358270903551506,
      "grad_norm": 2.412614583969116,
      "learning_rate": 7.706748188101724e-05,
      "loss": 1.994,
      "step": 43200
    },
    {
      "epoch": 0.636563001339358,
      "grad_norm": 2.407820463180542,
      "learning_rate": 7.701886712514167e-05,
      "loss": 2.0539,
      "step": 43250
    },
    {
      "epoch": 0.6372989123235654,
      "grad_norm": 2.3303449153900146,
      "learning_rate": 7.697021626509224e-05,
      "loss": 2.0155,
      "step": 43300
    },
    {
      "epoch": 0.6380348233077727,
      "grad_norm": 2.208932399749756,
      "learning_rate": 7.692152936587907e-05,
      "loss": 2.0461,
      "step": 43350
    },
    {
      "epoch": 0.6387707342919801,
      "grad_norm": 2.3841660022735596,
      "learning_rate": 7.68728064925604e-05,
      "loss": 1.9845,
      "step": 43400
    },
    {
      "epoch": 0.6395066452761874,
      "grad_norm": 2.3597590923309326,
      "learning_rate": 7.682404771024254e-05,
      "loss": 1.9704,
      "step": 43450
    },
    {
      "epoch": 0.6402425562603947,
      "grad_norm": 2.8422775268554688,
      "learning_rate": 7.677525308407983e-05,
      "loss": 2.0225,
      "step": 43500
    },
    {
      "epoch": 0.6409784672446021,
      "grad_norm": 1.9134119749069214,
      "learning_rate": 7.672642267927443e-05,
      "loss": 2.0578,
      "step": 43550
    },
    {
      "epoch": 0.6417143782288094,
      "grad_norm": 1.9700686931610107,
      "learning_rate": 7.667755656107638e-05,
      "loss": 2.0004,
      "step": 43600
    },
    {
      "epoch": 0.6424502892130168,
      "grad_norm": 2.176558017730713,
      "learning_rate": 7.66286547947834e-05,
      "loss": 2.0394,
      "step": 43650
    },
    {
      "epoch": 0.6431862001972242,
      "grad_norm": 1.9141056537628174,
      "learning_rate": 7.657971744574088e-05,
      "loss": 2.0292,
      "step": 43700
    },
    {
      "epoch": 0.6439221111814315,
      "grad_norm": 2.1808948516845703,
      "learning_rate": 7.653074457934173e-05,
      "loss": 1.9938,
      "step": 43750
    },
    {
      "epoch": 0.6446580221656388,
      "grad_norm": 2.2033326625823975,
      "learning_rate": 7.648173626102632e-05,
      "loss": 2.015,
      "step": 43800
    },
    {
      "epoch": 0.6453939331498462,
      "grad_norm": 2.027742862701416,
      "learning_rate": 7.64326925562824e-05,
      "loss": 2.057,
      "step": 43850
    },
    {
      "epoch": 0.6461298441340535,
      "grad_norm": 1.7777310609817505,
      "learning_rate": 7.638361353064502e-05,
      "loss": 2.0403,
      "step": 43900
    },
    {
      "epoch": 0.6468657551182609,
      "grad_norm": 1.7752690315246582,
      "learning_rate": 7.63344992496964e-05,
      "loss": 1.9957,
      "step": 43950
    },
    {
      "epoch": 0.6476016661024683,
      "grad_norm": 2.274796962738037,
      "learning_rate": 7.628534977906589e-05,
      "loss": 2.0047,
      "step": 44000
    },
    {
      "epoch": 0.6483375770866756,
      "grad_norm": 1.9063990116119385,
      "learning_rate": 7.623616518442989e-05,
      "loss": 1.9737,
      "step": 44050
    },
    {
      "epoch": 0.6490734880708829,
      "grad_norm": 1.9381705522537231,
      "learning_rate": 7.618694553151164e-05,
      "loss": 2.0519,
      "step": 44100
    },
    {
      "epoch": 0.6498093990550903,
      "grad_norm": 1.9870785474777222,
      "learning_rate": 7.613769088608133e-05,
      "loss": 2.0908,
      "step": 44150
    },
    {
      "epoch": 0.6505453100392976,
      "grad_norm": 1.8339300155639648,
      "learning_rate": 7.608840131395584e-05,
      "loss": 2.0446,
      "step": 44200
    },
    {
      "epoch": 0.6512812210235049,
      "grad_norm": 1.9833385944366455,
      "learning_rate": 7.603907688099878e-05,
      "loss": 1.9974,
      "step": 44250
    },
    {
      "epoch": 0.6520171320077124,
      "grad_norm": 1.9588335752487183,
      "learning_rate": 7.598971765312029e-05,
      "loss": 2.0461,
      "step": 44300
    },
    {
      "epoch": 0.6527530429919197,
      "grad_norm": 1.9029541015625,
      "learning_rate": 7.594032369627703e-05,
      "loss": 2.0364,
      "step": 44350
    },
    {
      "epoch": 0.653488953976127,
      "grad_norm": 2.2481188774108887,
      "learning_rate": 7.589089507647207e-05,
      "loss": 2.0553,
      "step": 44400
    },
    {
      "epoch": 0.6542248649603344,
      "grad_norm": 2.069359302520752,
      "learning_rate": 7.584143185975478e-05,
      "loss": 2.0105,
      "step": 44450
    },
    {
      "epoch": 0.6549607759445417,
      "grad_norm": 2.0515949726104736,
      "learning_rate": 7.57919341122208e-05,
      "loss": 2.0298,
      "step": 44500
    },
    {
      "epoch": 0.655696686928749,
      "grad_norm": 2.1616785526275635,
      "learning_rate": 7.574240190001187e-05,
      "loss": 2.0354,
      "step": 44550
    },
    {
      "epoch": 0.6564325979129565,
      "grad_norm": 2.2839648723602295,
      "learning_rate": 7.569283528931576e-05,
      "loss": 2.0096,
      "step": 44600
    },
    {
      "epoch": 0.6571685088971638,
      "grad_norm": 2.1627635955810547,
      "learning_rate": 7.564323434636631e-05,
      "loss": 2.0305,
      "step": 44650
    },
    {
      "epoch": 0.6579044198813712,
      "grad_norm": 1.9958099126815796,
      "learning_rate": 7.559359913744314e-05,
      "loss": 2.0581,
      "step": 44700
    },
    {
      "epoch": 0.6586403308655785,
      "grad_norm": 2.053663492202759,
      "learning_rate": 7.554392972887168e-05,
      "loss": 2.0098,
      "step": 44750
    },
    {
      "epoch": 0.6593762418497858,
      "grad_norm": 1.8618756532669067,
      "learning_rate": 7.549422618702307e-05,
      "loss": 1.9746,
      "step": 44800
    },
    {
      "epoch": 0.6601121528339932,
      "grad_norm": 1.8282039165496826,
      "learning_rate": 7.544448857831408e-05,
      "loss": 2.022,
      "step": 44850
    },
    {
      "epoch": 0.6608480638182006,
      "grad_norm": 2.0083255767822266,
      "learning_rate": 7.539471696920695e-05,
      "loss": 1.9543,
      "step": 44900
    },
    {
      "epoch": 0.6615839748024079,
      "grad_norm": 2.221083879470825,
      "learning_rate": 7.534491142620941e-05,
      "loss": 1.9674,
      "step": 44950
    },
    {
      "epoch": 0.6623198857866153,
      "grad_norm": 2.2202444076538086,
      "learning_rate": 7.52950720158745e-05,
      "loss": 1.9898,
      "step": 45000
    },
    {
      "epoch": 0.6630557967708226,
      "grad_norm": 1.9793230295181274,
      "learning_rate": 7.52451988048005e-05,
      "loss": 2.0579,
      "step": 45050
    },
    {
      "epoch": 0.6637917077550299,
      "grad_norm": 1.917121171951294,
      "learning_rate": 7.519529185963092e-05,
      "loss": 2.0233,
      "step": 45100
    },
    {
      "epoch": 0.6645276187392373,
      "grad_norm": 2.1539244651794434,
      "learning_rate": 7.514535124705431e-05,
      "loss": 1.9671,
      "step": 45150
    },
    {
      "epoch": 0.6652635297234446,
      "grad_norm": 1.831558346748352,
      "learning_rate": 7.509537703380416e-05,
      "loss": 1.9909,
      "step": 45200
    },
    {
      "epoch": 0.665999440707652,
      "grad_norm": 2.0678844451904297,
      "learning_rate": 7.504536928665895e-05,
      "loss": 1.9918,
      "step": 45250
    },
    {
      "epoch": 0.6667353516918594,
      "grad_norm": 2.1852285861968994,
      "learning_rate": 7.49953280724419e-05,
      "loss": 2.0275,
      "step": 45300
    },
    {
      "epoch": 0.6674712626760667,
      "grad_norm": 2.0829319953918457,
      "learning_rate": 7.4945253458021e-05,
      "loss": 1.9446,
      "step": 45350
    },
    {
      "epoch": 0.668207173660274,
      "grad_norm": 1.9371180534362793,
      "learning_rate": 7.489514551030883e-05,
      "loss": 2.0966,
      "step": 45400
    },
    {
      "epoch": 0.6689430846444814,
      "grad_norm": 2.395772695541382,
      "learning_rate": 7.484500429626254e-05,
      "loss": 2.0763,
      "step": 45450
    },
    {
      "epoch": 0.6696789956286887,
      "grad_norm": 2.2918787002563477,
      "learning_rate": 7.479482988288372e-05,
      "loss": 1.9982,
      "step": 45500
    },
    {
      "epoch": 0.6704149066128962,
      "grad_norm": 2.1289048194885254,
      "learning_rate": 7.47446223372183e-05,
      "loss": 2.0065,
      "step": 45550
    },
    {
      "epoch": 0.6711508175971035,
      "grad_norm": 1.8338481187820435,
      "learning_rate": 7.469438172635654e-05,
      "loss": 2.0273,
      "step": 45600
    },
    {
      "epoch": 0.6718867285813108,
      "grad_norm": 1.8296141624450684,
      "learning_rate": 7.464410811743282e-05,
      "loss": 2.009,
      "step": 45650
    },
    {
      "epoch": 0.6726226395655182,
      "grad_norm": 2.3842861652374268,
      "learning_rate": 7.45938015776257e-05,
      "loss": 2.0471,
      "step": 45700
    },
    {
      "epoch": 0.6733585505497255,
      "grad_norm": 2.1688430309295654,
      "learning_rate": 7.454346217415764e-05,
      "loss": 2.0352,
      "step": 45750
    },
    {
      "epoch": 0.6740944615339328,
      "grad_norm": 2.0131118297576904,
      "learning_rate": 7.44930899742951e-05,
      "loss": 2.0332,
      "step": 45800
    },
    {
      "epoch": 0.6748303725181402,
      "grad_norm": 1.6302851438522339,
      "learning_rate": 7.444268504534828e-05,
      "loss": 1.9849,
      "step": 45850
    },
    {
      "epoch": 0.6755662835023476,
      "grad_norm": 2.094651937484741,
      "learning_rate": 7.43922474546712e-05,
      "loss": 1.9605,
      "step": 45900
    },
    {
      "epoch": 0.6763021944865549,
      "grad_norm": 2.035708427429199,
      "learning_rate": 7.434177726966149e-05,
      "loss": 2.0078,
      "step": 45950
    },
    {
      "epoch": 0.6770381054707623,
      "grad_norm": 2.3133206367492676,
      "learning_rate": 7.42912745577603e-05,
      "loss": 2.0203,
      "step": 46000
    },
    {
      "epoch": 0.6777740164549696,
      "grad_norm": 2.7066125869750977,
      "learning_rate": 7.424073938645231e-05,
      "loss": 2.0166,
      "step": 46050
    },
    {
      "epoch": 0.6785099274391769,
      "grad_norm": 2.013289213180542,
      "learning_rate": 7.419017182326553e-05,
      "loss": 1.9983,
      "step": 46100
    },
    {
      "epoch": 0.6792458384233843,
      "grad_norm": 1.9911422729492188,
      "learning_rate": 7.413957193577126e-05,
      "loss": 2.0062,
      "step": 46150
    },
    {
      "epoch": 0.6799817494075917,
      "grad_norm": 2.8562545776367188,
      "learning_rate": 7.408893979158399e-05,
      "loss": 2.0126,
      "step": 46200
    },
    {
      "epoch": 0.680717660391799,
      "grad_norm": 1.9918253421783447,
      "learning_rate": 7.403827545836135e-05,
      "loss": 2.0536,
      "step": 46250
    },
    {
      "epoch": 0.6814535713760064,
      "grad_norm": 2.058398723602295,
      "learning_rate": 7.398757900380392e-05,
      "loss": 1.9713,
      "step": 46300
    },
    {
      "epoch": 0.6821894823602137,
      "grad_norm": 1.94040846824646,
      "learning_rate": 7.393685049565526e-05,
      "loss": 2.0144,
      "step": 46350
    },
    {
      "epoch": 0.682925393344421,
      "grad_norm": 2.551265239715576,
      "learning_rate": 7.388609000170172e-05,
      "loss": 2.0137,
      "step": 46400
    },
    {
      "epoch": 0.6836613043286284,
      "grad_norm": 2.019970417022705,
      "learning_rate": 7.383529758977242e-05,
      "loss": 2.0121,
      "step": 46450
    },
    {
      "epoch": 0.6843972153128357,
      "grad_norm": 2.1727287769317627,
      "learning_rate": 7.378447332773913e-05,
      "loss": 1.9834,
      "step": 46500
    },
    {
      "epoch": 0.6851331262970431,
      "grad_norm": 2.2095947265625,
      "learning_rate": 7.373361728351614e-05,
      "loss": 2.0313,
      "step": 46550
    },
    {
      "epoch": 0.6858690372812505,
      "grad_norm": 2.0791492462158203,
      "learning_rate": 7.368272952506028e-05,
      "loss": 2.0146,
      "step": 46600
    },
    {
      "epoch": 0.6866049482654578,
      "grad_norm": 2.0102882385253906,
      "learning_rate": 7.363181012037068e-05,
      "loss": 2.0261,
      "step": 46650
    },
    {
      "epoch": 0.6873408592496651,
      "grad_norm": 2.1815595626831055,
      "learning_rate": 7.358085913748882e-05,
      "loss": 2.0314,
      "step": 46700
    },
    {
      "epoch": 0.6880767702338725,
      "grad_norm": 2.21707820892334,
      "learning_rate": 7.352987664449835e-05,
      "loss": 2.0556,
      "step": 46750
    },
    {
      "epoch": 0.6888126812180798,
      "grad_norm": 2.222801923751831,
      "learning_rate": 7.347886270952502e-05,
      "loss": 2.0366,
      "step": 46800
    },
    {
      "epoch": 0.6895485922022873,
      "grad_norm": 2.3689610958099365,
      "learning_rate": 7.342781740073659e-05,
      "loss": 1.9822,
      "step": 46850
    },
    {
      "epoch": 0.6902845031864946,
      "grad_norm": 2.4485719203948975,
      "learning_rate": 7.337674078634277e-05,
      "loss": 2.013,
      "step": 46900
    },
    {
      "epoch": 0.6910204141707019,
      "grad_norm": 2.210901975631714,
      "learning_rate": 7.332563293459511e-05,
      "loss": 2.0088,
      "step": 46950
    },
    {
      "epoch": 0.6917563251549093,
      "grad_norm": 1.9963090419769287,
      "learning_rate": 7.327449391378684e-05,
      "loss": 2.0134,
      "step": 47000
    },
    {
      "epoch": 0.6924922361391166,
      "grad_norm": 2.026479721069336,
      "learning_rate": 7.32233237922529e-05,
      "loss": 2.0408,
      "step": 47050
    },
    {
      "epoch": 0.6932281471233239,
      "grad_norm": 2.0055503845214844,
      "learning_rate": 7.317212263836975e-05,
      "loss": 2.0352,
      "step": 47100
    },
    {
      "epoch": 0.6939640581075314,
      "grad_norm": 1.9919265508651733,
      "learning_rate": 7.312089052055536e-05,
      "loss": 2.0865,
      "step": 47150
    },
    {
      "epoch": 0.6946999690917387,
      "grad_norm": 2.4306082725524902,
      "learning_rate": 7.306962750726905e-05,
      "loss": 2.0174,
      "step": 47200
    },
    {
      "epoch": 0.695435880075946,
      "grad_norm": 2.0315303802490234,
      "learning_rate": 7.301833366701139e-05,
      "loss": 1.9611,
      "step": 47250
    },
    {
      "epoch": 0.6961717910601534,
      "grad_norm": 2.152404546737671,
      "learning_rate": 7.296700906832421e-05,
      "loss": 2.007,
      "step": 47300
    },
    {
      "epoch": 0.6969077020443607,
      "grad_norm": 1.988271713256836,
      "learning_rate": 7.29156537797904e-05,
      "loss": 1.9239,
      "step": 47350
    },
    {
      "epoch": 0.697643613028568,
      "grad_norm": 1.8269318342208862,
      "learning_rate": 7.286426787003388e-05,
      "loss": 2.0517,
      "step": 47400
    },
    {
      "epoch": 0.6983795240127754,
      "grad_norm": 1.9975345134735107,
      "learning_rate": 7.281285140771945e-05,
      "loss": 2.0622,
      "step": 47450
    },
    {
      "epoch": 0.6991154349969828,
      "grad_norm": 2.5467286109924316,
      "learning_rate": 7.276140446155277e-05,
      "loss": 1.9727,
      "step": 47500
    },
    {
      "epoch": 0.6998513459811901,
      "grad_norm": 2.190396308898926,
      "learning_rate": 7.270992710028024e-05,
      "loss": 1.9962,
      "step": 47550
    },
    {
      "epoch": 0.7005872569653975,
      "grad_norm": 2.071579694747925,
      "learning_rate": 7.265841939268891e-05,
      "loss": 1.966,
      "step": 47600
    },
    {
      "epoch": 0.7013231679496048,
      "grad_norm": 1.8376390933990479,
      "learning_rate": 7.26068814076063e-05,
      "loss": 1.9723,
      "step": 47650
    },
    {
      "epoch": 0.7020590789338121,
      "grad_norm": 2.0107789039611816,
      "learning_rate": 7.25553132139005e-05,
      "loss": 1.9981,
      "step": 47700
    },
    {
      "epoch": 0.7027949899180195,
      "grad_norm": 1.9613068103790283,
      "learning_rate": 7.250371488047989e-05,
      "loss": 1.9916,
      "step": 47750
    },
    {
      "epoch": 0.7035309009022269,
      "grad_norm": 1.8978015184402466,
      "learning_rate": 7.245208647629313e-05,
      "loss": 2.0119,
      "step": 47800
    },
    {
      "epoch": 0.7042668118864343,
      "grad_norm": 2.2908647060394287,
      "learning_rate": 7.240042807032914e-05,
      "loss": 2.0245,
      "step": 47850
    },
    {
      "epoch": 0.7050027228706416,
      "grad_norm": 1.9937344789505005,
      "learning_rate": 7.234873973161683e-05,
      "loss": 2.0109,
      "step": 47900
    },
    {
      "epoch": 0.7057386338548489,
      "grad_norm": 2.228334665298462,
      "learning_rate": 7.229702152922516e-05,
      "loss": 1.9924,
      "step": 47950
    },
    {
      "epoch": 0.7064745448390563,
      "grad_norm": 1.9413760900497437,
      "learning_rate": 7.224527353226297e-05,
      "loss": 1.9811,
      "step": 48000
    },
    {
      "epoch": 0.7072104558232636,
      "grad_norm": 2.154552459716797,
      "learning_rate": 7.219349580987896e-05,
      "loss": 1.9745,
      "step": 48050
    },
    {
      "epoch": 0.7079463668074709,
      "grad_norm": 2.0213563442230225,
      "learning_rate": 7.214168843126149e-05,
      "loss": 1.9433,
      "step": 48100
    },
    {
      "epoch": 0.7086822777916784,
      "grad_norm": 1.979310154914856,
      "learning_rate": 7.208985146563856e-05,
      "loss": 2.0308,
      "step": 48150
    },
    {
      "epoch": 0.7094181887758857,
      "grad_norm": 2.411316394805908,
      "learning_rate": 7.203798498227779e-05,
      "loss": 1.9635,
      "step": 48200
    },
    {
      "epoch": 0.710154099760093,
      "grad_norm": 2.0226027965545654,
      "learning_rate": 7.198608905048611e-05,
      "loss": 2.0341,
      "step": 48250
    },
    {
      "epoch": 0.7108900107443004,
      "grad_norm": 2.349757432937622,
      "learning_rate": 7.193416373960992e-05,
      "loss": 1.9602,
      "step": 48300
    },
    {
      "epoch": 0.7116259217285077,
      "grad_norm": 2.3909099102020264,
      "learning_rate": 7.188220911903478e-05,
      "loss": 1.9903,
      "step": 48350
    },
    {
      "epoch": 0.712361832712715,
      "grad_norm": 2.2437243461608887,
      "learning_rate": 7.183022525818549e-05,
      "loss": 1.9273,
      "step": 48400
    },
    {
      "epoch": 0.7130977436969225,
      "grad_norm": 2.0593621730804443,
      "learning_rate": 7.177821222652588e-05,
      "loss": 1.9767,
      "step": 48450
    },
    {
      "epoch": 0.7138336546811298,
      "grad_norm": 2.154322862625122,
      "learning_rate": 7.172617009355877e-05,
      "loss": 1.9495,
      "step": 48500
    },
    {
      "epoch": 0.7145695656653371,
      "grad_norm": 2.141200542449951,
      "learning_rate": 7.167409892882588e-05,
      "loss": 2.0173,
      "step": 48550
    },
    {
      "epoch": 0.7153054766495445,
      "grad_norm": 2.1297175884246826,
      "learning_rate": 7.162199880190772e-05,
      "loss": 2.0785,
      "step": 48600
    },
    {
      "epoch": 0.7160413876337518,
      "grad_norm": 2.202470541000366,
      "learning_rate": 7.156986978242349e-05,
      "loss": 2.0506,
      "step": 48650
    },
    {
      "epoch": 0.7167772986179591,
      "grad_norm": 2.2761430740356445,
      "learning_rate": 7.151771194003098e-05,
      "loss": 1.9591,
      "step": 48700
    },
    {
      "epoch": 0.7175132096021665,
      "grad_norm": 1.9304052591323853,
      "learning_rate": 7.146552534442654e-05,
      "loss": 1.9725,
      "step": 48750
    },
    {
      "epoch": 0.7182491205863739,
      "grad_norm": 1.914645791053772,
      "learning_rate": 7.141331006534489e-05,
      "loss": 1.9364,
      "step": 48800
    },
    {
      "epoch": 0.7189850315705812,
      "grad_norm": 2.070723533630371,
      "learning_rate": 7.136106617255916e-05,
      "loss": 1.9898,
      "step": 48850
    },
    {
      "epoch": 0.7197209425547886,
      "grad_norm": 1.6305971145629883,
      "learning_rate": 7.130879373588061e-05,
      "loss": 2.0526,
      "step": 48900
    },
    {
      "epoch": 0.7204568535389959,
      "grad_norm": 2.332184076309204,
      "learning_rate": 7.125649282515876e-05,
      "loss": 1.9904,
      "step": 48950
    },
    {
      "epoch": 0.7211927645232032,
      "grad_norm": 2.0407447814941406,
      "learning_rate": 7.120416351028106e-05,
      "loss": 2.0066,
      "step": 49000
    },
    {
      "epoch": 0.7219286755074106,
      "grad_norm": 1.9695219993591309,
      "learning_rate": 7.115180586117296e-05,
      "loss": 2.0065,
      "step": 49050
    },
    {
      "epoch": 0.722664586491618,
      "grad_norm": 1.9658828973770142,
      "learning_rate": 7.109941994779781e-05,
      "loss": 1.9735,
      "step": 49100
    },
    {
      "epoch": 0.7234004974758254,
      "grad_norm": 2.1637699604034424,
      "learning_rate": 7.104700584015669e-05,
      "loss": 1.9779,
      "step": 49150
    },
    {
      "epoch": 0.7241364084600327,
      "grad_norm": 1.7108842134475708,
      "learning_rate": 7.099456360828837e-05,
      "loss": 1.9697,
      "step": 49200
    },
    {
      "epoch": 0.72487231944424,
      "grad_norm": 2.169436454772949,
      "learning_rate": 7.094209332226918e-05,
      "loss": 1.9676,
      "step": 49250
    },
    {
      "epoch": 0.7256082304284474,
      "grad_norm": 2.147289752960205,
      "learning_rate": 7.088959505221298e-05,
      "loss": 1.9723,
      "step": 49300
    },
    {
      "epoch": 0.7263441414126547,
      "grad_norm": 1.911899209022522,
      "learning_rate": 7.083706886827098e-05,
      "loss": 2.0083,
      "step": 49350
    },
    {
      "epoch": 0.7270800523968621,
      "grad_norm": 2.2279253005981445,
      "learning_rate": 7.07845148406317e-05,
      "loss": 1.9214,
      "step": 49400
    },
    {
      "epoch": 0.7278159633810695,
      "grad_norm": 2.1808276176452637,
      "learning_rate": 7.073193303952086e-05,
      "loss": 1.9828,
      "step": 49450
    },
    {
      "epoch": 0.7285518743652768,
      "grad_norm": 2.0813231468200684,
      "learning_rate": 7.067932353520135e-05,
      "loss": 1.9542,
      "step": 49500
    },
    {
      "epoch": 0.7292877853494841,
      "grad_norm": 2.1850709915161133,
      "learning_rate": 7.062668639797299e-05,
      "loss": 1.9734,
      "step": 49550
    },
    {
      "epoch": 0.7300236963336915,
      "grad_norm": 2.0419116020202637,
      "learning_rate": 7.057402169817262e-05,
      "loss": 2.0136,
      "step": 49600
    },
    {
      "epoch": 0.7307596073178988,
      "grad_norm": 2.0548183917999268,
      "learning_rate": 7.05213295061738e-05,
      "loss": 2.0159,
      "step": 49650
    },
    {
      "epoch": 0.7314955183021061,
      "grad_norm": 2.0102715492248535,
      "learning_rate": 7.046860989238692e-05,
      "loss": 1.9919,
      "step": 49700
    },
    {
      "epoch": 0.7322314292863136,
      "grad_norm": 2.0732452869415283,
      "learning_rate": 7.041586292725896e-05,
      "loss": 2.0207,
      "step": 49750
    },
    {
      "epoch": 0.7329673402705209,
      "grad_norm": 2.214763641357422,
      "learning_rate": 7.036308868127351e-05,
      "loss": 1.9721,
      "step": 49800
    },
    {
      "epoch": 0.7337032512547282,
      "grad_norm": 2.2247745990753174,
      "learning_rate": 7.031028722495051e-05,
      "loss": 1.9635,
      "step": 49850
    },
    {
      "epoch": 0.7344391622389356,
      "grad_norm": 2.150618314743042,
      "learning_rate": 7.025745862884637e-05,
      "loss": 1.9586,
      "step": 49900
    },
    {
      "epoch": 0.7351750732231429,
      "grad_norm": 2.0530498027801514,
      "learning_rate": 7.02046029635537e-05,
      "loss": 1.9712,
      "step": 49950
    },
    {
      "epoch": 0.7359109842073502,
      "grad_norm": 2.419154405593872,
      "learning_rate": 7.01517202997013e-05,
      "loss": 2.0236,
      "step": 50000
    },
    {
      "epoch": 0.7366468951915577,
      "grad_norm": 1.8829072713851929,
      "learning_rate": 7.009881070795403e-05,
      "loss": 1.9869,
      "step": 50050
    },
    {
      "epoch": 0.737382806175765,
      "grad_norm": 1.9436888694763184,
      "learning_rate": 7.004587425901277e-05,
      "loss": 1.9361,
      "step": 50100
    },
    {
      "epoch": 0.7381187171599723,
      "grad_norm": 1.9874234199523926,
      "learning_rate": 6.999291102361427e-05,
      "loss": 1.9261,
      "step": 50150
    },
    {
      "epoch": 0.7388546281441797,
      "grad_norm": 1.9775464534759521,
      "learning_rate": 6.993992107253104e-05,
      "loss": 1.9763,
      "step": 50200
    },
    {
      "epoch": 0.739590539128387,
      "grad_norm": 2.0262112617492676,
      "learning_rate": 6.988690447657137e-05,
      "loss": 1.976,
      "step": 50250
    },
    {
      "epoch": 0.7403264501125943,
      "grad_norm": 2.2598679065704346,
      "learning_rate": 6.983386130657906e-05,
      "loss": 1.9534,
      "step": 50300
    },
    {
      "epoch": 0.7410623610968017,
      "grad_norm": 2.0087075233459473,
      "learning_rate": 6.978079163343347e-05,
      "loss": 1.9224,
      "step": 50350
    },
    {
      "epoch": 0.7417982720810091,
      "grad_norm": 2.07368803024292,
      "learning_rate": 6.972769552804937e-05,
      "loss": 2.005,
      "step": 50400
    },
    {
      "epoch": 0.7425341830652165,
      "grad_norm": 2.3116798400878906,
      "learning_rate": 6.967457306137687e-05,
      "loss": 1.9439,
      "step": 50450
    },
    {
      "epoch": 0.7432700940494238,
      "grad_norm": 2.1703100204467773,
      "learning_rate": 6.962142430440126e-05,
      "loss": 1.9708,
      "step": 50500
    },
    {
      "epoch": 0.7440060050336311,
      "grad_norm": 2.159639358520508,
      "learning_rate": 6.956824932814299e-05,
      "loss": 2.0074,
      "step": 50550
    },
    {
      "epoch": 0.7447419160178385,
      "grad_norm": 1.7279272079467773,
      "learning_rate": 6.951504820365754e-05,
      "loss": 1.9567,
      "step": 50600
    },
    {
      "epoch": 0.7454778270020458,
      "grad_norm": 2.155816078186035,
      "learning_rate": 6.946182100203534e-05,
      "loss": 1.9809,
      "step": 50650
    },
    {
      "epoch": 0.7462137379862532,
      "grad_norm": 1.9140743017196655,
      "learning_rate": 6.940856779440165e-05,
      "loss": 2.0649,
      "step": 50700
    },
    {
      "epoch": 0.7469496489704606,
      "grad_norm": 2.2922656536102295,
      "learning_rate": 6.935528865191649e-05,
      "loss": 1.9445,
      "step": 50750
    },
    {
      "epoch": 0.7476855599546679,
      "grad_norm": 2.1520841121673584,
      "learning_rate": 6.930198364577453e-05,
      "loss": 1.9768,
      "step": 50800
    },
    {
      "epoch": 0.7484214709388752,
      "grad_norm": 1.6838291883468628,
      "learning_rate": 6.9248652847205e-05,
      "loss": 1.9679,
      "step": 50850
    },
    {
      "epoch": 0.7491573819230826,
      "grad_norm": 1.9275925159454346,
      "learning_rate": 6.919529632747161e-05,
      "loss": 2.0012,
      "step": 50900
    },
    {
      "epoch": 0.7498932929072899,
      "grad_norm": 2.1520071029663086,
      "learning_rate": 6.914191415787243e-05,
      "loss": 1.9312,
      "step": 50950
    },
    {
      "epoch": 0.7506292038914972,
      "grad_norm": 2.181675434112549,
      "learning_rate": 6.908850640973981e-05,
      "loss": 2.0512,
      "step": 51000
    },
    {
      "epoch": 0.7513651148757047,
      "grad_norm": 2.28106427192688,
      "learning_rate": 6.903507315444025e-05,
      "loss": 1.8988,
      "step": 51050
    },
    {
      "epoch": 0.752101025859912,
      "grad_norm": 1.9399179220199585,
      "learning_rate": 6.898161446337437e-05,
      "loss": 2.0252,
      "step": 51100
    },
    {
      "epoch": 0.7528369368441193,
      "grad_norm": 1.8652279376983643,
      "learning_rate": 6.892813040797678e-05,
      "loss": 1.9148,
      "step": 51150
    },
    {
      "epoch": 0.7535728478283267,
      "grad_norm": 2.432137966156006,
      "learning_rate": 6.887462105971595e-05,
      "loss": 2.0003,
      "step": 51200
    },
    {
      "epoch": 0.754308758812534,
      "grad_norm": 2.223405122756958,
      "learning_rate": 6.882108649009418e-05,
      "loss": 1.9638,
      "step": 51250
    },
    {
      "epoch": 0.7550446697967413,
      "grad_norm": 2.1160035133361816,
      "learning_rate": 6.876752677064745e-05,
      "loss": 2.0485,
      "step": 51300
    },
    {
      "epoch": 0.7557805807809488,
      "grad_norm": 1.7927567958831787,
      "learning_rate": 6.871394197294536e-05,
      "loss": 1.9444,
      "step": 51350
    },
    {
      "epoch": 0.7565164917651561,
      "grad_norm": 2.1701977252960205,
      "learning_rate": 6.866033216859101e-05,
      "loss": 1.9502,
      "step": 51400
    },
    {
      "epoch": 0.7572524027493635,
      "grad_norm": 2.182844638824463,
      "learning_rate": 6.860669742922096e-05,
      "loss": 1.9149,
      "step": 51450
    },
    {
      "epoch": 0.7579883137335708,
      "grad_norm": 2.185004949569702,
      "learning_rate": 6.855303782650501e-05,
      "loss": 2.0422,
      "step": 51500
    },
    {
      "epoch": 0.7587242247177781,
      "grad_norm": 2.1693854331970215,
      "learning_rate": 6.849935343214624e-05,
      "loss": 1.9902,
      "step": 51550
    },
    {
      "epoch": 0.7594601357019855,
      "grad_norm": 2.2821905612945557,
      "learning_rate": 6.844564431788085e-05,
      "loss": 1.9522,
      "step": 51600
    },
    {
      "epoch": 0.7601960466861928,
      "grad_norm": 2.346890449523926,
      "learning_rate": 6.839191055547808e-05,
      "loss": 1.9442,
      "step": 51650
    },
    {
      "epoch": 0.7609319576704002,
      "grad_norm": 2.493281364440918,
      "learning_rate": 6.833815221674005e-05,
      "loss": 2.0221,
      "step": 51700
    },
    {
      "epoch": 0.7616678686546076,
      "grad_norm": 2.5566835403442383,
      "learning_rate": 6.82843693735018e-05,
      "loss": 2.0026,
      "step": 51750
    },
    {
      "epoch": 0.7624037796388149,
      "grad_norm": 2.1505484580993652,
      "learning_rate": 6.823056209763107e-05,
      "loss": 1.9871,
      "step": 51800
    },
    {
      "epoch": 0.7631396906230222,
      "grad_norm": 2.1611459255218506,
      "learning_rate": 6.817673046102825e-05,
      "loss": 1.9651,
      "step": 51850
    },
    {
      "epoch": 0.7638756016072296,
      "grad_norm": 1.8647565841674805,
      "learning_rate": 6.812287453562629e-05,
      "loss": 1.9902,
      "step": 51900
    },
    {
      "epoch": 0.7646115125914369,
      "grad_norm": 2.01111102104187,
      "learning_rate": 6.80689943933906e-05,
      "loss": 1.9682,
      "step": 51950
    },
    {
      "epoch": 0.7653474235756443,
      "grad_norm": 2.188262701034546,
      "learning_rate": 6.801509010631895e-05,
      "loss": 1.928,
      "step": 52000
    },
    {
      "epoch": 0.7660833345598517,
      "grad_norm": 2.0859007835388184,
      "learning_rate": 6.796116174644134e-05,
      "loss": 1.9999,
      "step": 52050
    },
    {
      "epoch": 0.766819245544059,
      "grad_norm": 2.137799024581909,
      "learning_rate": 6.790720938581997e-05,
      "loss": 1.9633,
      "step": 52100
    },
    {
      "epoch": 0.7675551565282663,
      "grad_norm": 1.8898332118988037,
      "learning_rate": 6.785323309654915e-05,
      "loss": 1.9974,
      "step": 52150
    },
    {
      "epoch": 0.7682910675124737,
      "grad_norm": 1.982764482498169,
      "learning_rate": 6.779923295075507e-05,
      "loss": 1.9715,
      "step": 52200
    },
    {
      "epoch": 0.769026978496681,
      "grad_norm": 1.9110991954803467,
      "learning_rate": 6.774520902059589e-05,
      "loss": 2.014,
      "step": 52250
    },
    {
      "epoch": 0.7697628894808884,
      "grad_norm": 2.0636367797851562,
      "learning_rate": 6.769116137826146e-05,
      "loss": 1.9593,
      "step": 52300
    },
    {
      "epoch": 0.7704988004650958,
      "grad_norm": 2.1658968925476074,
      "learning_rate": 6.763709009597342e-05,
      "loss": 1.9794,
      "step": 52350
    },
    {
      "epoch": 0.7712347114493031,
      "grad_norm": 1.7924185991287231,
      "learning_rate": 6.758299524598493e-05,
      "loss": 1.957,
      "step": 52400
    },
    {
      "epoch": 0.7719706224335104,
      "grad_norm": 2.1293256282806396,
      "learning_rate": 6.752887690058064e-05,
      "loss": 1.9561,
      "step": 52450
    },
    {
      "epoch": 0.7727065334177178,
      "grad_norm": 2.249880313873291,
      "learning_rate": 6.747473513207664e-05,
      "loss": 1.9573,
      "step": 52500
    },
    {
      "epoch": 0.7734424444019251,
      "grad_norm": 2.2206079959869385,
      "learning_rate": 6.74205700128203e-05,
      "loss": 2.0195,
      "step": 52550
    },
    {
      "epoch": 0.7741783553861324,
      "grad_norm": 1.857671856880188,
      "learning_rate": 6.736638161519015e-05,
      "loss": 1.9228,
      "step": 52600
    },
    {
      "epoch": 0.7749142663703399,
      "grad_norm": 2.2245073318481445,
      "learning_rate": 6.731217001159589e-05,
      "loss": 1.9257,
      "step": 52650
    },
    {
      "epoch": 0.7756501773545472,
      "grad_norm": 2.6407127380371094,
      "learning_rate": 6.72579352744782e-05,
      "loss": 1.9737,
      "step": 52700
    },
    {
      "epoch": 0.7763860883387546,
      "grad_norm": 1.984390139579773,
      "learning_rate": 6.720367747630865e-05,
      "loss": 1.9981,
      "step": 52750
    },
    {
      "epoch": 0.7771219993229619,
      "grad_norm": 2.0022478103637695,
      "learning_rate": 6.71493966895897e-05,
      "loss": 1.9776,
      "step": 52800
    },
    {
      "epoch": 0.7778579103071692,
      "grad_norm": 2.123533010482788,
      "learning_rate": 6.709509298685443e-05,
      "loss": 1.9728,
      "step": 52850
    },
    {
      "epoch": 0.7785938212913766,
      "grad_norm": 2.0820255279541016,
      "learning_rate": 6.704076644066661e-05,
      "loss": 1.9468,
      "step": 52900
    },
    {
      "epoch": 0.779329732275584,
      "grad_norm": 2.082064151763916,
      "learning_rate": 6.698641712362053e-05,
      "loss": 1.9934,
      "step": 52950
    },
    {
      "epoch": 0.7800656432597913,
      "grad_norm": 1.868796706199646,
      "learning_rate": 6.693204510834085e-05,
      "loss": 2.0081,
      "step": 53000
    },
    {
      "epoch": 0.7808015542439987,
      "grad_norm": 2.1281652450561523,
      "learning_rate": 6.687765046748266e-05,
      "loss": 1.9713,
      "step": 53050
    },
    {
      "epoch": 0.781537465228206,
      "grad_norm": 2.188688039779663,
      "learning_rate": 6.682323327373118e-05,
      "loss": 1.9674,
      "step": 53100
    },
    {
      "epoch": 0.7822733762124133,
      "grad_norm": 2.2902889251708984,
      "learning_rate": 6.676879359980182e-05,
      "loss": 1.9366,
      "step": 53150
    },
    {
      "epoch": 0.7830092871966207,
      "grad_norm": 1.9810749292373657,
      "learning_rate": 6.671433151844006e-05,
      "loss": 1.9051,
      "step": 53200
    },
    {
      "epoch": 0.783745198180828,
      "grad_norm": 2.253436326980591,
      "learning_rate": 6.665984710242124e-05,
      "loss": 1.9678,
      "step": 53250
    },
    {
      "epoch": 0.7844811091650354,
      "grad_norm": 2.197518825531006,
      "learning_rate": 6.66053404245506e-05,
      "loss": 1.9486,
      "step": 53300
    },
    {
      "epoch": 0.7852170201492428,
      "grad_norm": 2.224349021911621,
      "learning_rate": 6.655081155766314e-05,
      "loss": 1.9851,
      "step": 53350
    },
    {
      "epoch": 0.7859529311334501,
      "grad_norm": 1.8918505907058716,
      "learning_rate": 6.649626057462348e-05,
      "loss": 2.0056,
      "step": 53400
    },
    {
      "epoch": 0.7866888421176574,
      "grad_norm": 2.0334606170654297,
      "learning_rate": 6.644168754832578e-05,
      "loss": 1.9503,
      "step": 53450
    },
    {
      "epoch": 0.7874247531018648,
      "grad_norm": 2.4949545860290527,
      "learning_rate": 6.638709255169369e-05,
      "loss": 1.9322,
      "step": 53500
    },
    {
      "epoch": 0.7881606640860721,
      "grad_norm": 2.018327474594116,
      "learning_rate": 6.633247565768021e-05,
      "loss": 2.0025,
      "step": 53550
    },
    {
      "epoch": 0.7888965750702795,
      "grad_norm": 1.9407150745391846,
      "learning_rate": 6.627783693926757e-05,
      "loss": 1.8751,
      "step": 53600
    },
    {
      "epoch": 0.7896324860544869,
      "grad_norm": 2.0427258014678955,
      "learning_rate": 6.62231764694672e-05,
      "loss": 1.9751,
      "step": 53650
    },
    {
      "epoch": 0.7903683970386942,
      "grad_norm": 1.9426146745681763,
      "learning_rate": 6.61684943213196e-05,
      "loss": 1.9187,
      "step": 53700
    },
    {
      "epoch": 0.7911043080229015,
      "grad_norm": 2.1778604984283447,
      "learning_rate": 6.611379056789421e-05,
      "loss": 1.9429,
      "step": 53750
    },
    {
      "epoch": 0.7918402190071089,
      "grad_norm": 2.064872980117798,
      "learning_rate": 6.605906528228937e-05,
      "loss": 1.9166,
      "step": 53800
    },
    {
      "epoch": 0.7925761299913162,
      "grad_norm": 1.865333914756775,
      "learning_rate": 6.600431853763213e-05,
      "loss": 1.9535,
      "step": 53850
    },
    {
      "epoch": 0.7933120409755235,
      "grad_norm": 2.1424498558044434,
      "learning_rate": 6.59495504070783e-05,
      "loss": 1.9375,
      "step": 53900
    },
    {
      "epoch": 0.794047951959731,
      "grad_norm": 1.8767508268356323,
      "learning_rate": 6.589476096381218e-05,
      "loss": 1.9732,
      "step": 53950
    },
    {
      "epoch": 0.7947838629439383,
      "grad_norm": 2.6583287715911865,
      "learning_rate": 6.583995028104663e-05,
      "loss": 1.9342,
      "step": 54000
    },
    {
      "epoch": 0.7955197739281457,
      "grad_norm": 2.0145418643951416,
      "learning_rate": 6.578511843202286e-05,
      "loss": 1.9155,
      "step": 54050
    },
    {
      "epoch": 0.796255684912353,
      "grad_norm": 1.9446485042572021,
      "learning_rate": 6.573026549001034e-05,
      "loss": 1.984,
      "step": 54100
    },
    {
      "epoch": 0.7969915958965603,
      "grad_norm": 2.1438961029052734,
      "learning_rate": 6.567539152830676e-05,
      "loss": 1.9412,
      "step": 54150
    },
    {
      "epoch": 0.7977275068807677,
      "grad_norm": 2.0139260292053223,
      "learning_rate": 6.562049662023787e-05,
      "loss": 1.9674,
      "step": 54200
    },
    {
      "epoch": 0.7984634178649751,
      "grad_norm": 2.1208744049072266,
      "learning_rate": 6.556558083915744e-05,
      "loss": 1.9379,
      "step": 54250
    },
    {
      "epoch": 0.7991993288491824,
      "grad_norm": 2.661149740219116,
      "learning_rate": 6.551064425844709e-05,
      "loss": 1.9391,
      "step": 54300
    },
    {
      "epoch": 0.7999352398333898,
      "grad_norm": 2.0866475105285645,
      "learning_rate": 6.545568695151628e-05,
      "loss": 1.9675,
      "step": 54350
    },
    {
      "epoch": 0.8006711508175971,
      "grad_norm": 1.9236136674880981,
      "learning_rate": 6.540070899180213e-05,
      "loss": 1.9433,
      "step": 54400
    },
    {
      "epoch": 0.8014070618018044,
      "grad_norm": 2.20888352394104,
      "learning_rate": 6.534571045276939e-05,
      "loss": 1.9773,
      "step": 54450
    },
    {
      "epoch": 0.8021429727860118,
      "grad_norm": 2.0321156978607178,
      "learning_rate": 6.529069140791027e-05,
      "loss": 1.9444,
      "step": 54500
    },
    {
      "epoch": 0.8028788837702192,
      "grad_norm": 2.147765636444092,
      "learning_rate": 6.523565193074441e-05,
      "loss": 1.9513,
      "step": 54550
    },
    {
      "epoch": 0.8036147947544265,
      "grad_norm": 1.8648512363433838,
      "learning_rate": 6.518059209481875e-05,
      "loss": 1.9442,
      "step": 54600
    },
    {
      "epoch": 0.8043507057386339,
      "grad_norm": 2.0816166400909424,
      "learning_rate": 6.512551197370742e-05,
      "loss": 1.9623,
      "step": 54650
    },
    {
      "epoch": 0.8050866167228412,
      "grad_norm": 2.046621799468994,
      "learning_rate": 6.507041164101168e-05,
      "loss": 1.9392,
      "step": 54700
    },
    {
      "epoch": 0.8058225277070485,
      "grad_norm": 2.0626261234283447,
      "learning_rate": 6.501529117035976e-05,
      "loss": 1.9603,
      "step": 54750
    },
    {
      "epoch": 0.8065584386912559,
      "grad_norm": 1.7383619546890259,
      "learning_rate": 6.496015063540685e-05,
      "loss": 1.9743,
      "step": 54800
    },
    {
      "epoch": 0.8072943496754632,
      "grad_norm": 2.2329676151275635,
      "learning_rate": 6.49049901098349e-05,
      "loss": 1.9459,
      "step": 54850
    },
    {
      "epoch": 0.8080302606596707,
      "grad_norm": 2.0502564907073975,
      "learning_rate": 6.484980966735259e-05,
      "loss": 1.985,
      "step": 54900
    },
    {
      "epoch": 0.808766171643878,
      "grad_norm": 2.0138001441955566,
      "learning_rate": 6.479460938169525e-05,
      "loss": 1.9644,
      "step": 54950
    },
    {
      "epoch": 0.8095020826280853,
      "grad_norm": 2.2292287349700928,
      "learning_rate": 6.47393893266247e-05,
      "loss": 1.9896,
      "step": 55000
    },
    {
      "epoch": 0.8102379936122927,
      "grad_norm": 2.252042055130005,
      "learning_rate": 6.468414957592912e-05,
      "loss": 1.9813,
      "step": 55050
    },
    {
      "epoch": 0.8109739045965,
      "grad_norm": 1.949156641960144,
      "learning_rate": 6.462889020342312e-05,
      "loss": 1.9578,
      "step": 55100
    },
    {
      "epoch": 0.8117098155807073,
      "grad_norm": 1.893305778503418,
      "learning_rate": 6.457361128294744e-05,
      "loss": 1.9032,
      "step": 55150
    },
    {
      "epoch": 0.8124457265649148,
      "grad_norm": 2.051551342010498,
      "learning_rate": 6.451831288836899e-05,
      "loss": 1.9839,
      "step": 55200
    },
    {
      "epoch": 0.8131816375491221,
      "grad_norm": 2.3925657272338867,
      "learning_rate": 6.446299509358066e-05,
      "loss": 1.9185,
      "step": 55250
    },
    {
      "epoch": 0.8139175485333294,
      "grad_norm": 2.0961976051330566,
      "learning_rate": 6.440765797250133e-05,
      "loss": 1.9556,
      "step": 55300
    },
    {
      "epoch": 0.8146534595175368,
      "grad_norm": 1.8685270547866821,
      "learning_rate": 6.435230159907563e-05,
      "loss": 1.9615,
      "step": 55350
    },
    {
      "epoch": 0.8153893705017441,
      "grad_norm": 2.1394107341766357,
      "learning_rate": 6.429692604727395e-05,
      "loss": 1.9907,
      "step": 55400
    },
    {
      "epoch": 0.8161252814859514,
      "grad_norm": 2.0045900344848633,
      "learning_rate": 6.424153139109233e-05,
      "loss": 1.8926,
      "step": 55450
    },
    {
      "epoch": 0.8168611924701588,
      "grad_norm": 2.341982126235962,
      "learning_rate": 6.418611770455228e-05,
      "loss": 1.9279,
      "step": 55500
    },
    {
      "epoch": 0.8175971034543662,
      "grad_norm": 1.9182305335998535,
      "learning_rate": 6.41306850617008e-05,
      "loss": 1.8641,
      "step": 55550
    },
    {
      "epoch": 0.8183330144385735,
      "grad_norm": 2.324741840362549,
      "learning_rate": 6.40752335366102e-05,
      "loss": 1.9732,
      "step": 55600
    },
    {
      "epoch": 0.8190689254227809,
      "grad_norm": 2.401738405227661,
      "learning_rate": 6.4019763203378e-05,
      "loss": 1.9593,
      "step": 55650
    },
    {
      "epoch": 0.8198048364069882,
      "grad_norm": 1.8799383640289307,
      "learning_rate": 6.396427413612688e-05,
      "loss": 1.8986,
      "step": 55700
    },
    {
      "epoch": 0.8205407473911955,
      "grad_norm": 2.235391616821289,
      "learning_rate": 6.390876640900454e-05,
      "loss": 1.9523,
      "step": 55750
    },
    {
      "epoch": 0.8212766583754029,
      "grad_norm": 2.257097005844116,
      "learning_rate": 6.38532400961836e-05,
      "loss": 2.0183,
      "step": 55800
    },
    {
      "epoch": 0.8220125693596103,
      "grad_norm": 2.134484052658081,
      "learning_rate": 6.379769527186157e-05,
      "loss": 1.9325,
      "step": 55850
    },
    {
      "epoch": 0.8227484803438176,
      "grad_norm": 2.2576892375946045,
      "learning_rate": 6.374213201026062e-05,
      "loss": 1.9129,
      "step": 55900
    },
    {
      "epoch": 0.823484391328025,
      "grad_norm": 1.8664743900299072,
      "learning_rate": 6.368655038562763e-05,
      "loss": 1.9461,
      "step": 55950
    },
    {
      "epoch": 0.8242203023122323,
      "grad_norm": 2.007537603378296,
      "learning_rate": 6.363095047223396e-05,
      "loss": 1.9071,
      "step": 56000
    },
    {
      "epoch": 0.8249562132964396,
      "grad_norm": 2.212252140045166,
      "learning_rate": 6.357533234437546e-05,
      "loss": 1.9359,
      "step": 56050
    },
    {
      "epoch": 0.825692124280647,
      "grad_norm": 2.0252268314361572,
      "learning_rate": 6.351969607637225e-05,
      "loss": 1.9482,
      "step": 56100
    },
    {
      "epoch": 0.8264280352648543,
      "grad_norm": 1.9969592094421387,
      "learning_rate": 6.346404174256875e-05,
      "loss": 1.9615,
      "step": 56150
    },
    {
      "epoch": 0.8271639462490618,
      "grad_norm": 2.290156602859497,
      "learning_rate": 6.340836941733349e-05,
      "loss": 1.8802,
      "step": 56200
    },
    {
      "epoch": 0.8278998572332691,
      "grad_norm": 1.9913694858551025,
      "learning_rate": 6.335267917505909e-05,
      "loss": 1.9565,
      "step": 56250
    },
    {
      "epoch": 0.8286357682174764,
      "grad_norm": 2.139108419418335,
      "learning_rate": 6.329697109016204e-05,
      "loss": 1.9417,
      "step": 56300
    },
    {
      "epoch": 0.8293716792016838,
      "grad_norm": 2.3311445713043213,
      "learning_rate": 6.32412452370827e-05,
      "loss": 1.9317,
      "step": 56350
    },
    {
      "epoch": 0.8301075901858911,
      "grad_norm": 1.9920424222946167,
      "learning_rate": 6.318550169028522e-05,
      "loss": 1.9668,
      "step": 56400
    },
    {
      "epoch": 0.8308435011700984,
      "grad_norm": 2.155425786972046,
      "learning_rate": 6.312974052425732e-05,
      "loss": 1.9555,
      "step": 56450
    },
    {
      "epoch": 0.8315794121543059,
      "grad_norm": 1.7935768365859985,
      "learning_rate": 6.30739618135103e-05,
      "loss": 1.9105,
      "step": 56500
    },
    {
      "epoch": 0.8323153231385132,
      "grad_norm": 2.2282207012176514,
      "learning_rate": 6.301816563257893e-05,
      "loss": 1.9022,
      "step": 56550
    },
    {
      "epoch": 0.8330512341227205,
      "grad_norm": 2.0323078632354736,
      "learning_rate": 6.296235205602126e-05,
      "loss": 1.9054,
      "step": 56600
    },
    {
      "epoch": 0.8337871451069279,
      "grad_norm": 2.2122159004211426,
      "learning_rate": 6.290652115841864e-05,
      "loss": 1.9868,
      "step": 56650
    },
    {
      "epoch": 0.8345230560911352,
      "grad_norm": 1.8917738199234009,
      "learning_rate": 6.285067301437552e-05,
      "loss": 1.8886,
      "step": 56700
    },
    {
      "epoch": 0.8352589670753425,
      "grad_norm": 2.590712547302246,
      "learning_rate": 6.279480769851946e-05,
      "loss": 1.959,
      "step": 56750
    },
    {
      "epoch": 0.8359948780595499,
      "grad_norm": 2.264035701751709,
      "learning_rate": 6.273892528550091e-05,
      "loss": 1.9447,
      "step": 56800
    },
    {
      "epoch": 0.8367307890437573,
      "grad_norm": 2.209861993789673,
      "learning_rate": 6.268302584999318e-05,
      "loss": 1.9071,
      "step": 56850
    },
    {
      "epoch": 0.8374667000279646,
      "grad_norm": 1.758234977722168,
      "learning_rate": 6.262710946669235e-05,
      "loss": 1.91,
      "step": 56900
    },
    {
      "epoch": 0.838202611012172,
      "grad_norm": 2.0343635082244873,
      "learning_rate": 6.257117621031711e-05,
      "loss": 1.8902,
      "step": 56950
    },
    {
      "epoch": 0.8389385219963793,
      "grad_norm": 2.3664326667785645,
      "learning_rate": 6.25152261556087e-05,
      "loss": 1.9278,
      "step": 57000
    },
    {
      "epoch": 0.8396744329805866,
      "grad_norm": 2.0257086753845215,
      "learning_rate": 6.245925937733086e-05,
      "loss": 1.9565,
      "step": 57050
    },
    {
      "epoch": 0.840410343964794,
      "grad_norm": 1.7791410684585571,
      "learning_rate": 6.24032759502696e-05,
      "loss": 1.9034,
      "step": 57100
    },
    {
      "epoch": 0.8411462549490014,
      "grad_norm": 2.1251323223114014,
      "learning_rate": 6.234727594923324e-05,
      "loss": 1.9291,
      "step": 57150
    },
    {
      "epoch": 0.8418821659332087,
      "grad_norm": 2.1695311069488525,
      "learning_rate": 6.22912594490522e-05,
      "loss": 1.9422,
      "step": 57200
    },
    {
      "epoch": 0.8426180769174161,
      "grad_norm": 1.8766205310821533,
      "learning_rate": 6.223522652457899e-05,
      "loss": 1.9624,
      "step": 57250
    },
    {
      "epoch": 0.8433539879016234,
      "grad_norm": 2.6767947673797607,
      "learning_rate": 6.217917725068805e-05,
      "loss": 1.9359,
      "step": 57300
    },
    {
      "epoch": 0.8440898988858307,
      "grad_norm": 2.3434195518493652,
      "learning_rate": 6.212311170227563e-05,
      "loss": 1.9554,
      "step": 57350
    },
    {
      "epoch": 0.8448258098700381,
      "grad_norm": 1.9602464437484741,
      "learning_rate": 6.206702995425982e-05,
      "loss": 1.9742,
      "step": 57400
    },
    {
      "epoch": 0.8455617208542455,
      "grad_norm": 1.8906139135360718,
      "learning_rate": 6.201093208158022e-05,
      "loss": 1.9034,
      "step": 57450
    },
    {
      "epoch": 0.8462976318384529,
      "grad_norm": 2.372973918914795,
      "learning_rate": 6.19548181591981e-05,
      "loss": 1.9555,
      "step": 57500
    },
    {
      "epoch": 0.8470335428226602,
      "grad_norm": 1.733971118927002,
      "learning_rate": 6.189868826209614e-05,
      "loss": 1.8928,
      "step": 57550
    },
    {
      "epoch": 0.8477694538068675,
      "grad_norm": 1.4523102045059204,
      "learning_rate": 6.184254246527834e-05,
      "loss": 1.9669,
      "step": 57600
    },
    {
      "epoch": 0.8485053647910749,
      "grad_norm": 1.8815158605575562,
      "learning_rate": 6.178638084376995e-05,
      "loss": 1.9657,
      "step": 57650
    },
    {
      "epoch": 0.8492412757752822,
      "grad_norm": 2.019984483718872,
      "learning_rate": 6.173020347261741e-05,
      "loss": 1.9129,
      "step": 57700
    },
    {
      "epoch": 0.8499771867594895,
      "grad_norm": 2.1103196144104004,
      "learning_rate": 6.167401042688815e-05,
      "loss": 1.9454,
      "step": 57750
    },
    {
      "epoch": 0.850713097743697,
      "grad_norm": 2.1278769969940186,
      "learning_rate": 6.161780178167057e-05,
      "loss": 1.9378,
      "step": 57800
    },
    {
      "epoch": 0.8514490087279043,
      "grad_norm": 2.1667540073394775,
      "learning_rate": 6.156157761207392e-05,
      "loss": 1.9119,
      "step": 57850
    },
    {
      "epoch": 0.8521849197121116,
      "grad_norm": 1.9218835830688477,
      "learning_rate": 6.15053379932282e-05,
      "loss": 1.8909,
      "step": 57900
    },
    {
      "epoch": 0.852920830696319,
      "grad_norm": 2.287416696548462,
      "learning_rate": 6.144908300028403e-05,
      "loss": 1.9209,
      "step": 57950
    },
    {
      "epoch": 0.8536567416805263,
      "grad_norm": 2.1059415340423584,
      "learning_rate": 6.13928127084126e-05,
      "loss": 1.9476,
      "step": 58000
    },
    {
      "epoch": 0.8543926526647336,
      "grad_norm": 2.233769178390503,
      "learning_rate": 6.133652719280551e-05,
      "loss": 1.9259,
      "step": 58050
    },
    {
      "epoch": 0.8551285636489411,
      "grad_norm": 1.778425693511963,
      "learning_rate": 6.128022652867471e-05,
      "loss": 1.9601,
      "step": 58100
    },
    {
      "epoch": 0.8558644746331484,
      "grad_norm": 2.0156192779541016,
      "learning_rate": 6.122391079125246e-05,
      "loss": 1.9051,
      "step": 58150
    },
    {
      "epoch": 0.8566003856173557,
      "grad_norm": 2.7551372051239014,
      "learning_rate": 6.116758005579107e-05,
      "loss": 1.9481,
      "step": 58200
    },
    {
      "epoch": 0.8573362966015631,
      "grad_norm": 2.020718574523926,
      "learning_rate": 6.111123439756296e-05,
      "loss": 1.9776,
      "step": 58250
    },
    {
      "epoch": 0.8580722075857704,
      "grad_norm": 2.115818500518799,
      "learning_rate": 6.10548738918604e-05,
      "loss": 1.9877,
      "step": 58300
    },
    {
      "epoch": 0.8588081185699777,
      "grad_norm": 1.9179272651672363,
      "learning_rate": 6.099849861399561e-05,
      "loss": 1.9423,
      "step": 58350
    },
    {
      "epoch": 0.8595440295541851,
      "grad_norm": 2.1910829544067383,
      "learning_rate": 6.094210863930048e-05,
      "loss": 1.98,
      "step": 58400
    },
    {
      "epoch": 0.8602799405383925,
      "grad_norm": 2.1822118759155273,
      "learning_rate": 6.088570404312654e-05,
      "loss": 1.9722,
      "step": 58450
    },
    {
      "epoch": 0.8610158515225999,
      "grad_norm": 2.113339900970459,
      "learning_rate": 6.082928490084493e-05,
      "loss": 1.9234,
      "step": 58500
    },
    {
      "epoch": 0.8617517625068072,
      "grad_norm": 1.7148672342300415,
      "learning_rate": 6.0772851287846124e-05,
      "loss": 1.8615,
      "step": 58550
    },
    {
      "epoch": 0.8624876734910145,
      "grad_norm": 2.0210726261138916,
      "learning_rate": 6.0716403279540004e-05,
      "loss": 1.9136,
      "step": 58600
    },
    {
      "epoch": 0.8632235844752219,
      "grad_norm": 2.1351518630981445,
      "learning_rate": 6.0659940951355654e-05,
      "loss": 1.8673,
      "step": 58650
    },
    {
      "epoch": 0.8639594954594292,
      "grad_norm": 2.0874063968658447,
      "learning_rate": 6.06034643787413e-05,
      "loss": 2.0046,
      "step": 58700
    },
    {
      "epoch": 0.8646954064436366,
      "grad_norm": 2.0640900135040283,
      "learning_rate": 6.0546973637164225e-05,
      "loss": 1.9589,
      "step": 58750
    },
    {
      "epoch": 0.865431317427844,
      "grad_norm": 2.381812810897827,
      "learning_rate": 6.0490468802110615e-05,
      "loss": 2.0218,
      "step": 58800
    },
    {
      "epoch": 0.8661672284120513,
      "grad_norm": 2.082230806350708,
      "learning_rate": 6.04339499490855e-05,
      "loss": 1.9686,
      "step": 58850
    },
    {
      "epoch": 0.8669031393962586,
      "grad_norm": 2.484437942504883,
      "learning_rate": 6.0377417153612646e-05,
      "loss": 1.9022,
      "step": 58900
    },
    {
      "epoch": 0.867639050380466,
      "grad_norm": 2.160332679748535,
      "learning_rate": 6.032087049123445e-05,
      "loss": 1.9238,
      "step": 58950
    },
    {
      "epoch": 0.8683749613646733,
      "grad_norm": 1.8005852699279785,
      "learning_rate": 6.026431003751183e-05,
      "loss": 1.9064,
      "step": 59000
    },
    {
      "epoch": 0.8691108723488806,
      "grad_norm": 1.7270241975784302,
      "learning_rate": 6.020773586802413e-05,
      "loss": 1.9172,
      "step": 59050
    },
    {
      "epoch": 0.8698467833330881,
      "grad_norm": 2.0510759353637695,
      "learning_rate": 6.015114805836903e-05,
      "loss": 1.8801,
      "step": 59100
    },
    {
      "epoch": 0.8705826943172954,
      "grad_norm": 1.9424960613250732,
      "learning_rate": 6.0094546684162455e-05,
      "loss": 1.9236,
      "step": 59150
    },
    {
      "epoch": 0.8713186053015027,
      "grad_norm": 2.149871826171875,
      "learning_rate": 6.0037931821038416e-05,
      "loss": 1.8772,
      "step": 59200
    },
    {
      "epoch": 0.8720545162857101,
      "grad_norm": 2.060520648956299,
      "learning_rate": 5.9981303544648984e-05,
      "loss": 1.8967,
      "step": 59250
    },
    {
      "epoch": 0.8727904272699174,
      "grad_norm": 1.9674365520477295,
      "learning_rate": 5.992466193066413e-05,
      "loss": 1.974,
      "step": 59300
    },
    {
      "epoch": 0.8735263382541247,
      "grad_norm": 2.2634971141815186,
      "learning_rate": 5.986800705477166e-05,
      "loss": 1.8983,
      "step": 59350
    },
    {
      "epoch": 0.8742622492383322,
      "grad_norm": 2.3494389057159424,
      "learning_rate": 5.981133899267708e-05,
      "loss": 1.9364,
      "step": 59400
    },
    {
      "epoch": 0.8749981602225395,
      "grad_norm": 2.066189765930176,
      "learning_rate": 5.975465782010357e-05,
      "loss": 1.92,
      "step": 59450
    },
    {
      "epoch": 0.8757340712067468,
      "grad_norm": 2.112844705581665,
      "learning_rate": 5.969796361279176e-05,
      "loss": 1.9462,
      "step": 59500
    },
    {
      "epoch": 0.8764699821909542,
      "grad_norm": 1.95489501953125,
      "learning_rate": 5.964125644649976e-05,
      "loss": 1.8939,
      "step": 59550
    },
    {
      "epoch": 0.8772058931751615,
      "grad_norm": 2.285566806793213,
      "learning_rate": 5.958453639700296e-05,
      "loss": 1.9182,
      "step": 59600
    },
    {
      "epoch": 0.8779418041593688,
      "grad_norm": 2.150920867919922,
      "learning_rate": 5.952780354009394e-05,
      "loss": 1.8393,
      "step": 59650
    },
    {
      "epoch": 0.8786777151435763,
      "grad_norm": 2.012554168701172,
      "learning_rate": 5.947105795158246e-05,
      "loss": 1.9147,
      "step": 59700
    },
    {
      "epoch": 0.8794136261277836,
      "grad_norm": 1.944409728050232,
      "learning_rate": 5.941429970729524e-05,
      "loss": 1.9293,
      "step": 59750
    },
    {
      "epoch": 0.880149537111991,
      "grad_norm": 2.3600969314575195,
      "learning_rate": 5.935752888307595e-05,
      "loss": 1.9117,
      "step": 59800
    },
    {
      "epoch": 0.8808854480961983,
      "grad_norm": 2.4209814071655273,
      "learning_rate": 5.930074555478504e-05,
      "loss": 1.9463,
      "step": 59850
    },
    {
      "epoch": 0.8816213590804056,
      "grad_norm": 2.061422824859619,
      "learning_rate": 5.924394979829969e-05,
      "loss": 1.9223,
      "step": 59900
    },
    {
      "epoch": 0.882357270064613,
      "grad_norm": 2.183863878250122,
      "learning_rate": 5.918714168951366e-05,
      "loss": 1.9452,
      "step": 59950
    },
    {
      "epoch": 0.8830931810488203,
      "grad_norm": 2.494098663330078,
      "learning_rate": 5.913032130433723e-05,
      "loss": 1.8486,
      "step": 60000
    },
    {
      "epoch": 0.8838290920330277,
      "grad_norm": 2.364905595779419,
      "learning_rate": 5.907348871869711e-05,
      "loss": 1.8952,
      "step": 60050
    },
    {
      "epoch": 0.8845650030172351,
      "grad_norm": 2.1851491928100586,
      "learning_rate": 5.901664400853628e-05,
      "loss": 1.9421,
      "step": 60100
    },
    {
      "epoch": 0.8853009140014424,
      "grad_norm": 2.1776702404022217,
      "learning_rate": 5.895978724981391e-05,
      "loss": 1.9221,
      "step": 60150
    },
    {
      "epoch": 0.8860368249856497,
      "grad_norm": 2.1302499771118164,
      "learning_rate": 5.890291851850532e-05,
      "loss": 1.8692,
      "step": 60200
    },
    {
      "epoch": 0.8867727359698571,
      "grad_norm": 2.139089584350586,
      "learning_rate": 5.884603789060179e-05,
      "loss": 1.923,
      "step": 60250
    },
    {
      "epoch": 0.8875086469540644,
      "grad_norm": 2.1587936878204346,
      "learning_rate": 5.878914544211051e-05,
      "loss": 1.9469,
      "step": 60300
    },
    {
      "epoch": 0.8882445579382718,
      "grad_norm": 2.0174643993377686,
      "learning_rate": 5.8732241249054445e-05,
      "loss": 1.8267,
      "step": 60350
    },
    {
      "epoch": 0.8889804689224792,
      "grad_norm": 1.7778016328811646,
      "learning_rate": 5.867532538747229e-05,
      "loss": 1.9451,
      "step": 60400
    },
    {
      "epoch": 0.8897163799066865,
      "grad_norm": 2.138237476348877,
      "learning_rate": 5.861839793341832e-05,
      "loss": 1.9219,
      "step": 60450
    },
    {
      "epoch": 0.8904522908908938,
      "grad_norm": 2.177090883255005,
      "learning_rate": 5.856145896296227e-05,
      "loss": 1.9016,
      "step": 60500
    },
    {
      "epoch": 0.8911882018751012,
      "grad_norm": 1.9894766807556152,
      "learning_rate": 5.850450855218929e-05,
      "loss": 1.9173,
      "step": 60550
    },
    {
      "epoch": 0.8919241128593085,
      "grad_norm": 2.1229207515716553,
      "learning_rate": 5.8447546777199836e-05,
      "loss": 1.8988,
      "step": 60600
    },
    {
      "epoch": 0.8926600238435158,
      "grad_norm": 2.034970283508301,
      "learning_rate": 5.83905737141095e-05,
      "loss": 1.8807,
      "step": 60650
    },
    {
      "epoch": 0.8933959348277233,
      "grad_norm": 2.4267632961273193,
      "learning_rate": 5.8333589439048994e-05,
      "loss": 1.8653,
      "step": 60700
    },
    {
      "epoch": 0.8941318458119306,
      "grad_norm": 2.3538780212402344,
      "learning_rate": 5.827659402816402e-05,
      "loss": 1.9152,
      "step": 60750
    },
    {
      "epoch": 0.894867756796138,
      "grad_norm": 2.272186517715454,
      "learning_rate": 5.8219587557615144e-05,
      "loss": 1.8485,
      "step": 60800
    },
    {
      "epoch": 0.8956036677803453,
      "grad_norm": 2.401541233062744,
      "learning_rate": 5.81625701035777e-05,
      "loss": 1.9533,
      "step": 60850
    },
    {
      "epoch": 0.8963395787645526,
      "grad_norm": 1.9913396835327148,
      "learning_rate": 5.81055417422417e-05,
      "loss": 1.8998,
      "step": 60900
    },
    {
      "epoch": 0.89707548974876,
      "grad_norm": 2.191797971725464,
      "learning_rate": 5.8048502549811767e-05,
      "loss": 1.8955,
      "step": 60950
    },
    {
      "epoch": 0.8978114007329674,
      "grad_norm": 2.1086387634277344,
      "learning_rate": 5.799145260250693e-05,
      "loss": 1.9139,
      "step": 61000
    },
    {
      "epoch": 0.8985473117171747,
      "grad_norm": 2.0214388370513916,
      "learning_rate": 5.7934391976560684e-05,
      "loss": 1.8871,
      "step": 61050
    },
    {
      "epoch": 0.8992832227013821,
      "grad_norm": 2.5010616779327393,
      "learning_rate": 5.7877320748220696e-05,
      "loss": 1.8968,
      "step": 61100
    },
    {
      "epoch": 0.9000191336855894,
      "grad_norm": 1.8196477890014648,
      "learning_rate": 5.782023899374886e-05,
      "loss": 1.9288,
      "step": 61150
    },
    {
      "epoch": 0.9007550446697967,
      "grad_norm": 2.2619776725769043,
      "learning_rate": 5.776314678942113e-05,
      "loss": 1.9676,
      "step": 61200
    },
    {
      "epoch": 0.9014909556540041,
      "grad_norm": 2.0765252113342285,
      "learning_rate": 5.7706044211527396e-05,
      "loss": 1.9387,
      "step": 61250
    },
    {
      "epoch": 0.9022268666382114,
      "grad_norm": 1.8171191215515137,
      "learning_rate": 5.7648931336371435e-05,
      "loss": 1.9749,
      "step": 61300
    },
    {
      "epoch": 0.9029627776224188,
      "grad_norm": 2.554616928100586,
      "learning_rate": 5.759180824027077e-05,
      "loss": 1.8983,
      "step": 61350
    },
    {
      "epoch": 0.9036986886066262,
      "grad_norm": 2.043393850326538,
      "learning_rate": 5.7534674999556604e-05,
      "loss": 1.9331,
      "step": 61400
    },
    {
      "epoch": 0.9044345995908335,
      "grad_norm": 2.1141488552093506,
      "learning_rate": 5.747753169057366e-05,
      "loss": 1.9294,
      "step": 61450
    },
    {
      "epoch": 0.9051705105750408,
      "grad_norm": 2.4188899993896484,
      "learning_rate": 5.742037838968014e-05,
      "loss": 1.9593,
      "step": 61500
    },
    {
      "epoch": 0.9059064215592482,
      "grad_norm": 2.149535894393921,
      "learning_rate": 5.73632151732476e-05,
      "loss": 1.9209,
      "step": 61550
    },
    {
      "epoch": 0.9066423325434555,
      "grad_norm": 2.3465993404388428,
      "learning_rate": 5.730604211766083e-05,
      "loss": 1.9415,
      "step": 61600
    },
    {
      "epoch": 0.9073782435276629,
      "grad_norm": 2.163700580596924,
      "learning_rate": 5.724885929931777e-05,
      "loss": 1.9582,
      "step": 61650
    },
    {
      "epoch": 0.9081141545118703,
      "grad_norm": 2.2779197692871094,
      "learning_rate": 5.719166679462943e-05,
      "loss": 1.9171,
      "step": 61700
    },
    {
      "epoch": 0.9088500654960776,
      "grad_norm": 2.31176495552063,
      "learning_rate": 5.713446468001973e-05,
      "loss": 1.939,
      "step": 61750
    },
    {
      "epoch": 0.9095859764802849,
      "grad_norm": 2.038696050643921,
      "learning_rate": 5.707725303192546e-05,
      "loss": 1.934,
      "step": 61800
    },
    {
      "epoch": 0.9103218874644923,
      "grad_norm": 2.3725080490112305,
      "learning_rate": 5.70200319267961e-05,
      "loss": 1.8597,
      "step": 61850
    },
    {
      "epoch": 0.9110577984486996,
      "grad_norm": 1.9400694370269775,
      "learning_rate": 5.696280144109385e-05,
      "loss": 1.9598,
      "step": 61900
    },
    {
      "epoch": 0.9117937094329069,
      "grad_norm": 1.8318618535995483,
      "learning_rate": 5.6905561651293356e-05,
      "loss": 1.9266,
      "step": 61950
    },
    {
      "epoch": 0.9125296204171144,
      "grad_norm": 1.9408516883850098,
      "learning_rate": 5.6848312633881763e-05,
      "loss": 1.9404,
      "step": 62000
    },
    {
      "epoch": 0.9132655314013217,
      "grad_norm": 1.9129489660263062,
      "learning_rate": 5.6791054465358505e-05,
      "loss": 1.9232,
      "step": 62050
    },
    {
      "epoch": 0.914001442385529,
      "grad_norm": 1.9112969636917114,
      "learning_rate": 5.673378722223528e-05,
      "loss": 1.8718,
      "step": 62100
    },
    {
      "epoch": 0.9147373533697364,
      "grad_norm": 2.0086746215820312,
      "learning_rate": 5.667651098103588e-05,
      "loss": 1.827,
      "step": 62150
    },
    {
      "epoch": 0.9154732643539437,
      "grad_norm": 2.1195766925811768,
      "learning_rate": 5.661922581829613e-05,
      "loss": 1.9454,
      "step": 62200
    },
    {
      "epoch": 0.916209175338151,
      "grad_norm": 2.5194766521453857,
      "learning_rate": 5.6561931810563784e-05,
      "loss": 1.9713,
      "step": 62250
    },
    {
      "epoch": 0.9169450863223585,
      "grad_norm": 2.108672618865967,
      "learning_rate": 5.6504629034398415e-05,
      "loss": 1.8992,
      "step": 62300
    },
    {
      "epoch": 0.9176809973065658,
      "grad_norm": 2.222867488861084,
      "learning_rate": 5.6447317566371295e-05,
      "loss": 1.9414,
      "step": 62350
    },
    {
      "epoch": 0.9184169082907732,
      "grad_norm": 2.1724905967712402,
      "learning_rate": 5.6389997483065335e-05,
      "loss": 1.8966,
      "step": 62400
    },
    {
      "epoch": 0.9191528192749805,
      "grad_norm": 2.0867512226104736,
      "learning_rate": 5.6332668861074954e-05,
      "loss": 1.9094,
      "step": 62450
    },
    {
      "epoch": 0.9198887302591878,
      "grad_norm": 2.520298957824707,
      "learning_rate": 5.627533177700596e-05,
      "loss": 1.8902,
      "step": 62500
    },
    {
      "epoch": 0.9206246412433952,
      "grad_norm": 2.3803677558898926,
      "learning_rate": 5.621798630747549e-05,
      "loss": 1.9293,
      "step": 62550
    },
    {
      "epoch": 0.9213605522276026,
      "grad_norm": 2.1407854557037354,
      "learning_rate": 5.616063252911187e-05,
      "loss": 1.9394,
      "step": 62600
    },
    {
      "epoch": 0.9220964632118099,
      "grad_norm": 2.0309321880340576,
      "learning_rate": 5.610327051855456e-05,
      "loss": 1.8666,
      "step": 62650
    },
    {
      "epoch": 0.9228323741960173,
      "grad_norm": 1.9631685018539429,
      "learning_rate": 5.6045900352453985e-05,
      "loss": 1.9623,
      "step": 62700
    },
    {
      "epoch": 0.9235682851802246,
      "grad_norm": 2.052330255508423,
      "learning_rate": 5.598852210747147e-05,
      "loss": 1.8941,
      "step": 62750
    },
    {
      "epoch": 0.9243041961644319,
      "grad_norm": 2.0341744422912598,
      "learning_rate": 5.5931135860279146e-05,
      "loss": 1.8376,
      "step": 62800
    },
    {
      "epoch": 0.9250401071486393,
      "grad_norm": 2.4407436847686768,
      "learning_rate": 5.5873741687559864e-05,
      "loss": 1.8957,
      "step": 62850
    },
    {
      "epoch": 0.9257760181328466,
      "grad_norm": 2.2358295917510986,
      "learning_rate": 5.581633966600702e-05,
      "loss": 1.8588,
      "step": 62900
    },
    {
      "epoch": 0.926511929117054,
      "grad_norm": 2.496023416519165,
      "learning_rate": 5.57589298723245e-05,
      "loss": 1.9238,
      "step": 62950
    },
    {
      "epoch": 0.9272478401012614,
      "grad_norm": 2.143184185028076,
      "learning_rate": 5.570151238322664e-05,
      "loss": 1.936,
      "step": 63000
    },
    {
      "epoch": 0.9279837510854687,
      "grad_norm": 2.003445625305176,
      "learning_rate": 5.564408727543797e-05,
      "loss": 1.8689,
      "step": 63050
    },
    {
      "epoch": 0.928719662069676,
      "grad_norm": 2.1944968700408936,
      "learning_rate": 5.558665462569326e-05,
      "loss": 1.9008,
      "step": 63100
    },
    {
      "epoch": 0.9294555730538834,
      "grad_norm": 2.0849432945251465,
      "learning_rate": 5.552921451073735e-05,
      "loss": 1.9436,
      "step": 63150
    },
    {
      "epoch": 0.9301914840380907,
      "grad_norm": 2.1035938262939453,
      "learning_rate": 5.5471767007325015e-05,
      "loss": 1.8897,
      "step": 63200
    },
    {
      "epoch": 0.9309273950222982,
      "grad_norm": 2.1815056800842285,
      "learning_rate": 5.541431219222094e-05,
      "loss": 1.9257,
      "step": 63250
    },
    {
      "epoch": 0.9316633060065055,
      "grad_norm": 2.146810531616211,
      "learning_rate": 5.53568501421996e-05,
      "loss": 1.8367,
      "step": 63300
    },
    {
      "epoch": 0.9323992169907128,
      "grad_norm": 2.57723069190979,
      "learning_rate": 5.52993809340451e-05,
      "loss": 1.9199,
      "step": 63350
    },
    {
      "epoch": 0.9331351279749202,
      "grad_norm": 1.9066988229751587,
      "learning_rate": 5.524190464455111e-05,
      "loss": 1.8997,
      "step": 63400
    },
    {
      "epoch": 0.9338710389591275,
      "grad_norm": 2.648531198501587,
      "learning_rate": 5.5184421350520784e-05,
      "loss": 1.9057,
      "step": 63450
    },
    {
      "epoch": 0.9346069499433348,
      "grad_norm": 1.9936367273330688,
      "learning_rate": 5.5126931128766634e-05,
      "loss": 1.8912,
      "step": 63500
    },
    {
      "epoch": 0.9353428609275422,
      "grad_norm": 1.697500467300415,
      "learning_rate": 5.50694340561104e-05,
      "loss": 1.9036,
      "step": 63550
    },
    {
      "epoch": 0.9360787719117496,
      "grad_norm": 2.2473104000091553,
      "learning_rate": 5.501193020938301e-05,
      "loss": 1.9274,
      "step": 63600
    },
    {
      "epoch": 0.9368146828959569,
      "grad_norm": 2.1237382888793945,
      "learning_rate": 5.495441966542444e-05,
      "loss": 1.9232,
      "step": 63650
    },
    {
      "epoch": 0.9375505938801643,
      "grad_norm": 1.9138387441635132,
      "learning_rate": 5.489690250108359e-05,
      "loss": 1.9283,
      "step": 63700
    },
    {
      "epoch": 0.9382865048643716,
      "grad_norm": 2.259721517562866,
      "learning_rate": 5.4839378793218236e-05,
      "loss": 1.9139,
      "step": 63750
    },
    {
      "epoch": 0.9390224158485789,
      "grad_norm": 2.0875320434570312,
      "learning_rate": 5.4781848618694877e-05,
      "loss": 1.9289,
      "step": 63800
    },
    {
      "epoch": 0.9397583268327863,
      "grad_norm": 2.2623870372772217,
      "learning_rate": 5.472431205438867e-05,
      "loss": 1.9212,
      "step": 63850
    },
    {
      "epoch": 0.9404942378169937,
      "grad_norm": 1.9670004844665527,
      "learning_rate": 5.4666769177183285e-05,
      "loss": 1.871,
      "step": 63900
    },
    {
      "epoch": 0.941230148801201,
      "grad_norm": 2.3266539573669434,
      "learning_rate": 5.460922006397087e-05,
      "loss": 1.8944,
      "step": 63950
    },
    {
      "epoch": 0.9419660597854084,
      "grad_norm": 2.064091444015503,
      "learning_rate": 5.455166479165187e-05,
      "loss": 1.8873,
      "step": 64000
    },
    {
      "epoch": 0.9427019707696157,
      "grad_norm": 2.0777862071990967,
      "learning_rate": 5.449410343713496e-05,
      "loss": 1.9352,
      "step": 64050
    },
    {
      "epoch": 0.943437881753823,
      "grad_norm": 2.514681577682495,
      "learning_rate": 5.4436536077336966e-05,
      "loss": 1.8995,
      "step": 64100
    },
    {
      "epoch": 0.9441737927380304,
      "grad_norm": 2.0844533443450928,
      "learning_rate": 5.4378962789182716e-05,
      "loss": 1.8482,
      "step": 64150
    },
    {
      "epoch": 0.9449097037222377,
      "grad_norm": 2.1980206966400146,
      "learning_rate": 5.432138364960498e-05,
      "loss": 1.9064,
      "step": 64200
    },
    {
      "epoch": 0.9456456147064451,
      "grad_norm": 2.165424108505249,
      "learning_rate": 5.4263798735544324e-05,
      "loss": 1.9053,
      "step": 64250
    },
    {
      "epoch": 0.9463815256906525,
      "grad_norm": 2.121450424194336,
      "learning_rate": 5.420620812394905e-05,
      "loss": 1.9174,
      "step": 64300
    },
    {
      "epoch": 0.9471174366748598,
      "grad_norm": 1.9312106370925903,
      "learning_rate": 5.414861189177507e-05,
      "loss": 1.8479,
      "step": 64350
    },
    {
      "epoch": 0.9478533476590671,
      "grad_norm": 2.2304372787475586,
      "learning_rate": 5.4091010115985795e-05,
      "loss": 1.8832,
      "step": 64400
    },
    {
      "epoch": 0.9485892586432745,
      "grad_norm": 2.156874895095825,
      "learning_rate": 5.4033402873552054e-05,
      "loss": 1.8666,
      "step": 64450
    },
    {
      "epoch": 0.9493251696274818,
      "grad_norm": 1.9225878715515137,
      "learning_rate": 5.3975790241452e-05,
      "loss": 1.9148,
      "step": 64500
    },
    {
      "epoch": 0.9500610806116893,
      "grad_norm": 2.2152962684631348,
      "learning_rate": 5.3918172296670896e-05,
      "loss": 1.8837,
      "step": 64550
    },
    {
      "epoch": 0.9507969915958966,
      "grad_norm": 2.222903251647949,
      "learning_rate": 5.3860549116201256e-05,
      "loss": 1.8454,
      "step": 64600
    },
    {
      "epoch": 0.9515329025801039,
      "grad_norm": 2.0267856121063232,
      "learning_rate": 5.3802920777042473e-05,
      "loss": 1.8783,
      "step": 64650
    },
    {
      "epoch": 0.9522688135643113,
      "grad_norm": 1.8508728742599487,
      "learning_rate": 5.374528735620087e-05,
      "loss": 1.8874,
      "step": 64700
    },
    {
      "epoch": 0.9530047245485186,
      "grad_norm": 2.0788118839263916,
      "learning_rate": 5.368764893068955e-05,
      "loss": 1.9257,
      "step": 64750
    },
    {
      "epoch": 0.9537406355327259,
      "grad_norm": 2.25044584274292,
      "learning_rate": 5.363000557752833e-05,
      "loss": 1.9235,
      "step": 64800
    },
    {
      "epoch": 0.9544765465169334,
      "grad_norm": 2.2290639877319336,
      "learning_rate": 5.357235737374359e-05,
      "loss": 1.8556,
      "step": 64850
    },
    {
      "epoch": 0.9552124575011407,
      "grad_norm": 1.9776251316070557,
      "learning_rate": 5.351470439636819e-05,
      "loss": 1.9082,
      "step": 64900
    },
    {
      "epoch": 0.955948368485348,
      "grad_norm": 2.173912286758423,
      "learning_rate": 5.345704672244137e-05,
      "loss": 1.9056,
      "step": 64950
    },
    {
      "epoch": 0.9566842794695554,
      "grad_norm": 2.3219525814056396,
      "learning_rate": 5.339938442900867e-05,
      "loss": 1.9198,
      "step": 65000
    },
    {
      "epoch": 0.9574201904537627,
      "grad_norm": 1.9807261228561401,
      "learning_rate": 5.3341717593121775e-05,
      "loss": 1.9919,
      "step": 65050
    },
    {
      "epoch": 0.95815610143797,
      "grad_norm": 1.8747106790542603,
      "learning_rate": 5.328404629183843e-05,
      "loss": 1.9204,
      "step": 65100
    },
    {
      "epoch": 0.9588920124221774,
      "grad_norm": 2.038902759552002,
      "learning_rate": 5.3226370602222394e-05,
      "loss": 1.8713,
      "step": 65150
    },
    {
      "epoch": 0.9596279234063848,
      "grad_norm": 2.245574951171875,
      "learning_rate": 5.316869060134323e-05,
      "loss": 1.8933,
      "step": 65200
    },
    {
      "epoch": 0.9603638343905921,
      "grad_norm": 1.9116032123565674,
      "learning_rate": 5.3111006366276315e-05,
      "loss": 1.8271,
      "step": 65250
    },
    {
      "epoch": 0.9610997453747995,
      "grad_norm": 2.038135290145874,
      "learning_rate": 5.305331797410267e-05,
      "loss": 1.9052,
      "step": 65300
    },
    {
      "epoch": 0.9618356563590068,
      "grad_norm": 2.2057650089263916,
      "learning_rate": 5.2995625501908864e-05,
      "loss": 1.9152,
      "step": 65350
    },
    {
      "epoch": 0.9625715673432141,
      "grad_norm": 2.2004189491271973,
      "learning_rate": 5.293792902678688e-05,
      "loss": 1.8435,
      "step": 65400
    },
    {
      "epoch": 0.9633074783274215,
      "grad_norm": 2.141788959503174,
      "learning_rate": 5.2880228625834114e-05,
      "loss": 1.8498,
      "step": 65450
    },
    {
      "epoch": 0.9640433893116289,
      "grad_norm": 1.8912653923034668,
      "learning_rate": 5.282252437615317e-05,
      "loss": 1.9541,
      "step": 65500
    },
    {
      "epoch": 0.9647793002958363,
      "grad_norm": 2.108304977416992,
      "learning_rate": 5.276481635485183e-05,
      "loss": 1.8528,
      "step": 65550
    },
    {
      "epoch": 0.9655152112800436,
      "grad_norm": 2.4887661933898926,
      "learning_rate": 5.270710463904287e-05,
      "loss": 1.9108,
      "step": 65600
    },
    {
      "epoch": 0.9662511222642509,
      "grad_norm": 1.878643274307251,
      "learning_rate": 5.2649389305844035e-05,
      "loss": 1.8989,
      "step": 65650
    },
    {
      "epoch": 0.9669870332484582,
      "grad_norm": 2.1276073455810547,
      "learning_rate": 5.259167043237787e-05,
      "loss": 1.9185,
      "step": 65700
    },
    {
      "epoch": 0.9677229442326656,
      "grad_norm": 2.33711838722229,
      "learning_rate": 5.253394809577171e-05,
      "loss": 1.8954,
      "step": 65750
    },
    {
      "epoch": 0.9684588552168729,
      "grad_norm": 1.7804486751556396,
      "learning_rate": 5.247622237315747e-05,
      "loss": 1.9467,
      "step": 65800
    },
    {
      "epoch": 0.9691947662010804,
      "grad_norm": 2.0397226810455322,
      "learning_rate": 5.241849334167157e-05,
      "loss": 1.8665,
      "step": 65850
    },
    {
      "epoch": 0.9699306771852877,
      "grad_norm": 2.1679961681365967,
      "learning_rate": 5.2360761078454924e-05,
      "loss": 1.9144,
      "step": 65900
    },
    {
      "epoch": 0.970666588169495,
      "grad_norm": 2.181936025619507,
      "learning_rate": 5.2303025660652704e-05,
      "loss": 1.8811,
      "step": 65950
    },
    {
      "epoch": 0.9714024991537024,
      "grad_norm": 2.7074596881866455,
      "learning_rate": 5.224528716541432e-05,
      "loss": 1.8585,
      "step": 66000
    },
    {
      "epoch": 0.9721384101379097,
      "grad_norm": 2.058621644973755,
      "learning_rate": 5.218754566989328e-05,
      "loss": 1.8957,
      "step": 66050
    },
    {
      "epoch": 0.972874321122117,
      "grad_norm": 2.418579578399658,
      "learning_rate": 5.2129801251247136e-05,
      "loss": 1.8784,
      "step": 66100
    },
    {
      "epoch": 0.9736102321063245,
      "grad_norm": 2.237544059753418,
      "learning_rate": 5.207205398663728e-05,
      "loss": 1.9014,
      "step": 66150
    },
    {
      "epoch": 0.9743461430905318,
      "grad_norm": 2.2437644004821777,
      "learning_rate": 5.2014303953229014e-05,
      "loss": 1.8778,
      "step": 66200
    },
    {
      "epoch": 0.9750820540747391,
      "grad_norm": 2.24876070022583,
      "learning_rate": 5.195655122819122e-05,
      "loss": 1.9081,
      "step": 66250
    },
    {
      "epoch": 0.9758179650589465,
      "grad_norm": 2.060641288757324,
      "learning_rate": 5.1898795888696436e-05,
      "loss": 1.8466,
      "step": 66300
    },
    {
      "epoch": 0.9765538760431538,
      "grad_norm": 1.8670721054077148,
      "learning_rate": 5.184103801192072e-05,
      "loss": 1.9017,
      "step": 66350
    },
    {
      "epoch": 0.9772897870273611,
      "grad_norm": 2.357126235961914,
      "learning_rate": 5.178327767504346e-05,
      "loss": 1.9333,
      "step": 66400
    },
    {
      "epoch": 0.9780256980115685,
      "grad_norm": 1.802575707435608,
      "learning_rate": 5.172551495524738e-05,
      "loss": 1.8808,
      "step": 66450
    },
    {
      "epoch": 0.9787616089957759,
      "grad_norm": 2.389381170272827,
      "learning_rate": 5.1667749929718346e-05,
      "loss": 1.9115,
      "step": 66500
    },
    {
      "epoch": 0.9794975199799832,
      "grad_norm": 2.183647632598877,
      "learning_rate": 5.1609982675645344e-05,
      "loss": 1.9368,
      "step": 66550
    },
    {
      "epoch": 0.9802334309641906,
      "grad_norm": 1.9808014631271362,
      "learning_rate": 5.155221327022033e-05,
      "loss": 1.9267,
      "step": 66600
    },
    {
      "epoch": 0.9809693419483979,
      "grad_norm": 2.154104709625244,
      "learning_rate": 5.1494441790638115e-05,
      "loss": 1.8695,
      "step": 66650
    },
    {
      "epoch": 0.9817052529326052,
      "grad_norm": 2.1590707302093506,
      "learning_rate": 5.1436668314096304e-05,
      "loss": 1.9741,
      "step": 66700
    },
    {
      "epoch": 0.9824411639168126,
      "grad_norm": 1.8831298351287842,
      "learning_rate": 5.137889291779514e-05,
      "loss": 1.8297,
      "step": 66750
    },
    {
      "epoch": 0.98317707490102,
      "grad_norm": 2.072707176208496,
      "learning_rate": 5.1321115678937456e-05,
      "loss": 1.9537,
      "step": 66800
    },
    {
      "epoch": 0.9839129858852274,
      "grad_norm": 1.9212974309921265,
      "learning_rate": 5.1263336674728556e-05,
      "loss": 1.9358,
      "step": 66850
    },
    {
      "epoch": 0.9846488968694347,
      "grad_norm": 2.254666566848755,
      "learning_rate": 5.1205555982376086e-05,
      "loss": 1.8826,
      "step": 66900
    },
    {
      "epoch": 0.985384807853642,
      "grad_norm": 1.7439477443695068,
      "learning_rate": 5.114777367908994e-05,
      "loss": 1.8688,
      "step": 66950
    },
    {
      "epoch": 0.9861207188378494,
      "grad_norm": 1.969642996788025,
      "learning_rate": 5.108998984208217e-05,
      "loss": 1.8928,
      "step": 67000
    },
    {
      "epoch": 0.9868566298220567,
      "grad_norm": 2.3703973293304443,
      "learning_rate": 5.103220454856692e-05,
      "loss": 1.8595,
      "step": 67050
    },
    {
      "epoch": 0.9875925408062641,
      "grad_norm": 1.902519702911377,
      "learning_rate": 5.09744178757602e-05,
      "loss": 1.8415,
      "step": 67100
    },
    {
      "epoch": 0.9883284517904715,
      "grad_norm": 2.3050615787506104,
      "learning_rate": 5.091662990087993e-05,
      "loss": 1.8829,
      "step": 67150
    },
    {
      "epoch": 0.9890643627746788,
      "grad_norm": 2.092099666595459,
      "learning_rate": 5.0858840701145736e-05,
      "loss": 1.9133,
      "step": 67200
    },
    {
      "epoch": 0.9898002737588861,
      "grad_norm": 2.0904250144958496,
      "learning_rate": 5.080105035377891e-05,
      "loss": 1.8662,
      "step": 67250
    },
    {
      "epoch": 0.9905361847430935,
      "grad_norm": 2.316138744354248,
      "learning_rate": 5.0743258936002236e-05,
      "loss": 1.8569,
      "step": 67300
    },
    {
      "epoch": 0.9912720957273008,
      "grad_norm": 1.8323367834091187,
      "learning_rate": 5.068546652503995e-05,
      "loss": 1.9378,
      "step": 67350
    },
    {
      "epoch": 0.9920080067115081,
      "grad_norm": 2.3184680938720703,
      "learning_rate": 5.0627673198117634e-05,
      "loss": 1.8708,
      "step": 67400
    },
    {
      "epoch": 0.9927439176957156,
      "grad_norm": 2.245211362838745,
      "learning_rate": 5.056987903246205e-05,
      "loss": 1.8673,
      "step": 67450
    },
    {
      "epoch": 0.9934798286799229,
      "grad_norm": 1.884015679359436,
      "learning_rate": 5.051208410530114e-05,
      "loss": 1.9016,
      "step": 67500
    },
    {
      "epoch": 0.9942157396641302,
      "grad_norm": 1.860787272453308,
      "learning_rate": 5.045428849386382e-05,
      "loss": 1.8739,
      "step": 67550
    },
    {
      "epoch": 0.9949516506483376,
      "grad_norm": 2.46533203125,
      "learning_rate": 5.039649227537993e-05,
      "loss": 1.8782,
      "step": 67600
    },
    {
      "epoch": 0.9956875616325449,
      "grad_norm": 2.3079721927642822,
      "learning_rate": 5.0338695527080095e-05,
      "loss": 1.8213,
      "step": 67650
    },
    {
      "epoch": 0.9964234726167522,
      "grad_norm": 2.0850512981414795,
      "learning_rate": 5.0280898326195704e-05,
      "loss": 1.8727,
      "step": 67700
    },
    {
      "epoch": 0.9971593836009597,
      "grad_norm": 2.056405544281006,
      "learning_rate": 5.0223100749958706e-05,
      "loss": 1.8519,
      "step": 67750
    },
    {
      "epoch": 0.997895294585167,
      "grad_norm": 2.153508424758911,
      "learning_rate": 5.0165302875601584e-05,
      "loss": 1.8762,
      "step": 67800
    },
    {
      "epoch": 0.9986312055693743,
      "grad_norm": 2.194922685623169,
      "learning_rate": 5.0107504780357196e-05,
      "loss": 1.8513,
      "step": 67850
    },
    {
      "epoch": 0.9993671165535817,
      "grad_norm": 2.1867387294769287,
      "learning_rate": 5.0049706541458696e-05,
      "loss": 1.8406,
      "step": 67900
    },
    {
      "epoch": 1.0001030275377891,
      "grad_norm": 2.270712375640869,
      "learning_rate": 4.9991908236139437e-05,
      "loss": 1.8757,
      "step": 67950
    },
    {
      "epoch": 1.0008389385219965,
      "grad_norm": 2.1132283210754395,
      "learning_rate": 4.9934109941632865e-05,
      "loss": 1.8376,
      "step": 68000
    },
    {
      "epoch": 1.0015748495062038,
      "grad_norm": 2.1835544109344482,
      "learning_rate": 4.98763117351724e-05,
      "loss": 1.8358,
      "step": 68050
    },
    {
      "epoch": 1.0023107604904111,
      "grad_norm": 2.0643084049224854,
      "learning_rate": 4.981851369399135e-05,
      "loss": 1.8424,
      "step": 68100
    },
    {
      "epoch": 1.0030466714746185,
      "grad_norm": 2.1185314655303955,
      "learning_rate": 4.9760715895322804e-05,
      "loss": 1.8902,
      "step": 68150
    },
    {
      "epoch": 1.0037825824588258,
      "grad_norm": 1.7932876348495483,
      "learning_rate": 4.970291841639954e-05,
      "loss": 1.8935,
      "step": 68200
    },
    {
      "epoch": 1.0045184934430331,
      "grad_norm": 1.9301456212997437,
      "learning_rate": 4.964512133445387e-05,
      "loss": 1.8332,
      "step": 68250
    },
    {
      "epoch": 1.0052544044272405,
      "grad_norm": 2.1667661666870117,
      "learning_rate": 4.958732472671761e-05,
      "loss": 1.841,
      "step": 68300
    },
    {
      "epoch": 1.0059903154114478,
      "grad_norm": 1.993957757949829,
      "learning_rate": 4.9529528670421926e-05,
      "loss": 1.8622,
      "step": 68350
    },
    {
      "epoch": 1.0067262263956551,
      "grad_norm": 1.8555831909179688,
      "learning_rate": 4.9471733242797264e-05,
      "loss": 1.9018,
      "step": 68400
    },
    {
      "epoch": 1.0074621373798625,
      "grad_norm": 2.378387689590454,
      "learning_rate": 4.941393852107322e-05,
      "loss": 1.8683,
      "step": 68450
    },
    {
      "epoch": 1.0081980483640698,
      "grad_norm": 2.0032413005828857,
      "learning_rate": 4.935614458247844e-05,
      "loss": 1.9239,
      "step": 68500
    },
    {
      "epoch": 1.0089339593482771,
      "grad_norm": 2.110853672027588,
      "learning_rate": 4.9298351504240504e-05,
      "loss": 1.9013,
      "step": 68550
    },
    {
      "epoch": 1.0096698703324847,
      "grad_norm": 1.9809749126434326,
      "learning_rate": 4.924055936358591e-05,
      "loss": 1.8573,
      "step": 68600
    },
    {
      "epoch": 1.010405781316692,
      "grad_norm": 2.1928372383117676,
      "learning_rate": 4.9182768237739846e-05,
      "loss": 1.8304,
      "step": 68650
    },
    {
      "epoch": 1.0111416923008993,
      "grad_norm": 2.1680097579956055,
      "learning_rate": 4.912497820392615e-05,
      "loss": 1.8503,
      "step": 68700
    },
    {
      "epoch": 1.0118776032851067,
      "grad_norm": 1.65667724609375,
      "learning_rate": 4.9067189339367227e-05,
      "loss": 1.9304,
      "step": 68750
    },
    {
      "epoch": 1.012613514269314,
      "grad_norm": 1.9852931499481201,
      "learning_rate": 4.900940172128389e-05,
      "loss": 1.8391,
      "step": 68800
    },
    {
      "epoch": 1.0133494252535213,
      "grad_norm": 1.8911526203155518,
      "learning_rate": 4.89516154268953e-05,
      "loss": 1.8867,
      "step": 68850
    },
    {
      "epoch": 1.0140853362377287,
      "grad_norm": 2.257051706314087,
      "learning_rate": 4.889383053341885e-05,
      "loss": 1.9063,
      "step": 68900
    },
    {
      "epoch": 1.014821247221936,
      "grad_norm": 1.9548885822296143,
      "learning_rate": 4.883604711807004e-05,
      "loss": 1.8669,
      "step": 68950
    },
    {
      "epoch": 1.0155571582061433,
      "grad_norm": 2.488088607788086,
      "learning_rate": 4.877826525806244e-05,
      "loss": 1.8982,
      "step": 69000
    },
    {
      "epoch": 1.0162930691903507,
      "grad_norm": 2.1657629013061523,
      "learning_rate": 4.87204850306075e-05,
      "loss": 1.8429,
      "step": 69050
    },
    {
      "epoch": 1.017028980174558,
      "grad_norm": 1.8581633567810059,
      "learning_rate": 4.866270651291452e-05,
      "loss": 1.813,
      "step": 69100
    },
    {
      "epoch": 1.0177648911587653,
      "grad_norm": 2.2253167629241943,
      "learning_rate": 4.860492978219048e-05,
      "loss": 1.8377,
      "step": 69150
    },
    {
      "epoch": 1.018500802142973,
      "grad_norm": 2.4672963619232178,
      "learning_rate": 4.854715491563997e-05,
      "loss": 1.839,
      "step": 69200
    },
    {
      "epoch": 1.0192367131271802,
      "grad_norm": 2.5230672359466553,
      "learning_rate": 4.8489381990465175e-05,
      "loss": 1.858,
      "step": 69250
    },
    {
      "epoch": 1.0199726241113876,
      "grad_norm": 2.233591318130493,
      "learning_rate": 4.8431611083865575e-05,
      "loss": 1.8733,
      "step": 69300
    },
    {
      "epoch": 1.020708535095595,
      "grad_norm": 2.0117902755737305,
      "learning_rate": 4.837384227303801e-05,
      "loss": 1.8396,
      "step": 69350
    },
    {
      "epoch": 1.0214444460798022,
      "grad_norm": 1.9323090314865112,
      "learning_rate": 4.8316075635176503e-05,
      "loss": 1.8521,
      "step": 69400
    },
    {
      "epoch": 1.0221803570640096,
      "grad_norm": 2.2376649379730225,
      "learning_rate": 4.825831124747219e-05,
      "loss": 1.889,
      "step": 69450
    },
    {
      "epoch": 1.022916268048217,
      "grad_norm": 2.069542407989502,
      "learning_rate": 4.820054918711317e-05,
      "loss": 1.8718,
      "step": 69500
    },
    {
      "epoch": 1.0236521790324242,
      "grad_norm": 1.8858448266983032,
      "learning_rate": 4.814278953128447e-05,
      "loss": 1.861,
      "step": 69550
    },
    {
      "epoch": 1.0243880900166316,
      "grad_norm": 2.0567426681518555,
      "learning_rate": 4.808503235716787e-05,
      "loss": 1.8654,
      "step": 69600
    },
    {
      "epoch": 1.025124001000839,
      "grad_norm": 2.23952579498291,
      "learning_rate": 4.802727774194185e-05,
      "loss": 1.8146,
      "step": 69650
    },
    {
      "epoch": 1.0258599119850462,
      "grad_norm": 2.24570369720459,
      "learning_rate": 4.7969525762781484e-05,
      "loss": 1.8672,
      "step": 69700
    },
    {
      "epoch": 1.0265958229692536,
      "grad_norm": 2.3594913482666016,
      "learning_rate": 4.791177649685829e-05,
      "loss": 1.8309,
      "step": 69750
    },
    {
      "epoch": 1.027331733953461,
      "grad_norm": 2.1819541454315186,
      "learning_rate": 4.785403002134019e-05,
      "loss": 1.8213,
      "step": 69800
    },
    {
      "epoch": 1.0280676449376684,
      "grad_norm": 2.056593179702759,
      "learning_rate": 4.779628641339136e-05,
      "loss": 1.8243,
      "step": 69850
    },
    {
      "epoch": 1.0288035559218758,
      "grad_norm": 1.6974815130233765,
      "learning_rate": 4.773854575017216e-05,
      "loss": 1.8505,
      "step": 69900
    },
    {
      "epoch": 1.0295394669060831,
      "grad_norm": 2.0436387062072754,
      "learning_rate": 4.7680808108839e-05,
      "loss": 1.8468,
      "step": 69950
    },
    {
      "epoch": 1.0302753778902904,
      "grad_norm": 1.938065767288208,
      "learning_rate": 4.7623073566544256e-05,
      "loss": 1.8366,
      "step": 70000
    },
    {
      "epoch": 1.0310112888744978,
      "grad_norm": 2.041921854019165,
      "learning_rate": 4.756534220043617e-05,
      "loss": 1.8259,
      "step": 70050
    },
    {
      "epoch": 1.031747199858705,
      "grad_norm": 1.955696702003479,
      "learning_rate": 4.7507614087658725e-05,
      "loss": 1.8417,
      "step": 70100
    },
    {
      "epoch": 1.0324831108429124,
      "grad_norm": 2.0709948539733887,
      "learning_rate": 4.7449889305351574e-05,
      "loss": 1.7826,
      "step": 70150
    },
    {
      "epoch": 1.0332190218271198,
      "grad_norm": 2.548227310180664,
      "learning_rate": 4.7392167930649925e-05,
      "loss": 1.939,
      "step": 70200
    },
    {
      "epoch": 1.033954932811327,
      "grad_norm": 2.461461305618286,
      "learning_rate": 4.733445004068439e-05,
      "loss": 1.8692,
      "step": 70250
    },
    {
      "epoch": 1.0346908437955344,
      "grad_norm": 1.9193300008773804,
      "learning_rate": 4.7276735712580965e-05,
      "loss": 1.9035,
      "step": 70300
    },
    {
      "epoch": 1.0354267547797418,
      "grad_norm": 1.8731763362884521,
      "learning_rate": 4.7219025023460886e-05,
      "loss": 1.8949,
      "step": 70350
    },
    {
      "epoch": 1.036162665763949,
      "grad_norm": 2.7826356887817383,
      "learning_rate": 4.7161318050440495e-05,
      "loss": 1.8883,
      "step": 70400
    },
    {
      "epoch": 1.0368985767481564,
      "grad_norm": 2.1298418045043945,
      "learning_rate": 4.710361487063121e-05,
      "loss": 1.8776,
      "step": 70450
    },
    {
      "epoch": 1.037634487732364,
      "grad_norm": 2.332761287689209,
      "learning_rate": 4.704591556113933e-05,
      "loss": 1.8593,
      "step": 70500
    },
    {
      "epoch": 1.0383703987165713,
      "grad_norm": 2.2633683681488037,
      "learning_rate": 4.6988220199066044e-05,
      "loss": 1.8749,
      "step": 70550
    },
    {
      "epoch": 1.0391063097007787,
      "grad_norm": 2.1181468963623047,
      "learning_rate": 4.693052886150723e-05,
      "loss": 1.8502,
      "step": 70600
    },
    {
      "epoch": 1.039842220684986,
      "grad_norm": 2.1633458137512207,
      "learning_rate": 4.687284162555339e-05,
      "loss": 1.8812,
      "step": 70650
    },
    {
      "epoch": 1.0405781316691933,
      "grad_norm": 1.952379584312439,
      "learning_rate": 4.6815158568289526e-05,
      "loss": 1.8355,
      "step": 70700
    },
    {
      "epoch": 1.0413140426534007,
      "grad_norm": 2.077924966812134,
      "learning_rate": 4.675747976679509e-05,
      "loss": 1.89,
      "step": 70750
    },
    {
      "epoch": 1.042049953637608,
      "grad_norm": 2.30289888381958,
      "learning_rate": 4.6699805298143844e-05,
      "loss": 1.8395,
      "step": 70800
    },
    {
      "epoch": 1.0427858646218153,
      "grad_norm": 1.5940357446670532,
      "learning_rate": 4.664213523940374e-05,
      "loss": 1.8183,
      "step": 70850
    },
    {
      "epoch": 1.0435217756060227,
      "grad_norm": 2.0366039276123047,
      "learning_rate": 4.658446966763685e-05,
      "loss": 1.8465,
      "step": 70900
    },
    {
      "epoch": 1.04425768659023,
      "grad_norm": 2.0610315799713135,
      "learning_rate": 4.6526808659899244e-05,
      "loss": 1.8958,
      "step": 70950
    },
    {
      "epoch": 1.0449935975744373,
      "grad_norm": 2.438149929046631,
      "learning_rate": 4.646915229324091e-05,
      "loss": 1.869,
      "step": 71000
    },
    {
      "epoch": 1.0457295085586447,
      "grad_norm": 2.0634031295776367,
      "learning_rate": 4.641150064470561e-05,
      "loss": 1.7962,
      "step": 71050
    },
    {
      "epoch": 1.046465419542852,
      "grad_norm": 2.3585219383239746,
      "learning_rate": 4.6353853791330815e-05,
      "loss": 1.8483,
      "step": 71100
    },
    {
      "epoch": 1.0472013305270595,
      "grad_norm": 2.019597291946411,
      "learning_rate": 4.6296211810147585e-05,
      "loss": 1.8944,
      "step": 71150
    },
    {
      "epoch": 1.0479372415112669,
      "grad_norm": 2.165266275405884,
      "learning_rate": 4.6238574778180475e-05,
      "loss": 1.8706,
      "step": 71200
    },
    {
      "epoch": 1.0486731524954742,
      "grad_norm": 2.20993709564209,
      "learning_rate": 4.6180942772447426e-05,
      "loss": 1.8839,
      "step": 71250
    },
    {
      "epoch": 1.0494090634796815,
      "grad_norm": 2.246731758117676,
      "learning_rate": 4.612331586995966e-05,
      "loss": 1.804,
      "step": 71300
    },
    {
      "epoch": 1.0501449744638889,
      "grad_norm": 2.2494301795959473,
      "learning_rate": 4.606569414772157e-05,
      "loss": 1.84,
      "step": 71350
    },
    {
      "epoch": 1.0508808854480962,
      "grad_norm": 1.7076655626296997,
      "learning_rate": 4.600807768273064e-05,
      "loss": 1.8536,
      "step": 71400
    },
    {
      "epoch": 1.0516167964323035,
      "grad_norm": 2.0568907260894775,
      "learning_rate": 4.595046655197733e-05,
      "loss": 1.8057,
      "step": 71450
    },
    {
      "epoch": 1.0523527074165109,
      "grad_norm": 2.287587881088257,
      "learning_rate": 4.589286083244496e-05,
      "loss": 1.7915,
      "step": 71500
    },
    {
      "epoch": 1.0530886184007182,
      "grad_norm": 2.1199331283569336,
      "learning_rate": 4.583526060110963e-05,
      "loss": 1.8675,
      "step": 71550
    },
    {
      "epoch": 1.0538245293849255,
      "grad_norm": 2.251779556274414,
      "learning_rate": 4.57776659349401e-05,
      "loss": 1.8578,
      "step": 71600
    },
    {
      "epoch": 1.0545604403691329,
      "grad_norm": 1.8010563850402832,
      "learning_rate": 4.572007691089769e-05,
      "loss": 1.8324,
      "step": 71650
    },
    {
      "epoch": 1.0552963513533402,
      "grad_norm": 2.3850555419921875,
      "learning_rate": 4.56624936059362e-05,
      "loss": 1.8622,
      "step": 71700
    },
    {
      "epoch": 1.0560322623375475,
      "grad_norm": 2.3268778324127197,
      "learning_rate": 4.560491609700176e-05,
      "loss": 1.8211,
      "step": 71750
    },
    {
      "epoch": 1.056768173321755,
      "grad_norm": 2.3797030448913574,
      "learning_rate": 4.554734446103276e-05,
      "loss": 1.8718,
      "step": 71800
    },
    {
      "epoch": 1.0575040843059624,
      "grad_norm": 3.034787893295288,
      "learning_rate": 4.548977877495979e-05,
      "loss": 1.8805,
      "step": 71850
    },
    {
      "epoch": 1.0582399952901698,
      "grad_norm": 2.1413025856018066,
      "learning_rate": 4.543221911570543e-05,
      "loss": 1.8854,
      "step": 71900
    },
    {
      "epoch": 1.058975906274377,
      "grad_norm": 2.3202383518218994,
      "learning_rate": 4.537466556018423e-05,
      "loss": 1.8282,
      "step": 71950
    },
    {
      "epoch": 1.0597118172585844,
      "grad_norm": 1.9207435846328735,
      "learning_rate": 4.5317118185302575e-05,
      "loss": 1.9184,
      "step": 72000
    },
    {
      "epoch": 1.0604477282427918,
      "grad_norm": 1.8870614767074585,
      "learning_rate": 4.5259577067958605e-05,
      "loss": 1.8681,
      "step": 72050
    },
    {
      "epoch": 1.061183639226999,
      "grad_norm": 2.3887779712677,
      "learning_rate": 4.520204228504208e-05,
      "loss": 1.8148,
      "step": 72100
    },
    {
      "epoch": 1.0619195502112064,
      "grad_norm": 2.132418632507324,
      "learning_rate": 4.514451391343432e-05,
      "loss": 1.8208,
      "step": 72150
    },
    {
      "epoch": 1.0626554611954138,
      "grad_norm": 1.997495174407959,
      "learning_rate": 4.5086992030008055e-05,
      "loss": 1.9002,
      "step": 72200
    },
    {
      "epoch": 1.063391372179621,
      "grad_norm": 2.201450824737549,
      "learning_rate": 4.502947671162736e-05,
      "loss": 1.8637,
      "step": 72250
    },
    {
      "epoch": 1.0641272831638284,
      "grad_norm": 2.2772696018218994,
      "learning_rate": 4.497196803514753e-05,
      "loss": 1.8453,
      "step": 72300
    },
    {
      "epoch": 1.0648631941480358,
      "grad_norm": 1.8637219667434692,
      "learning_rate": 4.491446607741499e-05,
      "loss": 1.8244,
      "step": 72350
    },
    {
      "epoch": 1.065599105132243,
      "grad_norm": 2.083570718765259,
      "learning_rate": 4.485697091526717e-05,
      "loss": 1.8626,
      "step": 72400
    },
    {
      "epoch": 1.0663350161164507,
      "grad_norm": 2.2511720657348633,
      "learning_rate": 4.4799482625532443e-05,
      "loss": 1.8591,
      "step": 72450
    },
    {
      "epoch": 1.067070927100658,
      "grad_norm": 2.128000497817993,
      "learning_rate": 4.474200128502999e-05,
      "loss": 1.8703,
      "step": 72500
    },
    {
      "epoch": 1.0678068380848653,
      "grad_norm": 2.1718177795410156,
      "learning_rate": 4.468452697056971e-05,
      "loss": 1.8339,
      "step": 72550
    },
    {
      "epoch": 1.0685427490690727,
      "grad_norm": 2.014866590499878,
      "learning_rate": 4.462705975895209e-05,
      "loss": 1.8714,
      "step": 72600
    },
    {
      "epoch": 1.06927866005328,
      "grad_norm": 2.382260799407959,
      "learning_rate": 4.4569599726968166e-05,
      "loss": 1.8184,
      "step": 72650
    },
    {
      "epoch": 1.0700145710374873,
      "grad_norm": 2.543163299560547,
      "learning_rate": 4.451214695139934e-05,
      "loss": 1.8242,
      "step": 72700
    },
    {
      "epoch": 1.0707504820216946,
      "grad_norm": 2.4788777828216553,
      "learning_rate": 4.445470150901735e-05,
      "loss": 1.8654,
      "step": 72750
    },
    {
      "epoch": 1.071486393005902,
      "grad_norm": 2.027604579925537,
      "learning_rate": 4.4397263476584106e-05,
      "loss": 1.8227,
      "step": 72800
    },
    {
      "epoch": 1.0722223039901093,
      "grad_norm": 2.6409027576446533,
      "learning_rate": 4.433983293085164e-05,
      "loss": 1.8995,
      "step": 72850
    },
    {
      "epoch": 1.0729582149743166,
      "grad_norm": 2.2069852352142334,
      "learning_rate": 4.428240994856198e-05,
      "loss": 1.8566,
      "step": 72900
    },
    {
      "epoch": 1.073694125958524,
      "grad_norm": 2.2514708042144775,
      "learning_rate": 4.4224994606447016e-05,
      "loss": 1.8341,
      "step": 72950
    },
    {
      "epoch": 1.0744300369427313,
      "grad_norm": 2.015127182006836,
      "learning_rate": 4.416758698122847e-05,
      "loss": 1.8915,
      "step": 73000
    },
    {
      "epoch": 1.0751659479269389,
      "grad_norm": 2.0719704627990723,
      "learning_rate": 4.411018714961771e-05,
      "loss": 1.8923,
      "step": 73050
    },
    {
      "epoch": 1.0759018589111462,
      "grad_norm": 2.391582489013672,
      "learning_rate": 4.405279518831572e-05,
      "loss": 1.8685,
      "step": 73100
    },
    {
      "epoch": 1.0766377698953535,
      "grad_norm": 2.3319804668426514,
      "learning_rate": 4.399541117401297e-05,
      "loss": 1.8073,
      "step": 73150
    },
    {
      "epoch": 1.0773736808795609,
      "grad_norm": 2.248012065887451,
      "learning_rate": 4.3938035183389315e-05,
      "loss": 1.8633,
      "step": 73200
    },
    {
      "epoch": 1.0781095918637682,
      "grad_norm": 2.080475330352783,
      "learning_rate": 4.388066729311386e-05,
      "loss": 1.8792,
      "step": 73250
    },
    {
      "epoch": 1.0788455028479755,
      "grad_norm": 2.1328630447387695,
      "learning_rate": 4.3823307579844885e-05,
      "loss": 1.856,
      "step": 73300
    },
    {
      "epoch": 1.0795814138321829,
      "grad_norm": 2.42179536819458,
      "learning_rate": 4.3765956120229775e-05,
      "loss": 1.8107,
      "step": 73350
    },
    {
      "epoch": 1.0803173248163902,
      "grad_norm": 1.9539235830307007,
      "learning_rate": 4.3708612990904864e-05,
      "loss": 1.8699,
      "step": 73400
    },
    {
      "epoch": 1.0810532358005975,
      "grad_norm": 2.4208786487579346,
      "learning_rate": 4.3651278268495356e-05,
      "loss": 1.8379,
      "step": 73450
    },
    {
      "epoch": 1.0817891467848049,
      "grad_norm": 2.2404236793518066,
      "learning_rate": 4.359395202961523e-05,
      "loss": 1.8179,
      "step": 73500
    },
    {
      "epoch": 1.0825250577690122,
      "grad_norm": 2.350052833557129,
      "learning_rate": 4.353663435086713e-05,
      "loss": 1.8735,
      "step": 73550
    },
    {
      "epoch": 1.0832609687532195,
      "grad_norm": 2.0867996215820312,
      "learning_rate": 4.3479325308842243e-05,
      "loss": 1.8628,
      "step": 73600
    },
    {
      "epoch": 1.0839968797374269,
      "grad_norm": 2.069335699081421,
      "learning_rate": 4.342202498012023e-05,
      "loss": 1.8251,
      "step": 73650
    },
    {
      "epoch": 1.0847327907216342,
      "grad_norm": 2.0780651569366455,
      "learning_rate": 4.3364733441269104e-05,
      "loss": 1.8536,
      "step": 73700
    },
    {
      "epoch": 1.0854687017058418,
      "grad_norm": 2.177673578262329,
      "learning_rate": 4.3307450768845153e-05,
      "loss": 1.8623,
      "step": 73750
    },
    {
      "epoch": 1.086204612690049,
      "grad_norm": 2.1459944248199463,
      "learning_rate": 4.3250177039392784e-05,
      "loss": 1.8489,
      "step": 73800
    },
    {
      "epoch": 1.0869405236742564,
      "grad_norm": 1.7501325607299805,
      "learning_rate": 4.319291232944447e-05,
      "loss": 1.8003,
      "step": 73850
    },
    {
      "epoch": 1.0876764346584638,
      "grad_norm": 2.1085422039031982,
      "learning_rate": 4.313565671552063e-05,
      "loss": 1.858,
      "step": 73900
    },
    {
      "epoch": 1.088412345642671,
      "grad_norm": 2.098686695098877,
      "learning_rate": 4.307841027412953e-05,
      "loss": 1.766,
      "step": 73950
    },
    {
      "epoch": 1.0891482566268784,
      "grad_norm": 1.9299988746643066,
      "learning_rate": 4.302117308176718e-05,
      "loss": 1.7881,
      "step": 74000
    },
    {
      "epoch": 1.0898841676110858,
      "grad_norm": 1.964589238166809,
      "learning_rate": 4.296394521491722e-05,
      "loss": 1.8344,
      "step": 74050
    },
    {
      "epoch": 1.090620078595293,
      "grad_norm": 2.1742427349090576,
      "learning_rate": 4.290672675005085e-05,
      "loss": 1.8042,
      "step": 74100
    },
    {
      "epoch": 1.0913559895795004,
      "grad_norm": 2.195193290710449,
      "learning_rate": 4.284951776362669e-05,
      "loss": 1.8338,
      "step": 74150
    },
    {
      "epoch": 1.0920919005637078,
      "grad_norm": 2.2271103858947754,
      "learning_rate": 4.2792318332090695e-05,
      "loss": 1.8741,
      "step": 74200
    },
    {
      "epoch": 1.092827811547915,
      "grad_norm": 1.7527282238006592,
      "learning_rate": 4.2735128531876064e-05,
      "loss": 1.8661,
      "step": 74250
    },
    {
      "epoch": 1.0935637225321224,
      "grad_norm": 2.3260045051574707,
      "learning_rate": 4.26779484394031e-05,
      "loss": 1.8665,
      "step": 74300
    },
    {
      "epoch": 1.09429963351633,
      "grad_norm": 2.3790643215179443,
      "learning_rate": 4.2620778131079145e-05,
      "loss": 1.8666,
      "step": 74350
    },
    {
      "epoch": 1.0950355445005373,
      "grad_norm": 1.9429339170455933,
      "learning_rate": 4.2563617683298504e-05,
      "loss": 1.799,
      "step": 74400
    },
    {
      "epoch": 1.0957714554847446,
      "grad_norm": 1.9326759576797485,
      "learning_rate": 4.2506467172442274e-05,
      "loss": 1.7748,
      "step": 74450
    },
    {
      "epoch": 1.096507366468952,
      "grad_norm": 2.44889497756958,
      "learning_rate": 4.2449326674878256e-05,
      "loss": 1.8684,
      "step": 74500
    },
    {
      "epoch": 1.0972432774531593,
      "grad_norm": 2.44413161277771,
      "learning_rate": 4.239219626696089e-05,
      "loss": 1.7877,
      "step": 74550
    },
    {
      "epoch": 1.0979791884373666,
      "grad_norm": 2.1392688751220703,
      "learning_rate": 4.233507602503116e-05,
      "loss": 1.8133,
      "step": 74600
    },
    {
      "epoch": 1.098715099421574,
      "grad_norm": 2.463465929031372,
      "learning_rate": 4.227796602541641e-05,
      "loss": 1.78,
      "step": 74650
    },
    {
      "epoch": 1.0994510104057813,
      "grad_norm": 2.4887781143188477,
      "learning_rate": 4.222086634443033e-05,
      "loss": 1.8576,
      "step": 74700
    },
    {
      "epoch": 1.1001869213899886,
      "grad_norm": 2.102100133895874,
      "learning_rate": 4.216377705837282e-05,
      "loss": 1.8099,
      "step": 74750
    },
    {
      "epoch": 1.100922832374196,
      "grad_norm": 1.7271937131881714,
      "learning_rate": 4.2106698243529896e-05,
      "loss": 1.9035,
      "step": 74800
    },
    {
      "epoch": 1.1016587433584033,
      "grad_norm": 2.3791491985321045,
      "learning_rate": 4.2049629976173566e-05,
      "loss": 1.8831,
      "step": 74850
    },
    {
      "epoch": 1.1023946543426106,
      "grad_norm": 1.8251837491989136,
      "learning_rate": 4.1992572332561756e-05,
      "loss": 1.744,
      "step": 74900
    },
    {
      "epoch": 1.103130565326818,
      "grad_norm": 2.212836980819702,
      "learning_rate": 4.193552538893819e-05,
      "loss": 1.8518,
      "step": 74950
    },
    {
      "epoch": 1.1038664763110253,
      "grad_norm": 2.1047122478485107,
      "learning_rate": 4.187848922153228e-05,
      "loss": 1.8011,
      "step": 75000
    },
    {
      "epoch": 1.1046023872952329,
      "grad_norm": 1.9744749069213867,
      "learning_rate": 4.182146390655908e-05,
      "loss": 1.8403,
      "step": 75050
    },
    {
      "epoch": 1.1053382982794402,
      "grad_norm": 2.4054131507873535,
      "learning_rate": 4.1764449520219105e-05,
      "loss": 1.8522,
      "step": 75100
    },
    {
      "epoch": 1.1060742092636475,
      "grad_norm": 2.499695301055908,
      "learning_rate": 4.170744613869828e-05,
      "loss": 1.8332,
      "step": 75150
    },
    {
      "epoch": 1.1068101202478549,
      "grad_norm": 2.106412172317505,
      "learning_rate": 4.1650453838167814e-05,
      "loss": 1.9036,
      "step": 75200
    },
    {
      "epoch": 1.1075460312320622,
      "grad_norm": 1.8925013542175293,
      "learning_rate": 4.1593472694784126e-05,
      "loss": 1.8609,
      "step": 75250
    },
    {
      "epoch": 1.1082819422162695,
      "grad_norm": 2.217834711074829,
      "learning_rate": 4.153650278468871e-05,
      "loss": 1.7983,
      "step": 75300
    },
    {
      "epoch": 1.1090178532004769,
      "grad_norm": 2.3104708194732666,
      "learning_rate": 4.147954418400806e-05,
      "loss": 1.8625,
      "step": 75350
    },
    {
      "epoch": 1.1097537641846842,
      "grad_norm": 2.150838851928711,
      "learning_rate": 4.142259696885355e-05,
      "loss": 1.8485,
      "step": 75400
    },
    {
      "epoch": 1.1104896751688915,
      "grad_norm": 2.393101692199707,
      "learning_rate": 4.136566121532136e-05,
      "loss": 1.8062,
      "step": 75450
    },
    {
      "epoch": 1.1112255861530989,
      "grad_norm": 2.2978522777557373,
      "learning_rate": 4.130873699949232e-05,
      "loss": 1.8525,
      "step": 75500
    },
    {
      "epoch": 1.1119614971373062,
      "grad_norm": 2.443338394165039,
      "learning_rate": 4.125182439743187e-05,
      "loss": 1.8185,
      "step": 75550
    },
    {
      "epoch": 1.1126974081215135,
      "grad_norm": 2.341628074645996,
      "learning_rate": 4.119492348518994e-05,
      "loss": 1.8504,
      "step": 75600
    },
    {
      "epoch": 1.113433319105721,
      "grad_norm": 2.0637428760528564,
      "learning_rate": 4.113803433880076e-05,
      "loss": 1.8224,
      "step": 75650
    },
    {
      "epoch": 1.1141692300899284,
      "grad_norm": 2.4581077098846436,
      "learning_rate": 4.108115703428297e-05,
      "loss": 1.8859,
      "step": 75700
    },
    {
      "epoch": 1.1149051410741357,
      "grad_norm": 2.172281265258789,
      "learning_rate": 4.102429164763929e-05,
      "loss": 1.8404,
      "step": 75750
    },
    {
      "epoch": 1.115641052058343,
      "grad_norm": 2.232426404953003,
      "learning_rate": 4.096743825485654e-05,
      "loss": 1.8565,
      "step": 75800
    },
    {
      "epoch": 1.1163769630425504,
      "grad_norm": 2.1647849082946777,
      "learning_rate": 4.091059693190551e-05,
      "loss": 1.8411,
      "step": 75850
    },
    {
      "epoch": 1.1171128740267577,
      "grad_norm": 2.311405897140503,
      "learning_rate": 4.085376775474087e-05,
      "loss": 1.8252,
      "step": 75900
    },
    {
      "epoch": 1.117848785010965,
      "grad_norm": 2.3242239952087402,
      "learning_rate": 4.079695079930107e-05,
      "loss": 1.9253,
      "step": 75950
    },
    {
      "epoch": 1.1185846959951724,
      "grad_norm": 1.9467071294784546,
      "learning_rate": 4.0740146141508176e-05,
      "loss": 1.8181,
      "step": 76000
    },
    {
      "epoch": 1.1193206069793797,
      "grad_norm": 2.151862382888794,
      "learning_rate": 4.068335385726788e-05,
      "loss": 1.8532,
      "step": 76050
    },
    {
      "epoch": 1.120056517963587,
      "grad_norm": 2.1145119667053223,
      "learning_rate": 4.062657402246933e-05,
      "loss": 1.8851,
      "step": 76100
    },
    {
      "epoch": 1.1207924289477944,
      "grad_norm": 2.2634499073028564,
      "learning_rate": 4.0569806712985005e-05,
      "loss": 1.8095,
      "step": 76150
    },
    {
      "epoch": 1.1215283399320017,
      "grad_norm": 2.5301077365875244,
      "learning_rate": 4.051305200467068e-05,
      "loss": 1.8131,
      "step": 76200
    },
    {
      "epoch": 1.122264250916209,
      "grad_norm": 2.440380811691284,
      "learning_rate": 4.0456309973365295e-05,
      "loss": 1.8416,
      "step": 76250
    },
    {
      "epoch": 1.1230001619004166,
      "grad_norm": 2.3251800537109375,
      "learning_rate": 4.03995806948908e-05,
      "loss": 1.765,
      "step": 76300
    },
    {
      "epoch": 1.123736072884624,
      "grad_norm": 2.025926351547241,
      "learning_rate": 4.034286424505217e-05,
      "loss": 1.8446,
      "step": 76350
    },
    {
      "epoch": 1.1244719838688313,
      "grad_norm": 2.024362087249756,
      "learning_rate": 4.0286160699637205e-05,
      "loss": 1.8326,
      "step": 76400
    },
    {
      "epoch": 1.1252078948530386,
      "grad_norm": 2.072702407836914,
      "learning_rate": 4.022947013441646e-05,
      "loss": 1.892,
      "step": 76450
    },
    {
      "epoch": 1.125943805837246,
      "grad_norm": 2.498087167739868,
      "learning_rate": 4.017279262514314e-05,
      "loss": 1.861,
      "step": 76500
    },
    {
      "epoch": 1.1266797168214533,
      "grad_norm": 2.1571576595306396,
      "learning_rate": 4.0116128247553026e-05,
      "loss": 1.8423,
      "step": 76550
    },
    {
      "epoch": 1.1274156278056606,
      "grad_norm": 2.3832173347473145,
      "learning_rate": 4.0059477077364325e-05,
      "loss": 1.7987,
      "step": 76600
    },
    {
      "epoch": 1.128151538789868,
      "grad_norm": 2.202962875366211,
      "learning_rate": 4.000283919027763e-05,
      "loss": 1.8668,
      "step": 76650
    },
    {
      "epoch": 1.1288874497740753,
      "grad_norm": 2.036783218383789,
      "learning_rate": 3.994621466197574e-05,
      "loss": 1.8092,
      "step": 76700
    },
    {
      "epoch": 1.1296233607582826,
      "grad_norm": 1.9983165264129639,
      "learning_rate": 3.988960356812363e-05,
      "loss": 1.8018,
      "step": 76750
    },
    {
      "epoch": 1.13035927174249,
      "grad_norm": 2.134889602661133,
      "learning_rate": 3.9833005984368345e-05,
      "loss": 1.8525,
      "step": 76800
    },
    {
      "epoch": 1.1310951827266973,
      "grad_norm": 1.9910097122192383,
      "learning_rate": 3.9776421986338817e-05,
      "loss": 1.8441,
      "step": 76850
    },
    {
      "epoch": 1.1318310937109048,
      "grad_norm": 2.186122179031372,
      "learning_rate": 3.9719851649645876e-05,
      "loss": 1.8829,
      "step": 76900
    },
    {
      "epoch": 1.1325670046951122,
      "grad_norm": 2.284175395965576,
      "learning_rate": 3.966329504988208e-05,
      "loss": 1.9037,
      "step": 76950
    },
    {
      "epoch": 1.1333029156793195,
      "grad_norm": 2.1917920112609863,
      "learning_rate": 3.960675226262163e-05,
      "loss": 1.8562,
      "step": 77000
    },
    {
      "epoch": 1.1340388266635268,
      "grad_norm": 2.2231125831604004,
      "learning_rate": 3.955022336342027e-05,
      "loss": 1.8224,
      "step": 77050
    },
    {
      "epoch": 1.1347747376477342,
      "grad_norm": 1.8661195039749146,
      "learning_rate": 3.9493708427815186e-05,
      "loss": 1.8116,
      "step": 77100
    },
    {
      "epoch": 1.1355106486319415,
      "grad_norm": 2.2601702213287354,
      "learning_rate": 3.9437207531324904e-05,
      "loss": 1.8941,
      "step": 77150
    },
    {
      "epoch": 1.1362465596161488,
      "grad_norm": 2.121901750564575,
      "learning_rate": 3.93807207494492e-05,
      "loss": 1.8504,
      "step": 77200
    },
    {
      "epoch": 1.1369824706003562,
      "grad_norm": 2.113661050796509,
      "learning_rate": 3.9324248157668966e-05,
      "loss": 1.8072,
      "step": 77250
    },
    {
      "epoch": 1.1377183815845635,
      "grad_norm": 2.1582860946655273,
      "learning_rate": 3.9267789831446166e-05,
      "loss": 1.8139,
      "step": 77300
    },
    {
      "epoch": 1.1384542925687708,
      "grad_norm": 2.3239481449127197,
      "learning_rate": 3.921134584622366e-05,
      "loss": 1.8532,
      "step": 77350
    },
    {
      "epoch": 1.1391902035529782,
      "grad_norm": 2.2585575580596924,
      "learning_rate": 3.915491627742519e-05,
      "loss": 1.826,
      "step": 77400
    },
    {
      "epoch": 1.1399261145371855,
      "grad_norm": 2.2244086265563965,
      "learning_rate": 3.909850120045519e-05,
      "loss": 1.8963,
      "step": 77450
    },
    {
      "epoch": 1.1406620255213928,
      "grad_norm": 2.4213204383850098,
      "learning_rate": 3.904210069069876e-05,
      "loss": 1.8045,
      "step": 77500
    },
    {
      "epoch": 1.1413979365056002,
      "grad_norm": 2.236180305480957,
      "learning_rate": 3.8985714823521535e-05,
      "loss": 1.8951,
      "step": 77550
    },
    {
      "epoch": 1.1421338474898077,
      "grad_norm": 2.3085291385650635,
      "learning_rate": 3.8929343674269544e-05,
      "loss": 1.8431,
      "step": 77600
    },
    {
      "epoch": 1.142869758474015,
      "grad_norm": 2.1567320823669434,
      "learning_rate": 3.8872987318269225e-05,
      "loss": 1.8475,
      "step": 77650
    },
    {
      "epoch": 1.1436056694582224,
      "grad_norm": 2.0177624225616455,
      "learning_rate": 3.881664583082717e-05,
      "loss": 1.775,
      "step": 77700
    },
    {
      "epoch": 1.1443415804424297,
      "grad_norm": 2.1846370697021484,
      "learning_rate": 3.876031928723014e-05,
      "loss": 1.7986,
      "step": 77750
    },
    {
      "epoch": 1.145077491426637,
      "grad_norm": 2.1246511936187744,
      "learning_rate": 3.870400776274493e-05,
      "loss": 1.8427,
      "step": 77800
    },
    {
      "epoch": 1.1458134024108444,
      "grad_norm": 2.2656326293945312,
      "learning_rate": 3.8647711332618254e-05,
      "loss": 1.8025,
      "step": 77850
    },
    {
      "epoch": 1.1465493133950517,
      "grad_norm": 2.25349497795105,
      "learning_rate": 3.859143007207665e-05,
      "loss": 1.9128,
      "step": 77900
    },
    {
      "epoch": 1.147285224379259,
      "grad_norm": 2.3699095249176025,
      "learning_rate": 3.853516405632641e-05,
      "loss": 1.8153,
      "step": 77950
    },
    {
      "epoch": 1.1480211353634664,
      "grad_norm": 2.067220449447632,
      "learning_rate": 3.847891336055343e-05,
      "loss": 1.781,
      "step": 78000
    },
    {
      "epoch": 1.1487570463476737,
      "grad_norm": 2.3949148654937744,
      "learning_rate": 3.842267805992315e-05,
      "loss": 1.8227,
      "step": 78050
    },
    {
      "epoch": 1.149492957331881,
      "grad_norm": 2.3183205127716064,
      "learning_rate": 3.836645822958043e-05,
      "loss": 1.9063,
      "step": 78100
    },
    {
      "epoch": 1.1502288683160884,
      "grad_norm": 2.4460880756378174,
      "learning_rate": 3.8310253944649465e-05,
      "loss": 1.8243,
      "step": 78150
    },
    {
      "epoch": 1.150964779300296,
      "grad_norm": 2.4323105812072754,
      "learning_rate": 3.8254065280233676e-05,
      "loss": 1.8231,
      "step": 78200
    },
    {
      "epoch": 1.1517006902845033,
      "grad_norm": 2.415008544921875,
      "learning_rate": 3.819789231141559e-05,
      "loss": 1.8133,
      "step": 78250
    },
    {
      "epoch": 1.1524366012687106,
      "grad_norm": 2.1544594764709473,
      "learning_rate": 3.814173511325679e-05,
      "loss": 1.8894,
      "step": 78300
    },
    {
      "epoch": 1.153172512252918,
      "grad_norm": 2.1901891231536865,
      "learning_rate": 3.808559376079779e-05,
      "loss": 1.8647,
      "step": 78350
    },
    {
      "epoch": 1.1539084232371253,
      "grad_norm": 2.3494627475738525,
      "learning_rate": 3.802946832905789e-05,
      "loss": 1.8524,
      "step": 78400
    },
    {
      "epoch": 1.1546443342213326,
      "grad_norm": 2.232811689376831,
      "learning_rate": 3.797335889303516e-05,
      "loss": 1.8343,
      "step": 78450
    },
    {
      "epoch": 1.15538024520554,
      "grad_norm": 2.0522730350494385,
      "learning_rate": 3.791726552770626e-05,
      "loss": 1.8412,
      "step": 78500
    },
    {
      "epoch": 1.1561161561897473,
      "grad_norm": 2.1891672611236572,
      "learning_rate": 3.7861188308026394e-05,
      "loss": 1.8557,
      "step": 78550
    },
    {
      "epoch": 1.1568520671739546,
      "grad_norm": 2.06390643119812,
      "learning_rate": 3.780512730892919e-05,
      "loss": 1.7687,
      "step": 78600
    },
    {
      "epoch": 1.157587978158162,
      "grad_norm": 1.9977071285247803,
      "learning_rate": 3.774908260532661e-05,
      "loss": 1.8621,
      "step": 78650
    },
    {
      "epoch": 1.1583238891423693,
      "grad_norm": 2.485771894454956,
      "learning_rate": 3.769305427210881e-05,
      "loss": 1.8701,
      "step": 78700
    },
    {
      "epoch": 1.1590598001265766,
      "grad_norm": 2.667346477508545,
      "learning_rate": 3.7637042384144106e-05,
      "loss": 1.9174,
      "step": 78750
    },
    {
      "epoch": 1.159795711110784,
      "grad_norm": 2.1954495906829834,
      "learning_rate": 3.7581047016278815e-05,
      "loss": 1.8414,
      "step": 78800
    },
    {
      "epoch": 1.1605316220949913,
      "grad_norm": 2.0579614639282227,
      "learning_rate": 3.752506824333717e-05,
      "loss": 1.8449,
      "step": 78850
    },
    {
      "epoch": 1.1612675330791988,
      "grad_norm": 2.2124834060668945,
      "learning_rate": 3.746910614012129e-05,
      "loss": 1.8212,
      "step": 78900
    },
    {
      "epoch": 1.1620034440634062,
      "grad_norm": 2.214799165725708,
      "learning_rate": 3.741316078141096e-05,
      "loss": 1.8538,
      "step": 78950
    },
    {
      "epoch": 1.1627393550476135,
      "grad_norm": 2.009622812271118,
      "learning_rate": 3.735723224196361e-05,
      "loss": 1.8383,
      "step": 79000
    },
    {
      "epoch": 1.1634752660318208,
      "grad_norm": 2.1830673217773438,
      "learning_rate": 3.730132059651417e-05,
      "loss": 1.843,
      "step": 79050
    },
    {
      "epoch": 1.1642111770160282,
      "grad_norm": 2.4918088912963867,
      "learning_rate": 3.724542591977502e-05,
      "loss": 1.7889,
      "step": 79100
    },
    {
      "epoch": 1.1649470880002355,
      "grad_norm": 2.108696699142456,
      "learning_rate": 3.7189548286435885e-05,
      "loss": 1.8755,
      "step": 79150
    },
    {
      "epoch": 1.1656829989844428,
      "grad_norm": 1.9405195713043213,
      "learning_rate": 3.713368777116367e-05,
      "loss": 1.8437,
      "step": 79200
    },
    {
      "epoch": 1.1664189099686502,
      "grad_norm": 1.7961505651474,
      "learning_rate": 3.7077844448602425e-05,
      "loss": 1.9079,
      "step": 79250
    },
    {
      "epoch": 1.1671548209528575,
      "grad_norm": 2.199695587158203,
      "learning_rate": 3.7022018393373256e-05,
      "loss": 1.8249,
      "step": 79300
    },
    {
      "epoch": 1.1678907319370648,
      "grad_norm": 2.4384000301361084,
      "learning_rate": 3.696620968007415e-05,
      "loss": 1.7407,
      "step": 79350
    },
    {
      "epoch": 1.1686266429212722,
      "grad_norm": 2.198868751525879,
      "learning_rate": 3.691041838327993e-05,
      "loss": 1.8621,
      "step": 79400
    },
    {
      "epoch": 1.1693625539054797,
      "grad_norm": 1.8781354427337646,
      "learning_rate": 3.6854644577542176e-05,
      "loss": 1.8484,
      "step": 79450
    },
    {
      "epoch": 1.170098464889687,
      "grad_norm": 2.3704769611358643,
      "learning_rate": 3.679888833738906e-05,
      "loss": 1.7747,
      "step": 79500
    },
    {
      "epoch": 1.1708343758738944,
      "grad_norm": 1.8736380338668823,
      "learning_rate": 3.67431497373253e-05,
      "loss": 1.843,
      "step": 79550
    },
    {
      "epoch": 1.1715702868581017,
      "grad_norm": 1.8770204782485962,
      "learning_rate": 3.668742885183205e-05,
      "loss": 1.8679,
      "step": 79600
    },
    {
      "epoch": 1.172306197842309,
      "grad_norm": 1.9970285892486572,
      "learning_rate": 3.6631725755366776e-05,
      "loss": 1.782,
      "step": 79650
    },
    {
      "epoch": 1.1730421088265164,
      "grad_norm": 2.0807251930236816,
      "learning_rate": 3.657604052236318e-05,
      "loss": 1.9069,
      "step": 79700
    },
    {
      "epoch": 1.1737780198107237,
      "grad_norm": 1.9562492370605469,
      "learning_rate": 3.652037322723107e-05,
      "loss": 1.8679,
      "step": 79750
    },
    {
      "epoch": 1.174513930794931,
      "grad_norm": 2.2947189807891846,
      "learning_rate": 3.646472394435635e-05,
      "loss": 1.8747,
      "step": 79800
    },
    {
      "epoch": 1.1752498417791384,
      "grad_norm": 2.294398069381714,
      "learning_rate": 3.6409092748100775e-05,
      "loss": 1.8307,
      "step": 79850
    },
    {
      "epoch": 1.1759857527633457,
      "grad_norm": 2.1505560874938965,
      "learning_rate": 3.635347971280199e-05,
      "loss": 1.7998,
      "step": 79900
    },
    {
      "epoch": 1.176721663747553,
      "grad_norm": 2.064962148666382,
      "learning_rate": 3.6297884912773336e-05,
      "loss": 1.8621,
      "step": 79950
    },
    {
      "epoch": 1.1774575747317604,
      "grad_norm": 3.124835252761841,
      "learning_rate": 3.6242308422303826e-05,
      "loss": 1.87,
      "step": 80000
    },
    {
      "epoch": 1.1781934857159677,
      "grad_norm": 2.2014145851135254,
      "learning_rate": 3.6186750315657955e-05,
      "loss": 1.8449,
      "step": 80050
    },
    {
      "epoch": 1.178929396700175,
      "grad_norm": 2.6756792068481445,
      "learning_rate": 3.61312106670757e-05,
      "loss": 1.8418,
      "step": 80100
    },
    {
      "epoch": 1.1796653076843824,
      "grad_norm": 2.2962164878845215,
      "learning_rate": 3.607568955077232e-05,
      "loss": 1.8341,
      "step": 80150
    },
    {
      "epoch": 1.18040121866859,
      "grad_norm": 2.0532467365264893,
      "learning_rate": 3.6020187040938385e-05,
      "loss": 1.7895,
      "step": 80200
    },
    {
      "epoch": 1.1811371296527973,
      "grad_norm": 2.298471212387085,
      "learning_rate": 3.596470321173955e-05,
      "loss": 1.8143,
      "step": 80250
    },
    {
      "epoch": 1.1818730406370046,
      "grad_norm": 2.2317659854888916,
      "learning_rate": 3.590923813731651e-05,
      "loss": 1.7998,
      "step": 80300
    },
    {
      "epoch": 1.182608951621212,
      "grad_norm": 2.125602960586548,
      "learning_rate": 3.5853791891784924e-05,
      "loss": 1.8751,
      "step": 80350
    },
    {
      "epoch": 1.1833448626054193,
      "grad_norm": 1.9536343812942505,
      "learning_rate": 3.579836454923525e-05,
      "loss": 1.8493,
      "step": 80400
    },
    {
      "epoch": 1.1840807735896266,
      "grad_norm": 2.3428192138671875,
      "learning_rate": 3.574295618373272e-05,
      "loss": 1.8635,
      "step": 80450
    },
    {
      "epoch": 1.184816684573834,
      "grad_norm": 2.2457704544067383,
      "learning_rate": 3.56875668693172e-05,
      "loss": 1.8611,
      "step": 80500
    },
    {
      "epoch": 1.1855525955580413,
      "grad_norm": 2.181349515914917,
      "learning_rate": 3.56321966800031e-05,
      "loss": 1.9055,
      "step": 80550
    },
    {
      "epoch": 1.1862885065422486,
      "grad_norm": 2.0339691638946533,
      "learning_rate": 3.557684568977927e-05,
      "loss": 1.8422,
      "step": 80600
    },
    {
      "epoch": 1.187024417526456,
      "grad_norm": 2.3192954063415527,
      "learning_rate": 3.55215139726089e-05,
      "loss": 1.8263,
      "step": 80650
    },
    {
      "epoch": 1.1877603285106633,
      "grad_norm": 2.0748696327209473,
      "learning_rate": 3.546620160242944e-05,
      "loss": 1.8635,
      "step": 80700
    },
    {
      "epoch": 1.1884962394948708,
      "grad_norm": 2.2912020683288574,
      "learning_rate": 3.541090865315248e-05,
      "loss": 1.7786,
      "step": 80750
    },
    {
      "epoch": 1.1892321504790782,
      "grad_norm": 2.4004039764404297,
      "learning_rate": 3.5355635198663636e-05,
      "loss": 1.8225,
      "step": 80800
    },
    {
      "epoch": 1.1899680614632855,
      "grad_norm": 2.232241630554199,
      "learning_rate": 3.530038131282253e-05,
      "loss": 1.8271,
      "step": 80850
    },
    {
      "epoch": 1.1907039724474928,
      "grad_norm": 2.1272008419036865,
      "learning_rate": 3.524514706946259e-05,
      "loss": 1.8112,
      "step": 80900
    },
    {
      "epoch": 1.1914398834317002,
      "grad_norm": 2.6303203105926514,
      "learning_rate": 3.518993254239101e-05,
      "loss": 1.8575,
      "step": 80950
    },
    {
      "epoch": 1.1921757944159075,
      "grad_norm": 2.345573902130127,
      "learning_rate": 3.513473780538863e-05,
      "loss": 1.8339,
      "step": 81000
    },
    {
      "epoch": 1.1929117054001148,
      "grad_norm": 2.2395682334899902,
      "learning_rate": 3.5079562932209853e-05,
      "loss": 1.7813,
      "step": 81050
    },
    {
      "epoch": 1.1936476163843222,
      "grad_norm": 2.153801441192627,
      "learning_rate": 3.5024407996582545e-05,
      "loss": 1.841,
      "step": 81100
    },
    {
      "epoch": 1.1943835273685295,
      "grad_norm": 1.862652063369751,
      "learning_rate": 3.4969273072207916e-05,
      "loss": 1.8175,
      "step": 81150
    },
    {
      "epoch": 1.1951194383527368,
      "grad_norm": 2.4080681800842285,
      "learning_rate": 3.491415823276044e-05,
      "loss": 1.8412,
      "step": 81200
    },
    {
      "epoch": 1.1958553493369442,
      "grad_norm": 2.0106449127197266,
      "learning_rate": 3.485906355188776e-05,
      "loss": 1.7896,
      "step": 81250
    },
    {
      "epoch": 1.1965912603211515,
      "grad_norm": 1.7851241827011108,
      "learning_rate": 3.4803989103210576e-05,
      "loss": 1.7872,
      "step": 81300
    },
    {
      "epoch": 1.1973271713053588,
      "grad_norm": 2.305856466293335,
      "learning_rate": 3.474893496032257e-05,
      "loss": 1.7671,
      "step": 81350
    },
    {
      "epoch": 1.1980630822895662,
      "grad_norm": 1.906153917312622,
      "learning_rate": 3.469390119679024e-05,
      "loss": 1.8335,
      "step": 81400
    },
    {
      "epoch": 1.1987989932737735,
      "grad_norm": 2.1002767086029053,
      "learning_rate": 3.463888788615288e-05,
      "loss": 1.8121,
      "step": 81450
    },
    {
      "epoch": 1.199534904257981,
      "grad_norm": 2.098567247390747,
      "learning_rate": 3.458389510192251e-05,
      "loss": 1.8313,
      "step": 81500
    },
    {
      "epoch": 1.2002708152421884,
      "grad_norm": 2.0937368869781494,
      "learning_rate": 3.452892291758364e-05,
      "loss": 1.8358,
      "step": 81550
    },
    {
      "epoch": 1.2010067262263957,
      "grad_norm": 2.030350923538208,
      "learning_rate": 3.447397140659329e-05,
      "loss": 1.8417,
      "step": 81600
    },
    {
      "epoch": 1.201742637210603,
      "grad_norm": 2.2768361568450928,
      "learning_rate": 3.441904064238084e-05,
      "loss": 1.8418,
      "step": 81650
    },
    {
      "epoch": 1.2024785481948104,
      "grad_norm": 1.9913806915283203,
      "learning_rate": 3.4364130698347976e-05,
      "loss": 1.8343,
      "step": 81700
    },
    {
      "epoch": 1.2032144591790177,
      "grad_norm": 1.9900513887405396,
      "learning_rate": 3.430924164786852e-05,
      "loss": 1.8727,
      "step": 81750
    },
    {
      "epoch": 1.203950370163225,
      "grad_norm": 1.9478458166122437,
      "learning_rate": 3.42543735642884e-05,
      "loss": 1.7502,
      "step": 81800
    },
    {
      "epoch": 1.2046862811474324,
      "grad_norm": 1.9554080963134766,
      "learning_rate": 3.4199526520925535e-05,
      "loss": 1.8338,
      "step": 81850
    },
    {
      "epoch": 1.2054221921316397,
      "grad_norm": 2.4944732189178467,
      "learning_rate": 3.414470059106971e-05,
      "loss": 1.8405,
      "step": 81900
    },
    {
      "epoch": 1.206158103115847,
      "grad_norm": 2.0592756271362305,
      "learning_rate": 3.408989584798252e-05,
      "loss": 1.7428,
      "step": 81950
    },
    {
      "epoch": 1.2068940141000544,
      "grad_norm": 2.2570018768310547,
      "learning_rate": 3.4035112364897215e-05,
      "loss": 1.8461,
      "step": 82000
    },
    {
      "epoch": 1.207629925084262,
      "grad_norm": 2.174853801727295,
      "learning_rate": 3.398035021501867e-05,
      "loss": 1.7999,
      "step": 82050
    },
    {
      "epoch": 1.2083658360684693,
      "grad_norm": 2.4586846828460693,
      "learning_rate": 3.3925609471523215e-05,
      "loss": 1.8043,
      "step": 82100
    },
    {
      "epoch": 1.2091017470526766,
      "grad_norm": 2.301455020904541,
      "learning_rate": 3.3870890207558634e-05,
      "loss": 1.8252,
      "step": 82150
    },
    {
      "epoch": 1.209837658036884,
      "grad_norm": 1.9599536657333374,
      "learning_rate": 3.381619249624396e-05,
      "loss": 1.8146,
      "step": 82200
    },
    {
      "epoch": 1.2105735690210913,
      "grad_norm": 1.8896820545196533,
      "learning_rate": 3.376151641066944e-05,
      "loss": 1.8703,
      "step": 82250
    },
    {
      "epoch": 1.2113094800052986,
      "grad_norm": 1.810192346572876,
      "learning_rate": 3.3706862023896416e-05,
      "loss": 1.8093,
      "step": 82300
    },
    {
      "epoch": 1.212045390989506,
      "grad_norm": 2.2011208534240723,
      "learning_rate": 3.365222940895724e-05,
      "loss": 1.7785,
      "step": 82350
    },
    {
      "epoch": 1.2127813019737133,
      "grad_norm": 2.3279075622558594,
      "learning_rate": 3.359761863885519e-05,
      "loss": 1.8215,
      "step": 82400
    },
    {
      "epoch": 1.2135172129579206,
      "grad_norm": 2.1224746704101562,
      "learning_rate": 3.354302978656433e-05,
      "loss": 1.863,
      "step": 82450
    },
    {
      "epoch": 1.214253123942128,
      "grad_norm": 2.176142692565918,
      "learning_rate": 3.348846292502944e-05,
      "loss": 1.7823,
      "step": 82500
    },
    {
      "epoch": 1.2149890349263353,
      "grad_norm": 2.015151262283325,
      "learning_rate": 3.3433918127165925e-05,
      "loss": 1.843,
      "step": 82550
    },
    {
      "epoch": 1.2157249459105426,
      "grad_norm": 2.119253396987915,
      "learning_rate": 3.33793954658597e-05,
      "loss": 1.8287,
      "step": 82600
    },
    {
      "epoch": 1.21646085689475,
      "grad_norm": 2.0918712615966797,
      "learning_rate": 3.332489501396708e-05,
      "loss": 1.9321,
      "step": 82650
    },
    {
      "epoch": 1.2171967678789573,
      "grad_norm": 2.2442755699157715,
      "learning_rate": 3.327041684431477e-05,
      "loss": 1.8669,
      "step": 82700
    },
    {
      "epoch": 1.2179326788631648,
      "grad_norm": 2.4489452838897705,
      "learning_rate": 3.321596102969958e-05,
      "loss": 1.7957,
      "step": 82750
    },
    {
      "epoch": 1.2186685898473721,
      "grad_norm": 2.0018200874328613,
      "learning_rate": 3.316152764288858e-05,
      "loss": 1.8604,
      "step": 82800
    },
    {
      "epoch": 1.2194045008315795,
      "grad_norm": 2.1956629753112793,
      "learning_rate": 3.3107116756618805e-05,
      "loss": 1.7961,
      "step": 82850
    },
    {
      "epoch": 1.2201404118157868,
      "grad_norm": 2.2140660285949707,
      "learning_rate": 3.305272844359721e-05,
      "loss": 1.7593,
      "step": 82900
    },
    {
      "epoch": 1.2208763227999941,
      "grad_norm": 2.0145230293273926,
      "learning_rate": 3.2998362776500624e-05,
      "loss": 1.8719,
      "step": 82950
    },
    {
      "epoch": 1.2216122337842015,
      "grad_norm": 2.7377893924713135,
      "learning_rate": 3.2944019827975594e-05,
      "loss": 1.8761,
      "step": 83000
    },
    {
      "epoch": 1.2223481447684088,
      "grad_norm": 2.3731391429901123,
      "learning_rate": 3.288969967063831e-05,
      "loss": 1.7967,
      "step": 83050
    },
    {
      "epoch": 1.2230840557526161,
      "grad_norm": 2.3008437156677246,
      "learning_rate": 3.28354023770745e-05,
      "loss": 1.8107,
      "step": 83100
    },
    {
      "epoch": 1.2238199667368235,
      "grad_norm": 1.9214129447937012,
      "learning_rate": 3.2781128019839355e-05,
      "loss": 1.842,
      "step": 83150
    },
    {
      "epoch": 1.2245558777210308,
      "grad_norm": 1.9101228713989258,
      "learning_rate": 3.272687667145742e-05,
      "loss": 1.7956,
      "step": 83200
    },
    {
      "epoch": 1.2252917887052381,
      "grad_norm": 2.1941046714782715,
      "learning_rate": 3.2672648404422475e-05,
      "loss": 1.8318,
      "step": 83250
    },
    {
      "epoch": 1.2260276996894455,
      "grad_norm": 2.2421505451202393,
      "learning_rate": 3.2618443291197476e-05,
      "loss": 1.7947,
      "step": 83300
    },
    {
      "epoch": 1.226763610673653,
      "grad_norm": 2.497931480407715,
      "learning_rate": 3.256426140421444e-05,
      "loss": 1.873,
      "step": 83350
    },
    {
      "epoch": 1.2274995216578604,
      "grad_norm": 2.337578535079956,
      "learning_rate": 3.251010281587431e-05,
      "loss": 1.8279,
      "step": 83400
    },
    {
      "epoch": 1.2282354326420677,
      "grad_norm": 2.2129499912261963,
      "learning_rate": 3.245596759854696e-05,
      "loss": 1.8374,
      "step": 83450
    },
    {
      "epoch": 1.228971343626275,
      "grad_norm": 1.9638625383377075,
      "learning_rate": 3.240185582457099e-05,
      "loss": 1.7965,
      "step": 83500
    },
    {
      "epoch": 1.2297072546104824,
      "grad_norm": 2.2005937099456787,
      "learning_rate": 3.234776756625369e-05,
      "loss": 1.8473,
      "step": 83550
    },
    {
      "epoch": 1.2304431655946897,
      "grad_norm": 2.1013402938842773,
      "learning_rate": 3.229370289587089e-05,
      "loss": 1.885,
      "step": 83600
    },
    {
      "epoch": 1.231179076578897,
      "grad_norm": 2.1717469692230225,
      "learning_rate": 3.223966188566697e-05,
      "loss": 1.8494,
      "step": 83650
    },
    {
      "epoch": 1.2319149875631044,
      "grad_norm": 2.240244150161743,
      "learning_rate": 3.2185644607854634e-05,
      "loss": 1.8288,
      "step": 83700
    },
    {
      "epoch": 1.2326508985473117,
      "grad_norm": 1.9677778482437134,
      "learning_rate": 3.2131651134614896e-05,
      "loss": 1.8203,
      "step": 83750
    },
    {
      "epoch": 1.233386809531519,
      "grad_norm": 2.4284133911132812,
      "learning_rate": 3.207768153809696e-05,
      "loss": 1.7611,
      "step": 83800
    },
    {
      "epoch": 1.2341227205157264,
      "grad_norm": 2.1752588748931885,
      "learning_rate": 3.2023735890418136e-05,
      "loss": 1.8301,
      "step": 83850
    },
    {
      "epoch": 1.2348586314999337,
      "grad_norm": 2.1016151905059814,
      "learning_rate": 3.19698142636637e-05,
      "loss": 1.813,
      "step": 83900
    },
    {
      "epoch": 1.235594542484141,
      "grad_norm": 1.987627625465393,
      "learning_rate": 3.1915916729886875e-05,
      "loss": 1.8428,
      "step": 83950
    },
    {
      "epoch": 1.2363304534683484,
      "grad_norm": 2.558410882949829,
      "learning_rate": 3.186204336110863e-05,
      "loss": 1.8038,
      "step": 84000
    },
    {
      "epoch": 1.237066364452556,
      "grad_norm": 2.26013445854187,
      "learning_rate": 3.180819422931772e-05,
      "loss": 1.7844,
      "step": 84050
    },
    {
      "epoch": 1.2378022754367632,
      "grad_norm": 2.068037509918213,
      "learning_rate": 3.175436940647044e-05,
      "loss": 1.8436,
      "step": 84100
    },
    {
      "epoch": 1.2385381864209706,
      "grad_norm": 2.0848984718322754,
      "learning_rate": 3.1700568964490645e-05,
      "loss": 1.8801,
      "step": 84150
    },
    {
      "epoch": 1.239274097405178,
      "grad_norm": 1.8990185260772705,
      "learning_rate": 3.1646792975269603e-05,
      "loss": 1.872,
      "step": 84200
    },
    {
      "epoch": 1.2400100083893852,
      "grad_norm": 2.1408424377441406,
      "learning_rate": 3.159304151066589e-05,
      "loss": 1.8895,
      "step": 84250
    },
    {
      "epoch": 1.2407459193735926,
      "grad_norm": 2.5201988220214844,
      "learning_rate": 3.153931464250533e-05,
      "loss": 1.8569,
      "step": 84300
    },
    {
      "epoch": 1.2414818303578,
      "grad_norm": 2.777305841445923,
      "learning_rate": 3.148561244258088e-05,
      "loss": 1.8848,
      "step": 84350
    },
    {
      "epoch": 1.2422177413420072,
      "grad_norm": 2.070983648300171,
      "learning_rate": 3.143193498265253e-05,
      "loss": 1.8174,
      "step": 84400
    },
    {
      "epoch": 1.2429536523262146,
      "grad_norm": 2.320093870162964,
      "learning_rate": 3.1378282334447186e-05,
      "loss": 1.8111,
      "step": 84450
    },
    {
      "epoch": 1.243689563310422,
      "grad_norm": 2.221548557281494,
      "learning_rate": 3.132465456965864e-05,
      "loss": 1.8129,
      "step": 84500
    },
    {
      "epoch": 1.2444254742946292,
      "grad_norm": 2.236488103866577,
      "learning_rate": 3.127105175994741e-05,
      "loss": 1.8574,
      "step": 84550
    },
    {
      "epoch": 1.2451613852788368,
      "grad_norm": 2.174487352371216,
      "learning_rate": 3.121747397694067e-05,
      "loss": 1.8143,
      "step": 84600
    },
    {
      "epoch": 1.2458972962630441,
      "grad_norm": 2.1812310218811035,
      "learning_rate": 3.116392129223213e-05,
      "loss": 1.7826,
      "step": 84650
    },
    {
      "epoch": 1.2466332072472515,
      "grad_norm": 2.3784055709838867,
      "learning_rate": 3.1110393777382045e-05,
      "loss": 1.8653,
      "step": 84700
    },
    {
      "epoch": 1.2473691182314588,
      "grad_norm": 2.342465877532959,
      "learning_rate": 3.105689150391695e-05,
      "loss": 1.8859,
      "step": 84750
    },
    {
      "epoch": 1.2481050292156661,
      "grad_norm": 2.1824543476104736,
      "learning_rate": 3.100341454332967e-05,
      "loss": 1.8247,
      "step": 84800
    },
    {
      "epoch": 1.2488409401998735,
      "grad_norm": 2.0348196029663086,
      "learning_rate": 3.0949962967079216e-05,
      "loss": 1.7563,
      "step": 84850
    },
    {
      "epoch": 1.2495768511840808,
      "grad_norm": 2.2900075912475586,
      "learning_rate": 3.089653684659068e-05,
      "loss": 1.8412,
      "step": 84900
    },
    {
      "epoch": 1.2503127621682881,
      "grad_norm": 2.452845811843872,
      "learning_rate": 3.0843136253255134e-05,
      "loss": 1.8712,
      "step": 84950
    },
    {
      "epoch": 1.2510486731524955,
      "grad_norm": 2.0335237979888916,
      "learning_rate": 3.078976125842955e-05,
      "loss": 1.7667,
      "step": 85000
    },
    {
      "epoch": 1.2517845841367028,
      "grad_norm": 1.995848536491394,
      "learning_rate": 3.073641193343668e-05,
      "loss": 1.7742,
      "step": 85050
    },
    {
      "epoch": 1.2525204951209101,
      "grad_norm": 2.001941442489624,
      "learning_rate": 3.068308834956497e-05,
      "loss": 1.8914,
      "step": 85100
    },
    {
      "epoch": 1.2532564061051175,
      "grad_norm": 2.0926461219787598,
      "learning_rate": 3.062979057806849e-05,
      "loss": 1.8347,
      "step": 85150
    },
    {
      "epoch": 1.2539923170893248,
      "grad_norm": 2.142875909805298,
      "learning_rate": 3.057651869016681e-05,
      "loss": 1.8065,
      "step": 85200
    },
    {
      "epoch": 1.2547282280735321,
      "grad_norm": 2.339829206466675,
      "learning_rate": 3.05232727570449e-05,
      "loss": 1.7787,
      "step": 85250
    },
    {
      "epoch": 1.2554641390577395,
      "grad_norm": 2.169076681137085,
      "learning_rate": 3.047005284985305e-05,
      "loss": 1.8453,
      "step": 85300
    },
    {
      "epoch": 1.2562000500419468,
      "grad_norm": 2.1730527877807617,
      "learning_rate": 3.0416859039706802e-05,
      "loss": 1.8389,
      "step": 85350
    },
    {
      "epoch": 1.2569359610261543,
      "grad_norm": 2.1005237102508545,
      "learning_rate": 3.0363691397686787e-05,
      "loss": 1.8031,
      "step": 85400
    },
    {
      "epoch": 1.2576718720103617,
      "grad_norm": 2.346470832824707,
      "learning_rate": 3.03105499948387e-05,
      "loss": 1.8374,
      "step": 85450
    },
    {
      "epoch": 1.258407782994569,
      "grad_norm": 2.2503278255462646,
      "learning_rate": 3.0257434902173138e-05,
      "loss": 1.7744,
      "step": 85500
    },
    {
      "epoch": 1.2591436939787763,
      "grad_norm": 1.8438206911087036,
      "learning_rate": 3.020434619066558e-05,
      "loss": 1.786,
      "step": 85550
    },
    {
      "epoch": 1.2598796049629837,
      "grad_norm": 1.8350975513458252,
      "learning_rate": 3.015128393125622e-05,
      "loss": 1.7742,
      "step": 85600
    },
    {
      "epoch": 1.260615515947191,
      "grad_norm": 2.479222059249878,
      "learning_rate": 3.0098248194849925e-05,
      "loss": 1.7783,
      "step": 85650
    },
    {
      "epoch": 1.2613514269313983,
      "grad_norm": 2.095562219619751,
      "learning_rate": 3.004523905231612e-05,
      "loss": 1.8163,
      "step": 85700
    },
    {
      "epoch": 1.2620873379156057,
      "grad_norm": 1.8672236204147339,
      "learning_rate": 2.9992256574488696e-05,
      "loss": 1.825,
      "step": 85750
    },
    {
      "epoch": 1.262823248899813,
      "grad_norm": 2.592714309692383,
      "learning_rate": 2.9939300832165896e-05,
      "loss": 1.8386,
      "step": 85800
    },
    {
      "epoch": 1.2635591598840206,
      "grad_norm": 2.13240122795105,
      "learning_rate": 2.9886371896110232e-05,
      "loss": 1.7709,
      "step": 85850
    },
    {
      "epoch": 1.264295070868228,
      "grad_norm": 2.0253074169158936,
      "learning_rate": 2.9833469837048435e-05,
      "loss": 1.8583,
      "step": 85900
    },
    {
      "epoch": 1.2650309818524352,
      "grad_norm": 1.957417607307434,
      "learning_rate": 2.9780594725671264e-05,
      "loss": 1.8213,
      "step": 85950
    },
    {
      "epoch": 1.2657668928366426,
      "grad_norm": 1.8543415069580078,
      "learning_rate": 2.9727746632633552e-05,
      "loss": 1.835,
      "step": 86000
    },
    {
      "epoch": 1.26650280382085,
      "grad_norm": 2.488361358642578,
      "learning_rate": 2.9674925628553952e-05,
      "loss": 1.7147,
      "step": 86050
    },
    {
      "epoch": 1.2672387148050572,
      "grad_norm": 1.8774917125701904,
      "learning_rate": 2.962213178401496e-05,
      "loss": 1.7784,
      "step": 86100
    },
    {
      "epoch": 1.2679746257892646,
      "grad_norm": 2.219142198562622,
      "learning_rate": 2.9569365169562758e-05,
      "loss": 1.8212,
      "step": 86150
    },
    {
      "epoch": 1.268710536773472,
      "grad_norm": 2.286355972290039,
      "learning_rate": 2.9516625855707137e-05,
      "loss": 1.7429,
      "step": 86200
    },
    {
      "epoch": 1.2694464477576792,
      "grad_norm": 2.2211880683898926,
      "learning_rate": 2.9463913912921428e-05,
      "loss": 1.7872,
      "step": 86250
    },
    {
      "epoch": 1.2701823587418866,
      "grad_norm": 2.525860548019409,
      "learning_rate": 2.941122941164239e-05,
      "loss": 1.8492,
      "step": 86300
    },
    {
      "epoch": 1.270918269726094,
      "grad_norm": 2.0507659912109375,
      "learning_rate": 2.9358572422270092e-05,
      "loss": 1.8139,
      "step": 86350
    },
    {
      "epoch": 1.2716541807103012,
      "grad_norm": 2.001121997833252,
      "learning_rate": 2.9305943015167854e-05,
      "loss": 1.8205,
      "step": 86400
    },
    {
      "epoch": 1.2723900916945086,
      "grad_norm": 2.3140463829040527,
      "learning_rate": 2.925334126066213e-05,
      "loss": 1.8084,
      "step": 86450
    },
    {
      "epoch": 1.273126002678716,
      "grad_norm": 2.385620355606079,
      "learning_rate": 2.9200767229042442e-05,
      "loss": 1.8186,
      "step": 86500
    },
    {
      "epoch": 1.2738619136629232,
      "grad_norm": 2.080533981323242,
      "learning_rate": 2.9148220990561248e-05,
      "loss": 1.8045,
      "step": 86550
    },
    {
      "epoch": 1.2745978246471306,
      "grad_norm": 2.249162435531616,
      "learning_rate": 2.9095702615433847e-05,
      "loss": 1.7905,
      "step": 86600
    },
    {
      "epoch": 1.275333735631338,
      "grad_norm": 2.30893611907959,
      "learning_rate": 2.904321217383837e-05,
      "loss": 1.8,
      "step": 86650
    },
    {
      "epoch": 1.2760696466155454,
      "grad_norm": 1.8766448497772217,
      "learning_rate": 2.8990749735915568e-05,
      "loss": 1.8233,
      "step": 86700
    },
    {
      "epoch": 1.2768055575997528,
      "grad_norm": 2.311314105987549,
      "learning_rate": 2.8938315371768783e-05,
      "loss": 1.7833,
      "step": 86750
    },
    {
      "epoch": 1.2775414685839601,
      "grad_norm": 2.4401304721832275,
      "learning_rate": 2.888590915146385e-05,
      "loss": 1.8072,
      "step": 86800
    },
    {
      "epoch": 1.2782773795681674,
      "grad_norm": 2.048332452774048,
      "learning_rate": 2.883353114502899e-05,
      "loss": 1.8125,
      "step": 86850
    },
    {
      "epoch": 1.2790132905523748,
      "grad_norm": 2.495378017425537,
      "learning_rate": 2.8781181422454728e-05,
      "loss": 1.8182,
      "step": 86900
    },
    {
      "epoch": 1.2797492015365821,
      "grad_norm": 2.302316904067993,
      "learning_rate": 2.8728860053693797e-05,
      "loss": 1.8659,
      "step": 86950
    },
    {
      "epoch": 1.2804851125207894,
      "grad_norm": 2.213949203491211,
      "learning_rate": 2.8676567108661034e-05,
      "loss": 1.7848,
      "step": 87000
    },
    {
      "epoch": 1.2812210235049968,
      "grad_norm": 2.0220351219177246,
      "learning_rate": 2.86243026572333e-05,
      "loss": 1.7903,
      "step": 87050
    },
    {
      "epoch": 1.2819569344892041,
      "grad_norm": 2.0685932636260986,
      "learning_rate": 2.8572066769249374e-05,
      "loss": 1.821,
      "step": 87100
    },
    {
      "epoch": 1.2826928454734117,
      "grad_norm": 2.1225192546844482,
      "learning_rate": 2.8519859514509878e-05,
      "loss": 1.8098,
      "step": 87150
    },
    {
      "epoch": 1.283428756457619,
      "grad_norm": 1.9742474555969238,
      "learning_rate": 2.8467680962777164e-05,
      "loss": 1.855,
      "step": 87200
    },
    {
      "epoch": 1.2841646674418263,
      "grad_norm": 1.8911393880844116,
      "learning_rate": 2.841553118377524e-05,
      "loss": 1.7743,
      "step": 87250
    },
    {
      "epoch": 1.2849005784260337,
      "grad_norm": 2.223560333251953,
      "learning_rate": 2.8363410247189646e-05,
      "loss": 1.8667,
      "step": 87300
    },
    {
      "epoch": 1.285636489410241,
      "grad_norm": 2.2288906574249268,
      "learning_rate": 2.8311318222667405e-05,
      "loss": 1.841,
      "step": 87350
    },
    {
      "epoch": 1.2863724003944483,
      "grad_norm": 2.0977978706359863,
      "learning_rate": 2.825925517981689e-05,
      "loss": 1.8079,
      "step": 87400
    },
    {
      "epoch": 1.2871083113786557,
      "grad_norm": 2.411290407180786,
      "learning_rate": 2.820722118820775e-05,
      "loss": 1.8383,
      "step": 87450
    },
    {
      "epoch": 1.287844222362863,
      "grad_norm": 2.536593437194824,
      "learning_rate": 2.815521631737082e-05,
      "loss": 1.796,
      "step": 87500
    },
    {
      "epoch": 1.2885801333470703,
      "grad_norm": 2.1955158710479736,
      "learning_rate": 2.8103240636798013e-05,
      "loss": 1.7743,
      "step": 87550
    },
    {
      "epoch": 1.2893160443312777,
      "grad_norm": 2.125002861022949,
      "learning_rate": 2.8051294215942248e-05,
      "loss": 1.8287,
      "step": 87600
    },
    {
      "epoch": 1.290051955315485,
      "grad_norm": 2.2649571895599365,
      "learning_rate": 2.799937712421733e-05,
      "loss": 1.7981,
      "step": 87650
    },
    {
      "epoch": 1.2907878662996923,
      "grad_norm": 2.046356439590454,
      "learning_rate": 2.7947489430997885e-05,
      "loss": 1.8135,
      "step": 87700
    },
    {
      "epoch": 1.2915237772838997,
      "grad_norm": 2.227206230163574,
      "learning_rate": 2.789563120561926e-05,
      "loss": 1.8156,
      "step": 87750
    },
    {
      "epoch": 1.292259688268107,
      "grad_norm": 2.4363059997558594,
      "learning_rate": 2.7843802517377406e-05,
      "loss": 1.8189,
      "step": 87800
    },
    {
      "epoch": 1.2929955992523143,
      "grad_norm": 1.991600751876831,
      "learning_rate": 2.779200343552882e-05,
      "loss": 1.8046,
      "step": 87850
    },
    {
      "epoch": 1.2937315102365217,
      "grad_norm": 1.8969892263412476,
      "learning_rate": 2.774023402929041e-05,
      "loss": 1.8091,
      "step": 87900
    },
    {
      "epoch": 1.2944674212207292,
      "grad_norm": 2.4241445064544678,
      "learning_rate": 2.768849436783949e-05,
      "loss": 1.7926,
      "step": 87950
    },
    {
      "epoch": 1.2952033322049366,
      "grad_norm": 2.178133249282837,
      "learning_rate": 2.7636784520313585e-05,
      "loss": 1.8519,
      "step": 88000
    },
    {
      "epoch": 1.2959392431891439,
      "grad_norm": 2.0044305324554443,
      "learning_rate": 2.758510455581037e-05,
      "loss": 1.8367,
      "step": 88050
    },
    {
      "epoch": 1.2966751541733512,
      "grad_norm": 2.3412625789642334,
      "learning_rate": 2.753345454338763e-05,
      "loss": 1.7722,
      "step": 88100
    },
    {
      "epoch": 1.2974110651575586,
      "grad_norm": 2.160054922103882,
      "learning_rate": 2.7481834552063052e-05,
      "loss": 1.7994,
      "step": 88150
    },
    {
      "epoch": 1.2981469761417659,
      "grad_norm": 2.3089048862457275,
      "learning_rate": 2.743024465081429e-05,
      "loss": 1.7865,
      "step": 88200
    },
    {
      "epoch": 1.2988828871259732,
      "grad_norm": 2.2232472896575928,
      "learning_rate": 2.737868490857875e-05,
      "loss": 1.7699,
      "step": 88250
    },
    {
      "epoch": 1.2996187981101806,
      "grad_norm": 2.3352363109588623,
      "learning_rate": 2.732715539425354e-05,
      "loss": 1.7822,
      "step": 88300
    },
    {
      "epoch": 1.3003547090943879,
      "grad_norm": 2.276597261428833,
      "learning_rate": 2.7275656176695386e-05,
      "loss": 1.7839,
      "step": 88350
    },
    {
      "epoch": 1.3010906200785952,
      "grad_norm": 2.1269075870513916,
      "learning_rate": 2.7224187324720513e-05,
      "loss": 1.8282,
      "step": 88400
    },
    {
      "epoch": 1.3018265310628028,
      "grad_norm": 2.243852138519287,
      "learning_rate": 2.7172748907104584e-05,
      "loss": 1.7931,
      "step": 88450
    },
    {
      "epoch": 1.30256244204701,
      "grad_norm": 2.231212615966797,
      "learning_rate": 2.7121340992582593e-05,
      "loss": 1.808,
      "step": 88500
    },
    {
      "epoch": 1.3032983530312174,
      "grad_norm": 2.24717116355896,
      "learning_rate": 2.706996364984875e-05,
      "loss": 1.7417,
      "step": 88550
    },
    {
      "epoch": 1.3040342640154248,
      "grad_norm": 1.8557652235031128,
      "learning_rate": 2.701861694755647e-05,
      "loss": 1.7996,
      "step": 88600
    },
    {
      "epoch": 1.304770174999632,
      "grad_norm": 2.306622266769409,
      "learning_rate": 2.6967300954318166e-05,
      "loss": 1.7522,
      "step": 88650
    },
    {
      "epoch": 1.3055060859838394,
      "grad_norm": 2.307500123977661,
      "learning_rate": 2.691601573870525e-05,
      "loss": 1.8032,
      "step": 88700
    },
    {
      "epoch": 1.3062419969680468,
      "grad_norm": 2.185483455657959,
      "learning_rate": 2.6864761369247975e-05,
      "loss": 1.8043,
      "step": 88750
    },
    {
      "epoch": 1.306977907952254,
      "grad_norm": 1.9771316051483154,
      "learning_rate": 2.6813537914435416e-05,
      "loss": 1.8019,
      "step": 88800
    },
    {
      "epoch": 1.3077138189364614,
      "grad_norm": 1.9771935939788818,
      "learning_rate": 2.6762345442715298e-05,
      "loss": 1.8136,
      "step": 88850
    },
    {
      "epoch": 1.3084497299206688,
      "grad_norm": 2.324152708053589,
      "learning_rate": 2.6711184022493985e-05,
      "loss": 1.8199,
      "step": 88900
    },
    {
      "epoch": 1.309185640904876,
      "grad_norm": 2.0369224548339844,
      "learning_rate": 2.6660053722136313e-05,
      "loss": 1.8214,
      "step": 88950
    },
    {
      "epoch": 1.3099215518890834,
      "grad_norm": 2.5416295528411865,
      "learning_rate": 2.6608954609965552e-05,
      "loss": 1.8374,
      "step": 89000
    },
    {
      "epoch": 1.3106574628732908,
      "grad_norm": 2.3128299713134766,
      "learning_rate": 2.6557886754263294e-05,
      "loss": 1.8073,
      "step": 89050
    },
    {
      "epoch": 1.311393373857498,
      "grad_norm": 2.4036529064178467,
      "learning_rate": 2.650685022326938e-05,
      "loss": 1.8248,
      "step": 89100
    },
    {
      "epoch": 1.3121292848417054,
      "grad_norm": 2.6755263805389404,
      "learning_rate": 2.6455845085181717e-05,
      "loss": 1.7884,
      "step": 89150
    },
    {
      "epoch": 1.3128651958259128,
      "grad_norm": 2.1382999420166016,
      "learning_rate": 2.640487140815639e-05,
      "loss": 1.8232,
      "step": 89200
    },
    {
      "epoch": 1.3136011068101203,
      "grad_norm": 2.2387919425964355,
      "learning_rate": 2.6353929260307348e-05,
      "loss": 1.9013,
      "step": 89250
    },
    {
      "epoch": 1.3143370177943277,
      "grad_norm": 2.0879080295562744,
      "learning_rate": 2.630301870970644e-05,
      "loss": 1.7706,
      "step": 89300
    },
    {
      "epoch": 1.315072928778535,
      "grad_norm": 2.099256992340088,
      "learning_rate": 2.6252139824383282e-05,
      "loss": 1.7618,
      "step": 89350
    },
    {
      "epoch": 1.3158088397627423,
      "grad_norm": 2.5308125019073486,
      "learning_rate": 2.6201292672325196e-05,
      "loss": 1.7783,
      "step": 89400
    },
    {
      "epoch": 1.3165447507469497,
      "grad_norm": 2.252535820007324,
      "learning_rate": 2.6150477321477084e-05,
      "loss": 1.8317,
      "step": 89450
    },
    {
      "epoch": 1.317280661731157,
      "grad_norm": 2.2694499492645264,
      "learning_rate": 2.6099693839741363e-05,
      "loss": 1.8366,
      "step": 89500
    },
    {
      "epoch": 1.3180165727153643,
      "grad_norm": 2.186072587966919,
      "learning_rate": 2.604894229497785e-05,
      "loss": 1.7969,
      "step": 89550
    },
    {
      "epoch": 1.3187524836995717,
      "grad_norm": 2.1624197959899902,
      "learning_rate": 2.599822275500371e-05,
      "loss": 1.8344,
      "step": 89600
    },
    {
      "epoch": 1.319488394683779,
      "grad_norm": 2.788994073867798,
      "learning_rate": 2.5947535287593318e-05,
      "loss": 1.7729,
      "step": 89650
    },
    {
      "epoch": 1.3202243056679863,
      "grad_norm": 2.307279109954834,
      "learning_rate": 2.5896879960478203e-05,
      "loss": 1.8021,
      "step": 89700
    },
    {
      "epoch": 1.3209602166521939,
      "grad_norm": 2.481797456741333,
      "learning_rate": 2.5846256841346944e-05,
      "loss": 1.8827,
      "step": 89750
    },
    {
      "epoch": 1.3216961276364012,
      "grad_norm": 1.926340937614441,
      "learning_rate": 2.5795665997845085e-05,
      "loss": 1.7682,
      "step": 89800
    },
    {
      "epoch": 1.3224320386206085,
      "grad_norm": 2.0968244075775146,
      "learning_rate": 2.574510749757504e-05,
      "loss": 1.7962,
      "step": 89850
    },
    {
      "epoch": 1.3231679496048159,
      "grad_norm": 2.1870710849761963,
      "learning_rate": 2.5694581408095992e-05,
      "loss": 1.7503,
      "step": 89900
    },
    {
      "epoch": 1.3239038605890232,
      "grad_norm": 2.1828439235687256,
      "learning_rate": 2.564408779692384e-05,
      "loss": 1.7495,
      "step": 89950
    },
    {
      "epoch": 1.3246397715732305,
      "grad_norm": 2.137957811355591,
      "learning_rate": 2.5593626731531063e-05,
      "loss": 1.7618,
      "step": 90000
    },
    {
      "epoch": 1.3253756825574379,
      "grad_norm": 2.560812473297119,
      "learning_rate": 2.554319827934665e-05,
      "loss": 1.8034,
      "step": 90050
    },
    {
      "epoch": 1.3261115935416452,
      "grad_norm": 2.2527270317077637,
      "learning_rate": 2.549280250775603e-05,
      "loss": 1.8127,
      "step": 90100
    },
    {
      "epoch": 1.3268475045258525,
      "grad_norm": 2.0789902210235596,
      "learning_rate": 2.5442439484100945e-05,
      "loss": 1.7796,
      "step": 90150
    },
    {
      "epoch": 1.3275834155100599,
      "grad_norm": 2.170086145401001,
      "learning_rate": 2.5392109275679377e-05,
      "loss": 1.7855,
      "step": 90200
    },
    {
      "epoch": 1.3283193264942672,
      "grad_norm": 2.4533512592315674,
      "learning_rate": 2.5341811949745465e-05,
      "loss": 1.766,
      "step": 90250
    },
    {
      "epoch": 1.3290552374784745,
      "grad_norm": 2.023625612258911,
      "learning_rate": 2.5291547573509412e-05,
      "loss": 1.771,
      "step": 90300
    },
    {
      "epoch": 1.3297911484626819,
      "grad_norm": 2.1703808307647705,
      "learning_rate": 2.524131621413738e-05,
      "loss": 1.8075,
      "step": 90350
    },
    {
      "epoch": 1.3305270594468892,
      "grad_norm": 2.1985254287719727,
      "learning_rate": 2.519111793875142e-05,
      "loss": 1.7782,
      "step": 90400
    },
    {
      "epoch": 1.3312629704310965,
      "grad_norm": 2.3342113494873047,
      "learning_rate": 2.5140952814429352e-05,
      "loss": 1.8326,
      "step": 90450
    },
    {
      "epoch": 1.3319988814153039,
      "grad_norm": 2.264052629470825,
      "learning_rate": 2.509082090820476e-05,
      "loss": 1.732,
      "step": 90500
    },
    {
      "epoch": 1.3327347923995114,
      "grad_norm": 1.959162712097168,
      "learning_rate": 2.504072228706678e-05,
      "loss": 1.7829,
      "step": 90550
    },
    {
      "epoch": 1.3334707033837188,
      "grad_norm": 2.58677077293396,
      "learning_rate": 2.4990657017960084e-05,
      "loss": 1.7869,
      "step": 90600
    },
    {
      "epoch": 1.334206614367926,
      "grad_norm": 2.343432664871216,
      "learning_rate": 2.4940625167784787e-05,
      "loss": 1.7329,
      "step": 90650
    },
    {
      "epoch": 1.3349425253521334,
      "grad_norm": 2.2265310287475586,
      "learning_rate": 2.4890626803396337e-05,
      "loss": 1.8136,
      "step": 90700
    },
    {
      "epoch": 1.3356784363363408,
      "grad_norm": 2.464879035949707,
      "learning_rate": 2.4840661991605453e-05,
      "loss": 1.7647,
      "step": 90750
    },
    {
      "epoch": 1.336414347320548,
      "grad_norm": 2.4778358936309814,
      "learning_rate": 2.4790730799178007e-05,
      "loss": 1.766,
      "step": 90800
    },
    {
      "epoch": 1.3371502583047554,
      "grad_norm": 2.334475040435791,
      "learning_rate": 2.4740833292834932e-05,
      "loss": 1.839,
      "step": 90850
    },
    {
      "epoch": 1.3378861692889628,
      "grad_norm": 1.907142162322998,
      "learning_rate": 2.469096953925217e-05,
      "loss": 1.8376,
      "step": 90900
    },
    {
      "epoch": 1.33862208027317,
      "grad_norm": 2.170037031173706,
      "learning_rate": 2.4641139605060547e-05,
      "loss": 1.7629,
      "step": 90950
    },
    {
      "epoch": 1.3393579912573776,
      "grad_norm": 2.1575469970703125,
      "learning_rate": 2.4591343556845725e-05,
      "loss": 1.7908,
      "step": 91000
    },
    {
      "epoch": 1.340093902241585,
      "grad_norm": 2.0695409774780273,
      "learning_rate": 2.454158146114805e-05,
      "loss": 1.7899,
      "step": 91050
    },
    {
      "epoch": 1.3408298132257923,
      "grad_norm": 1.9622888565063477,
      "learning_rate": 2.4491853384462505e-05,
      "loss": 1.8227,
      "step": 91100
    },
    {
      "epoch": 1.3415657242099996,
      "grad_norm": 2.217251777648926,
      "learning_rate": 2.444215939323865e-05,
      "loss": 1.8201,
      "step": 91150
    },
    {
      "epoch": 1.342301635194207,
      "grad_norm": 2.118825912475586,
      "learning_rate": 2.439249955388046e-05,
      "loss": 1.8218,
      "step": 91200
    },
    {
      "epoch": 1.3430375461784143,
      "grad_norm": 2.2674806118011475,
      "learning_rate": 2.4342873932746285e-05,
      "loss": 1.8214,
      "step": 91250
    },
    {
      "epoch": 1.3437734571626216,
      "grad_norm": 2.0124928951263428,
      "learning_rate": 2.429328259614876e-05,
      "loss": 1.8887,
      "step": 91300
    },
    {
      "epoch": 1.344509368146829,
      "grad_norm": 2.187981367111206,
      "learning_rate": 2.4243725610354684e-05,
      "loss": 1.7714,
      "step": 91350
    },
    {
      "epoch": 1.3452452791310363,
      "grad_norm": 2.1378061771392822,
      "learning_rate": 2.419420304158498e-05,
      "loss": 1.8146,
      "step": 91400
    },
    {
      "epoch": 1.3459811901152436,
      "grad_norm": 2.004389762878418,
      "learning_rate": 2.414471495601456e-05,
      "loss": 1.7909,
      "step": 91450
    },
    {
      "epoch": 1.346717101099451,
      "grad_norm": 2.5433454513549805,
      "learning_rate": 2.409526141977228e-05,
      "loss": 1.8395,
      "step": 91500
    },
    {
      "epoch": 1.3474530120836583,
      "grad_norm": 2.0920424461364746,
      "learning_rate": 2.404584249894081e-05,
      "loss": 1.8316,
      "step": 91550
    },
    {
      "epoch": 1.3481889230678656,
      "grad_norm": 2.318195343017578,
      "learning_rate": 2.3996458259556575e-05,
      "loss": 1.7779,
      "step": 91600
    },
    {
      "epoch": 1.348924834052073,
      "grad_norm": 2.172034740447998,
      "learning_rate": 2.3947108767609654e-05,
      "loss": 1.7561,
      "step": 91650
    },
    {
      "epoch": 1.3496607450362803,
      "grad_norm": 2.0065667629241943,
      "learning_rate": 2.3897794089043692e-05,
      "loss": 1.8103,
      "step": 91700
    },
    {
      "epoch": 1.3503966560204876,
      "grad_norm": 2.423085927963257,
      "learning_rate": 2.3848514289755814e-05,
      "loss": 1.8357,
      "step": 91750
    },
    {
      "epoch": 1.3511325670046952,
      "grad_norm": 2.296919822692871,
      "learning_rate": 2.379926943559654e-05,
      "loss": 1.7647,
      "step": 91800
    },
    {
      "epoch": 1.3518684779889025,
      "grad_norm": 1.7385238409042358,
      "learning_rate": 2.37500595923697e-05,
      "loss": 1.8912,
      "step": 91850
    },
    {
      "epoch": 1.3526043889731099,
      "grad_norm": 2.628951072692871,
      "learning_rate": 2.370088482583233e-05,
      "loss": 1.782,
      "step": 91900
    },
    {
      "epoch": 1.3533402999573172,
      "grad_norm": 2.2829301357269287,
      "learning_rate": 2.3651745201694597e-05,
      "loss": 1.8352,
      "step": 91950
    },
    {
      "epoch": 1.3540762109415245,
      "grad_norm": 2.1569039821624756,
      "learning_rate": 2.3602640785619717e-05,
      "loss": 1.81,
      "step": 92000
    },
    {
      "epoch": 1.3548121219257319,
      "grad_norm": 2.4324870109558105,
      "learning_rate": 2.3553571643223842e-05,
      "loss": 1.7454,
      "step": 92050
    },
    {
      "epoch": 1.3555480329099392,
      "grad_norm": 2.229504108428955,
      "learning_rate": 2.3504537840076014e-05,
      "loss": 1.85,
      "step": 92100
    },
    {
      "epoch": 1.3562839438941465,
      "grad_norm": 2.2182579040527344,
      "learning_rate": 2.345553944169803e-05,
      "loss": 1.8204,
      "step": 92150
    },
    {
      "epoch": 1.3570198548783539,
      "grad_norm": 2.5837581157684326,
      "learning_rate": 2.3406576513564375e-05,
      "loss": 1.8366,
      "step": 92200
    },
    {
      "epoch": 1.3577557658625612,
      "grad_norm": 1.9171907901763916,
      "learning_rate": 2.3357649121102166e-05,
      "loss": 1.7805,
      "step": 92250
    },
    {
      "epoch": 1.3584916768467687,
      "grad_norm": 2.2662856578826904,
      "learning_rate": 2.3308757329691005e-05,
      "loss": 1.7715,
      "step": 92300
    },
    {
      "epoch": 1.359227587830976,
      "grad_norm": 2.3402490615844727,
      "learning_rate": 2.3259901204662937e-05,
      "loss": 1.7803,
      "step": 92350
    },
    {
      "epoch": 1.3599634988151834,
      "grad_norm": 2.321744680404663,
      "learning_rate": 2.3211080811302327e-05,
      "loss": 1.8235,
      "step": 92400
    },
    {
      "epoch": 1.3606994097993907,
      "grad_norm": 2.170732021331787,
      "learning_rate": 2.316229621484584e-05,
      "loss": 1.7732,
      "step": 92450
    },
    {
      "epoch": 1.361435320783598,
      "grad_norm": 2.017000436782837,
      "learning_rate": 2.311354748048229e-05,
      "loss": 1.7558,
      "step": 92500
    },
    {
      "epoch": 1.3621712317678054,
      "grad_norm": 2.3292083740234375,
      "learning_rate": 2.3064834673352503e-05,
      "loss": 1.742,
      "step": 92550
    },
    {
      "epoch": 1.3629071427520127,
      "grad_norm": 2.1356680393218994,
      "learning_rate": 2.301615785854938e-05,
      "loss": 1.7326,
      "step": 92600
    },
    {
      "epoch": 1.36364305373622,
      "grad_norm": 2.330472230911255,
      "learning_rate": 2.29675171011177e-05,
      "loss": 1.7399,
      "step": 92650
    },
    {
      "epoch": 1.3643789647204274,
      "grad_norm": 2.326390266418457,
      "learning_rate": 2.2918912466054043e-05,
      "loss": 1.7826,
      "step": 92700
    },
    {
      "epoch": 1.3651148757046347,
      "grad_norm": 1.958366870880127,
      "learning_rate": 2.287034401830675e-05,
      "loss": 1.8106,
      "step": 92750
    },
    {
      "epoch": 1.365850786688842,
      "grad_norm": 2.0264086723327637,
      "learning_rate": 2.282181182277579e-05,
      "loss": 1.7806,
      "step": 92800
    },
    {
      "epoch": 1.3665866976730494,
      "grad_norm": 2.028512477874756,
      "learning_rate": 2.2773315944312684e-05,
      "loss": 1.8255,
      "step": 92850
    },
    {
      "epoch": 1.3673226086572567,
      "grad_norm": 2.0987095832824707,
      "learning_rate": 2.272485644772044e-05,
      "loss": 1.8103,
      "step": 92900
    },
    {
      "epoch": 1.368058519641464,
      "grad_norm": 2.4001622200012207,
      "learning_rate": 2.2676433397753437e-05,
      "loss": 1.8367,
      "step": 92950
    },
    {
      "epoch": 1.3687944306256714,
      "grad_norm": 1.9360772371292114,
      "learning_rate": 2.2628046859117353e-05,
      "loss": 1.8423,
      "step": 93000
    },
    {
      "epoch": 1.3695303416098787,
      "grad_norm": 2.4427506923675537,
      "learning_rate": 2.2579696896469072e-05,
      "loss": 1.8091,
      "step": 93050
    },
    {
      "epoch": 1.3702662525940863,
      "grad_norm": 1.8444209098815918,
      "learning_rate": 2.2531383574416636e-05,
      "loss": 1.8086,
      "step": 93100
    },
    {
      "epoch": 1.3710021635782936,
      "grad_norm": 2.1377198696136475,
      "learning_rate": 2.24831069575191e-05,
      "loss": 1.8491,
      "step": 93150
    },
    {
      "epoch": 1.371738074562501,
      "grad_norm": 1.9615428447723389,
      "learning_rate": 2.2434867110286462e-05,
      "loss": 1.7697,
      "step": 93200
    },
    {
      "epoch": 1.3724739855467083,
      "grad_norm": 2.3187038898468018,
      "learning_rate": 2.2386664097179594e-05,
      "loss": 1.7872,
      "step": 93250
    },
    {
      "epoch": 1.3732098965309156,
      "grad_norm": 2.5319101810455322,
      "learning_rate": 2.2338497982610164e-05,
      "loss": 1.8132,
      "step": 93300
    },
    {
      "epoch": 1.373945807515123,
      "grad_norm": 2.354595422744751,
      "learning_rate": 2.2290368830940512e-05,
      "loss": 1.7854,
      "step": 93350
    },
    {
      "epoch": 1.3746817184993303,
      "grad_norm": 1.9310060739517212,
      "learning_rate": 2.22422767064836e-05,
      "loss": 1.858,
      "step": 93400
    },
    {
      "epoch": 1.3754176294835376,
      "grad_norm": 2.0042614936828613,
      "learning_rate": 2.2194221673502906e-05,
      "loss": 1.8016,
      "step": 93450
    },
    {
      "epoch": 1.376153540467745,
      "grad_norm": 2.229498863220215,
      "learning_rate": 2.2146203796212366e-05,
      "loss": 1.8191,
      "step": 93500
    },
    {
      "epoch": 1.3768894514519523,
      "grad_norm": 2.0670106410980225,
      "learning_rate": 2.209822313877621e-05,
      "loss": 1.8087,
      "step": 93550
    },
    {
      "epoch": 1.3776253624361599,
      "grad_norm": 2.3163442611694336,
      "learning_rate": 2.2050279765308983e-05,
      "loss": 1.8145,
      "step": 93600
    },
    {
      "epoch": 1.3783612734203672,
      "grad_norm": 2.2337005138397217,
      "learning_rate": 2.2002373739875403e-05,
      "loss": 1.7987,
      "step": 93650
    },
    {
      "epoch": 1.3790971844045745,
      "grad_norm": 2.19472599029541,
      "learning_rate": 2.195450512649025e-05,
      "loss": 1.7484,
      "step": 93700
    },
    {
      "epoch": 1.3798330953887818,
      "grad_norm": 2.23341703414917,
      "learning_rate": 2.190667398911837e-05,
      "loss": 1.7685,
      "step": 93750
    },
    {
      "epoch": 1.3805690063729892,
      "grad_norm": 2.64953351020813,
      "learning_rate": 2.185888039167448e-05,
      "loss": 1.8072,
      "step": 93800
    },
    {
      "epoch": 1.3813049173571965,
      "grad_norm": 2.263638496398926,
      "learning_rate": 2.181112439802315e-05,
      "loss": 1.8123,
      "step": 93850
    },
    {
      "epoch": 1.3820408283414038,
      "grad_norm": 1.736199975013733,
      "learning_rate": 2.1763406071978714e-05,
      "loss": 1.7968,
      "step": 93900
    },
    {
      "epoch": 1.3827767393256112,
      "grad_norm": 2.2237625122070312,
      "learning_rate": 2.1715725477305147e-05,
      "loss": 1.728,
      "step": 93950
    },
    {
      "epoch": 1.3835126503098185,
      "grad_norm": 2.1244313716888428,
      "learning_rate": 2.1668082677716022e-05,
      "loss": 1.8726,
      "step": 94000
    },
    {
      "epoch": 1.3842485612940258,
      "grad_norm": 2.2676901817321777,
      "learning_rate": 2.1620477736874406e-05,
      "loss": 1.8214,
      "step": 94050
    },
    {
      "epoch": 1.3849844722782332,
      "grad_norm": 2.5156569480895996,
      "learning_rate": 2.1572910718392775e-05,
      "loss": 1.7968,
      "step": 94100
    },
    {
      "epoch": 1.3857203832624405,
      "grad_norm": 2.397150754928589,
      "learning_rate": 2.1525381685832935e-05,
      "loss": 1.798,
      "step": 94150
    },
    {
      "epoch": 1.3864562942466478,
      "grad_norm": 2.23211932182312,
      "learning_rate": 2.147789070270592e-05,
      "loss": 1.8219,
      "step": 94200
    },
    {
      "epoch": 1.3871922052308552,
      "grad_norm": 2.195996046066284,
      "learning_rate": 2.1430437832471945e-05,
      "loss": 1.7317,
      "step": 94250
    },
    {
      "epoch": 1.3879281162150625,
      "grad_norm": 2.3560640811920166,
      "learning_rate": 2.138302313854027e-05,
      "loss": 1.8648,
      "step": 94300
    },
    {
      "epoch": 1.3886640271992698,
      "grad_norm": 1.9460387229919434,
      "learning_rate": 2.133564668426916e-05,
      "loss": 1.8306,
      "step": 94350
    },
    {
      "epoch": 1.3893999381834774,
      "grad_norm": 2.1254749298095703,
      "learning_rate": 2.1288308532965774e-05,
      "loss": 1.8196,
      "step": 94400
    },
    {
      "epoch": 1.3901358491676847,
      "grad_norm": 2.166682720184326,
      "learning_rate": 2.124100874788608e-05,
      "loss": 1.8012,
      "step": 94450
    },
    {
      "epoch": 1.390871760151892,
      "grad_norm": 1.983232021331787,
      "learning_rate": 2.1193747392234797e-05,
      "loss": 1.7417,
      "step": 94500
    },
    {
      "epoch": 1.3916076711360994,
      "grad_norm": 2.167600393295288,
      "learning_rate": 2.1146524529165278e-05,
      "loss": 1.8432,
      "step": 94550
    },
    {
      "epoch": 1.3923435821203067,
      "grad_norm": 2.141186475753784,
      "learning_rate": 2.1099340221779456e-05,
      "loss": 1.7841,
      "step": 94600
    },
    {
      "epoch": 1.393079493104514,
      "grad_norm": 2.136176109313965,
      "learning_rate": 2.1052194533127723e-05,
      "loss": 1.7986,
      "step": 94650
    },
    {
      "epoch": 1.3938154040887214,
      "grad_norm": 2.6401774883270264,
      "learning_rate": 2.1005087526208882e-05,
      "loss": 1.8243,
      "step": 94700
    },
    {
      "epoch": 1.3945513150729287,
      "grad_norm": 2.3034608364105225,
      "learning_rate": 2.0958019263970036e-05,
      "loss": 1.8289,
      "step": 94750
    },
    {
      "epoch": 1.395287226057136,
      "grad_norm": 2.030827522277832,
      "learning_rate": 2.0910989809306523e-05,
      "loss": 1.7899,
      "step": 94800
    },
    {
      "epoch": 1.3960231370413434,
      "grad_norm": 2.912626028060913,
      "learning_rate": 2.0863999225061827e-05,
      "loss": 1.8324,
      "step": 94850
    },
    {
      "epoch": 1.396759048025551,
      "grad_norm": 2.328474998474121,
      "learning_rate": 2.0817047574027475e-05,
      "loss": 1.7775,
      "step": 94900
    },
    {
      "epoch": 1.3974949590097583,
      "grad_norm": 2.6546475887298584,
      "learning_rate": 2.077013491894298e-05,
      "loss": 1.8069,
      "step": 94950
    },
    {
      "epoch": 1.3982308699939656,
      "grad_norm": 2.189257860183716,
      "learning_rate": 2.0723261322495762e-05,
      "loss": 1.8265,
      "step": 95000
    },
    {
      "epoch": 1.398966780978173,
      "grad_norm": 2.231517791748047,
      "learning_rate": 2.067642684732103e-05,
      "loss": 1.772,
      "step": 95050
    },
    {
      "epoch": 1.3997026919623803,
      "grad_norm": 2.4780588150024414,
      "learning_rate": 2.0629631556001715e-05,
      "loss": 1.8038,
      "step": 95100
    },
    {
      "epoch": 1.4004386029465876,
      "grad_norm": 1.8437472581863403,
      "learning_rate": 2.0582875511068395e-05,
      "loss": 1.8604,
      "step": 95150
    },
    {
      "epoch": 1.401174513930795,
      "grad_norm": 2.466266632080078,
      "learning_rate": 2.0536158774999216e-05,
      "loss": 1.7913,
      "step": 95200
    },
    {
      "epoch": 1.4019104249150023,
      "grad_norm": 2.0778067111968994,
      "learning_rate": 2.0489481410219753e-05,
      "loss": 1.8156,
      "step": 95250
    },
    {
      "epoch": 1.4026463358992096,
      "grad_norm": 2.230003833770752,
      "learning_rate": 2.044284347910302e-05,
      "loss": 1.7753,
      "step": 95300
    },
    {
      "epoch": 1.403382246883417,
      "grad_norm": 2.060133218765259,
      "learning_rate": 2.0396245043969314e-05,
      "loss": 1.8315,
      "step": 95350
    },
    {
      "epoch": 1.4041181578676243,
      "grad_norm": 1.8811296224594116,
      "learning_rate": 2.0349686167086167e-05,
      "loss": 1.8204,
      "step": 95400
    },
    {
      "epoch": 1.4048540688518316,
      "grad_norm": 2.070864200592041,
      "learning_rate": 2.030316691066825e-05,
      "loss": 1.8035,
      "step": 95450
    },
    {
      "epoch": 1.405589979836039,
      "grad_norm": 2.045896530151367,
      "learning_rate": 2.0256687336877268e-05,
      "loss": 1.8355,
      "step": 95500
    },
    {
      "epoch": 1.4063258908202463,
      "grad_norm": 2.425022602081299,
      "learning_rate": 2.0210247507821934e-05,
      "loss": 1.7613,
      "step": 95550
    },
    {
      "epoch": 1.4070618018044536,
      "grad_norm": 2.7663474082946777,
      "learning_rate": 2.016384748555781e-05,
      "loss": 1.816,
      "step": 95600
    },
    {
      "epoch": 1.407797712788661,
      "grad_norm": 2.348053216934204,
      "learning_rate": 2.011748733208732e-05,
      "loss": 1.8325,
      "step": 95650
    },
    {
      "epoch": 1.4085336237728685,
      "grad_norm": 2.300387382507324,
      "learning_rate": 2.007116710935958e-05,
      "loss": 1.8018,
      "step": 95700
    },
    {
      "epoch": 1.4092695347570758,
      "grad_norm": 2.109635829925537,
      "learning_rate": 2.0024886879270344e-05,
      "loss": 1.8538,
      "step": 95750
    },
    {
      "epoch": 1.4100054457412832,
      "grad_norm": 2.1229982376098633,
      "learning_rate": 1.997864670366194e-05,
      "loss": 1.8297,
      "step": 95800
    },
    {
      "epoch": 1.4107413567254905,
      "grad_norm": 2.551926374435425,
      "learning_rate": 1.9932446644323155e-05,
      "loss": 1.7899,
      "step": 95850
    },
    {
      "epoch": 1.4114772677096978,
      "grad_norm": 2.309434652328491,
      "learning_rate": 1.988628676298919e-05,
      "loss": 1.7797,
      "step": 95900
    },
    {
      "epoch": 1.4122131786939052,
      "grad_norm": 2.235576629638672,
      "learning_rate": 1.9840167121341556e-05,
      "loss": 1.7926,
      "step": 95950
    },
    {
      "epoch": 1.4129490896781125,
      "grad_norm": 2.156827211380005,
      "learning_rate": 1.9794087781007974e-05,
      "loss": 1.7409,
      "step": 96000
    },
    {
      "epoch": 1.4136850006623198,
      "grad_norm": 2.200122594833374,
      "learning_rate": 1.974804880356233e-05,
      "loss": 1.7707,
      "step": 96050
    },
    {
      "epoch": 1.4144209116465272,
      "grad_norm": 2.4771909713745117,
      "learning_rate": 1.9702050250524572e-05,
      "loss": 1.7535,
      "step": 96100
    },
    {
      "epoch": 1.4151568226307347,
      "grad_norm": 2.3429861068725586,
      "learning_rate": 1.9656092183360632e-05,
      "loss": 1.7974,
      "step": 96150
    },
    {
      "epoch": 1.415892733614942,
      "grad_norm": 2.433178424835205,
      "learning_rate": 1.9610174663482344e-05,
      "loss": 1.7919,
      "step": 96200
    },
    {
      "epoch": 1.4166286445991494,
      "grad_norm": 2.09958553314209,
      "learning_rate": 1.9564297752247318e-05,
      "loss": 1.7927,
      "step": 96250
    },
    {
      "epoch": 1.4173645555833567,
      "grad_norm": 2.200502634048462,
      "learning_rate": 1.951846151095898e-05,
      "loss": 1.7319,
      "step": 96300
    },
    {
      "epoch": 1.418100466567564,
      "grad_norm": 2.2896804809570312,
      "learning_rate": 1.9472666000866353e-05,
      "loss": 1.825,
      "step": 96350
    },
    {
      "epoch": 1.4188363775517714,
      "grad_norm": 2.2096498012542725,
      "learning_rate": 1.9426911283164045e-05,
      "loss": 1.8157,
      "step": 96400
    },
    {
      "epoch": 1.4195722885359787,
      "grad_norm": 1.987776517868042,
      "learning_rate": 1.938119741899217e-05,
      "loss": 1.7844,
      "step": 96450
    },
    {
      "epoch": 1.420308199520186,
      "grad_norm": 2.5963945388793945,
      "learning_rate": 1.9335524469436224e-05,
      "loss": 1.7791,
      "step": 96500
    },
    {
      "epoch": 1.4210441105043934,
      "grad_norm": 1.7556045055389404,
      "learning_rate": 1.9289892495527052e-05,
      "loss": 1.7411,
      "step": 96550
    },
    {
      "epoch": 1.4217800214886007,
      "grad_norm": 2.353501319885254,
      "learning_rate": 1.9244301558240734e-05,
      "loss": 1.8228,
      "step": 96600
    },
    {
      "epoch": 1.422515932472808,
      "grad_norm": 2.3069188594818115,
      "learning_rate": 1.9198751718498526e-05,
      "loss": 1.8262,
      "step": 96650
    },
    {
      "epoch": 1.4232518434570154,
      "grad_norm": 1.9640178680419922,
      "learning_rate": 1.9153243037166758e-05,
      "loss": 1.7935,
      "step": 96700
    },
    {
      "epoch": 1.4239877544412227,
      "grad_norm": 2.1995410919189453,
      "learning_rate": 1.9107775575056758e-05,
      "loss": 1.7569,
      "step": 96750
    },
    {
      "epoch": 1.42472366542543,
      "grad_norm": 1.7508821487426758,
      "learning_rate": 1.9062349392924784e-05,
      "loss": 1.7953,
      "step": 96800
    },
    {
      "epoch": 1.4254595764096374,
      "grad_norm": 2.158066511154175,
      "learning_rate": 1.9016964551471926e-05,
      "loss": 1.776,
      "step": 96850
    },
    {
      "epoch": 1.4261954873938447,
      "grad_norm": 2.0219011306762695,
      "learning_rate": 1.8971621111344047e-05,
      "loss": 1.7778,
      "step": 96900
    },
    {
      "epoch": 1.4269313983780523,
      "grad_norm": 2.046584367752075,
      "learning_rate": 1.8926319133131666e-05,
      "loss": 1.8287,
      "step": 96950
    },
    {
      "epoch": 1.4276673093622596,
      "grad_norm": 2.0859057903289795,
      "learning_rate": 1.8881058677369906e-05,
      "loss": 1.7792,
      "step": 97000
    },
    {
      "epoch": 1.428403220346467,
      "grad_norm": 2.3530890941619873,
      "learning_rate": 1.8835839804538412e-05,
      "loss": 1.8605,
      "step": 97050
    },
    {
      "epoch": 1.4291391313306743,
      "grad_norm": 2.297758102416992,
      "learning_rate": 1.8790662575061258e-05,
      "loss": 1.8102,
      "step": 97100
    },
    {
      "epoch": 1.4298750423148816,
      "grad_norm": 2.265650987625122,
      "learning_rate": 1.8745527049306864e-05,
      "loss": 1.7737,
      "step": 97150
    },
    {
      "epoch": 1.430610953299089,
      "grad_norm": 2.517777681350708,
      "learning_rate": 1.8700433287587948e-05,
      "loss": 1.8524,
      "step": 97200
    },
    {
      "epoch": 1.4313468642832963,
      "grad_norm": 2.2144579887390137,
      "learning_rate": 1.86553813501614e-05,
      "loss": 1.7744,
      "step": 97250
    },
    {
      "epoch": 1.4320827752675036,
      "grad_norm": 2.2646946907043457,
      "learning_rate": 1.8610371297228213e-05,
      "loss": 1.7335,
      "step": 97300
    },
    {
      "epoch": 1.432818686251711,
      "grad_norm": 2.267272472381592,
      "learning_rate": 1.8565403188933435e-05,
      "loss": 1.8229,
      "step": 97350
    },
    {
      "epoch": 1.4335545972359183,
      "grad_norm": 2.389561176300049,
      "learning_rate": 1.852047708536605e-05,
      "loss": 1.7852,
      "step": 97400
    },
    {
      "epoch": 1.4342905082201258,
      "grad_norm": 1.7253276109695435,
      "learning_rate": 1.8475593046558925e-05,
      "loss": 1.846,
      "step": 97450
    },
    {
      "epoch": 1.4350264192043332,
      "grad_norm": 2.0086417198181152,
      "learning_rate": 1.8430751132488705e-05,
      "loss": 1.8438,
      "step": 97500
    },
    {
      "epoch": 1.4357623301885405,
      "grad_norm": 2.2658753395080566,
      "learning_rate": 1.8385951403075728e-05,
      "loss": 1.8259,
      "step": 97550
    },
    {
      "epoch": 1.4364982411727478,
      "grad_norm": 2.3149466514587402,
      "learning_rate": 1.834119391818403e-05,
      "loss": 1.7739,
      "step": 97600
    },
    {
      "epoch": 1.4372341521569552,
      "grad_norm": 2.198848009109497,
      "learning_rate": 1.8296478737621125e-05,
      "loss": 1.7961,
      "step": 97650
    },
    {
      "epoch": 1.4379700631411625,
      "grad_norm": 2.3283309936523438,
      "learning_rate": 1.8251805921138038e-05,
      "loss": 1.7417,
      "step": 97700
    },
    {
      "epoch": 1.4387059741253698,
      "grad_norm": 2.2176198959350586,
      "learning_rate": 1.820717552842916e-05,
      "loss": 1.7898,
      "step": 97750
    },
    {
      "epoch": 1.4394418851095772,
      "grad_norm": 2.3049917221069336,
      "learning_rate": 1.8162587619132203e-05,
      "loss": 1.7794,
      "step": 97800
    },
    {
      "epoch": 1.4401777960937845,
      "grad_norm": 2.112225294113159,
      "learning_rate": 1.8118042252828127e-05,
      "loss": 1.7944,
      "step": 97850
    },
    {
      "epoch": 1.4409137070779918,
      "grad_norm": 2.6817386150360107,
      "learning_rate": 1.8073539489041024e-05,
      "loss": 1.7935,
      "step": 97900
    },
    {
      "epoch": 1.4416496180621992,
      "grad_norm": 2.331240653991699,
      "learning_rate": 1.8029079387238045e-05,
      "loss": 1.8127,
      "step": 97950
    },
    {
      "epoch": 1.4423855290464065,
      "grad_norm": 2.393932342529297,
      "learning_rate": 1.7984662006829366e-05,
      "loss": 1.8083,
      "step": 98000
    },
    {
      "epoch": 1.4431214400306138,
      "grad_norm": 2.338496208190918,
      "learning_rate": 1.7940287407168056e-05,
      "loss": 1.8004,
      "step": 98050
    },
    {
      "epoch": 1.4438573510148212,
      "grad_norm": 2.1889662742614746,
      "learning_rate": 1.7895955647550023e-05,
      "loss": 1.8128,
      "step": 98100
    },
    {
      "epoch": 1.4445932619990285,
      "grad_norm": 2.1916565895080566,
      "learning_rate": 1.7851666787213933e-05,
      "loss": 1.8112,
      "step": 98150
    },
    {
      "epoch": 1.4453291729832358,
      "grad_norm": 2.4757919311523438,
      "learning_rate": 1.780742088534111e-05,
      "loss": 1.8549,
      "step": 98200
    },
    {
      "epoch": 1.4460650839674434,
      "grad_norm": 2.1412694454193115,
      "learning_rate": 1.776321800105551e-05,
      "loss": 1.7606,
      "step": 98250
    },
    {
      "epoch": 1.4468009949516507,
      "grad_norm": 2.469681978225708,
      "learning_rate": 1.771905819342358e-05,
      "loss": 1.7496,
      "step": 98300
    },
    {
      "epoch": 1.447536905935858,
      "grad_norm": 2.3900883197784424,
      "learning_rate": 1.767494152145421e-05,
      "loss": 1.7865,
      "step": 98350
    },
    {
      "epoch": 1.4482728169200654,
      "grad_norm": 2.3557729721069336,
      "learning_rate": 1.7630868044098643e-05,
      "loss": 1.8147,
      "step": 98400
    },
    {
      "epoch": 1.4490087279042727,
      "grad_norm": 2.2752630710601807,
      "learning_rate": 1.7586837820250413e-05,
      "loss": 1.8041,
      "step": 98450
    },
    {
      "epoch": 1.44974463888848,
      "grad_norm": 2.2107458114624023,
      "learning_rate": 1.7542850908745255e-05,
      "loss": 1.8091,
      "step": 98500
    },
    {
      "epoch": 1.4504805498726874,
      "grad_norm": 2.287432909011841,
      "learning_rate": 1.7498907368361027e-05,
      "loss": 1.765,
      "step": 98550
    },
    {
      "epoch": 1.4512164608568947,
      "grad_norm": 2.0269436836242676,
      "learning_rate": 1.745500725781763e-05,
      "loss": 1.747,
      "step": 98600
    },
    {
      "epoch": 1.451952371841102,
      "grad_norm": 2.0263314247131348,
      "learning_rate": 1.741115063577693e-05,
      "loss": 1.7889,
      "step": 98650
    },
    {
      "epoch": 1.4526882828253094,
      "grad_norm": 2.4972481727600098,
      "learning_rate": 1.7367337560842683e-05,
      "loss": 1.8093,
      "step": 98700
    },
    {
      "epoch": 1.453424193809517,
      "grad_norm": 2.108743190765381,
      "learning_rate": 1.7323568091560454e-05,
      "loss": 1.8414,
      "step": 98750
    },
    {
      "epoch": 1.4541601047937243,
      "grad_norm": 2.0530343055725098,
      "learning_rate": 1.7279842286417545e-05,
      "loss": 1.767,
      "step": 98800
    },
    {
      "epoch": 1.4548960157779316,
      "grad_norm": 2.3002021312713623,
      "learning_rate": 1.723616020384291e-05,
      "loss": 1.755,
      "step": 98850
    },
    {
      "epoch": 1.455631926762139,
      "grad_norm": 2.0464463233947754,
      "learning_rate": 1.719252190220707e-05,
      "loss": 1.7176,
      "step": 98900
    },
    {
      "epoch": 1.4563678377463463,
      "grad_norm": 2.1468729972839355,
      "learning_rate": 1.714892743982205e-05,
      "loss": 1.8744,
      "step": 98950
    },
    {
      "epoch": 1.4571037487305536,
      "grad_norm": 2.1035330295562744,
      "learning_rate": 1.7105376874941285e-05,
      "loss": 1.7907,
      "step": 99000
    },
    {
      "epoch": 1.457839659714761,
      "grad_norm": 2.138674020767212,
      "learning_rate": 1.7061870265759577e-05,
      "loss": 1.8559,
      "step": 99050
    },
    {
      "epoch": 1.4585755706989683,
      "grad_norm": 2.4299259185791016,
      "learning_rate": 1.7018407670412966e-05,
      "loss": 1.8618,
      "step": 99100
    },
    {
      "epoch": 1.4593114816831756,
      "grad_norm": 2.384138822555542,
      "learning_rate": 1.6974989146978688e-05,
      "loss": 1.7864,
      "step": 99150
    },
    {
      "epoch": 1.460047392667383,
      "grad_norm": 2.3872063159942627,
      "learning_rate": 1.6931614753475082e-05,
      "loss": 1.7641,
      "step": 99200
    },
    {
      "epoch": 1.4607833036515903,
      "grad_norm": 2.2502012252807617,
      "learning_rate": 1.6888284547861532e-05,
      "loss": 1.7954,
      "step": 99250
    },
    {
      "epoch": 1.4615192146357976,
      "grad_norm": 2.2486016750335693,
      "learning_rate": 1.6844998588038356e-05,
      "loss": 1.7353,
      "step": 99300
    },
    {
      "epoch": 1.462255125620005,
      "grad_norm": 1.9098271131515503,
      "learning_rate": 1.6801756931846775e-05,
      "loss": 1.8461,
      "step": 99350
    },
    {
      "epoch": 1.4629910366042123,
      "grad_norm": 2.278320550918579,
      "learning_rate": 1.675855963706877e-05,
      "loss": 1.8143,
      "step": 99400
    },
    {
      "epoch": 1.4637269475884196,
      "grad_norm": 2.2769646644592285,
      "learning_rate": 1.6715406761427067e-05,
      "loss": 1.761,
      "step": 99450
    },
    {
      "epoch": 1.464462858572627,
      "grad_norm": 3.126065254211426,
      "learning_rate": 1.6672298362585058e-05,
      "loss": 1.7579,
      "step": 99500
    },
    {
      "epoch": 1.4651987695568345,
      "grad_norm": 2.150905132293701,
      "learning_rate": 1.6629234498146673e-05,
      "loss": 1.7958,
      "step": 99550
    },
    {
      "epoch": 1.4659346805410418,
      "grad_norm": 2.01230525970459,
      "learning_rate": 1.6586215225656348e-05,
      "loss": 1.797,
      "step": 99600
    },
    {
      "epoch": 1.4666705915252491,
      "grad_norm": 2.1841673851013184,
      "learning_rate": 1.6543240602598897e-05,
      "loss": 1.7999,
      "step": 99650
    },
    {
      "epoch": 1.4674065025094565,
      "grad_norm": 1.6532238721847534,
      "learning_rate": 1.6500310686399518e-05,
      "loss": 1.7657,
      "step": 99700
    },
    {
      "epoch": 1.4681424134936638,
      "grad_norm": 2.3701486587524414,
      "learning_rate": 1.6457425534423638e-05,
      "loss": 1.8119,
      "step": 99750
    },
    {
      "epoch": 1.4688783244778711,
      "grad_norm": 2.302097797393799,
      "learning_rate": 1.6414585203976883e-05,
      "loss": 1.828,
      "step": 99800
    },
    {
      "epoch": 1.4696142354620785,
      "grad_norm": 2.113724708557129,
      "learning_rate": 1.637178975230499e-05,
      "loss": 1.6963,
      "step": 99850
    },
    {
      "epoch": 1.4703501464462858,
      "grad_norm": 2.4342658519744873,
      "learning_rate": 1.6329039236593713e-05,
      "loss": 1.7686,
      "step": 99900
    },
    {
      "epoch": 1.4710860574304931,
      "grad_norm": 2.3512351512908936,
      "learning_rate": 1.628633371396876e-05,
      "loss": 1.7897,
      "step": 99950
    },
    {
      "epoch": 1.4718219684147005,
      "grad_norm": 2.4333691596984863,
      "learning_rate": 1.624367324149573e-05,
      "loss": 1.7586,
      "step": 100000
    },
    {
      "epoch": 1.472557879398908,
      "grad_norm": 2.2938828468322754,
      "learning_rate": 1.6201057876180004e-05,
      "loss": 1.7349,
      "step": 100050
    },
    {
      "epoch": 1.4732937903831154,
      "grad_norm": 2.034269094467163,
      "learning_rate": 1.6158487674966694e-05,
      "loss": 1.7553,
      "step": 100100
    },
    {
      "epoch": 1.4740297013673227,
      "grad_norm": 2.013530969619751,
      "learning_rate": 1.6115962694740593e-05,
      "loss": 1.8066,
      "step": 100150
    },
    {
      "epoch": 1.47476561235153,
      "grad_norm": 2.07877516746521,
      "learning_rate": 1.607348299232603e-05,
      "loss": 1.7383,
      "step": 100200
    },
    {
      "epoch": 1.4755015233357374,
      "grad_norm": 2.061037540435791,
      "learning_rate": 1.6031048624486832e-05,
      "loss": 1.7536,
      "step": 100250
    },
    {
      "epoch": 1.4762374343199447,
      "grad_norm": 2.248782157897949,
      "learning_rate": 1.598865964792627e-05,
      "loss": 1.7829,
      "step": 100300
    },
    {
      "epoch": 1.476973345304152,
      "grad_norm": 1.9982764720916748,
      "learning_rate": 1.5946316119286936e-05,
      "loss": 1.7732,
      "step": 100350
    },
    {
      "epoch": 1.4777092562883594,
      "grad_norm": 2.3470661640167236,
      "learning_rate": 1.5904018095150712e-05,
      "loss": 1.8067,
      "step": 100400
    },
    {
      "epoch": 1.4784451672725667,
      "grad_norm": 2.049436569213867,
      "learning_rate": 1.586176563203866e-05,
      "loss": 1.7876,
      "step": 100450
    },
    {
      "epoch": 1.479181078256774,
      "grad_norm": 2.1840076446533203,
      "learning_rate": 1.5819558786410964e-05,
      "loss": 1.8207,
      "step": 100500
    },
    {
      "epoch": 1.4799169892409814,
      "grad_norm": 2.6106069087982178,
      "learning_rate": 1.577739761466686e-05,
      "loss": 1.7673,
      "step": 100550
    },
    {
      "epoch": 1.4806529002251887,
      "grad_norm": 2.596708297729492,
      "learning_rate": 1.5735282173144562e-05,
      "loss": 1.8466,
      "step": 100600
    },
    {
      "epoch": 1.481388811209396,
      "grad_norm": 2.2915329933166504,
      "learning_rate": 1.5693212518121126e-05,
      "loss": 1.8358,
      "step": 100650
    },
    {
      "epoch": 1.4821247221936034,
      "grad_norm": 2.3829939365386963,
      "learning_rate": 1.565118870581247e-05,
      "loss": 1.813,
      "step": 100700
    },
    {
      "epoch": 1.4828606331778107,
      "grad_norm": 2.4844698905944824,
      "learning_rate": 1.5609210792373237e-05,
      "loss": 1.7973,
      "step": 100750
    },
    {
      "epoch": 1.483596544162018,
      "grad_norm": 1.798634648323059,
      "learning_rate": 1.556727883389677e-05,
      "loss": 1.7412,
      "step": 100800
    },
    {
      "epoch": 1.4843324551462256,
      "grad_norm": 2.4001338481903076,
      "learning_rate": 1.5525392886414962e-05,
      "loss": 1.7655,
      "step": 100850
    },
    {
      "epoch": 1.485068366130433,
      "grad_norm": 2.104074239730835,
      "learning_rate": 1.5483553005898243e-05,
      "loss": 1.727,
      "step": 100900
    },
    {
      "epoch": 1.4858042771146402,
      "grad_norm": 2.3280367851257324,
      "learning_rate": 1.5441759248255487e-05,
      "loss": 1.7968,
      "step": 100950
    },
    {
      "epoch": 1.4865401880988476,
      "grad_norm": 1.954351782798767,
      "learning_rate": 1.5400011669333924e-05,
      "loss": 1.7709,
      "step": 101000
    },
    {
      "epoch": 1.487276099083055,
      "grad_norm": 2.5677831172943115,
      "learning_rate": 1.535831032491909e-05,
      "loss": 1.7678,
      "step": 101050
    },
    {
      "epoch": 1.4880120100672622,
      "grad_norm": 2.3129305839538574,
      "learning_rate": 1.531665527073473e-05,
      "loss": 1.7686,
      "step": 101100
    },
    {
      "epoch": 1.4887479210514696,
      "grad_norm": 2.1254003047943115,
      "learning_rate": 1.5275046562442746e-05,
      "loss": 1.7843,
      "step": 101150
    },
    {
      "epoch": 1.489483832035677,
      "grad_norm": 3.502180337905884,
      "learning_rate": 1.5233484255643094e-05,
      "loss": 1.7761,
      "step": 101200
    },
    {
      "epoch": 1.4902197430198842,
      "grad_norm": 2.1573314666748047,
      "learning_rate": 1.519196840587373e-05,
      "loss": 1.8322,
      "step": 101250
    },
    {
      "epoch": 1.4909556540040918,
      "grad_norm": 1.9466677904129028,
      "learning_rate": 1.5150499068610547e-05,
      "loss": 1.8154,
      "step": 101300
    },
    {
      "epoch": 1.4916915649882991,
      "grad_norm": 1.9951765537261963,
      "learning_rate": 1.5109076299267266e-05,
      "loss": 1.7785,
      "step": 101350
    },
    {
      "epoch": 1.4924274759725065,
      "grad_norm": 1.8639063835144043,
      "learning_rate": 1.5067700153195392e-05,
      "loss": 1.8479,
      "step": 101400
    },
    {
      "epoch": 1.4931633869567138,
      "grad_norm": 2.863910436630249,
      "learning_rate": 1.5026370685684122e-05,
      "loss": 1.7687,
      "step": 101450
    },
    {
      "epoch": 1.4938992979409211,
      "grad_norm": 2.4932940006256104,
      "learning_rate": 1.4985087951960281e-05,
      "loss": 1.759,
      "step": 101500
    },
    {
      "epoch": 1.4946352089251285,
      "grad_norm": 2.1208062171936035,
      "learning_rate": 1.4943852007188253e-05,
      "loss": 1.8024,
      "step": 101550
    },
    {
      "epoch": 1.4953711199093358,
      "grad_norm": 2.0957870483398438,
      "learning_rate": 1.490266290646989e-05,
      "loss": 1.8201,
      "step": 101600
    },
    {
      "epoch": 1.4961070308935431,
      "grad_norm": 2.19972562789917,
      "learning_rate": 1.4861520704844457e-05,
      "loss": 1.803,
      "step": 101650
    },
    {
      "epoch": 1.4968429418777505,
      "grad_norm": 2.224130153656006,
      "learning_rate": 1.4820425457288534e-05,
      "loss": 1.8203,
      "step": 101700
    },
    {
      "epoch": 1.4975788528619578,
      "grad_norm": 2.0932934284210205,
      "learning_rate": 1.4779377218715979e-05,
      "loss": 1.8429,
      "step": 101750
    },
    {
      "epoch": 1.4983147638461651,
      "grad_norm": 2.1644318103790283,
      "learning_rate": 1.4738376043977814e-05,
      "loss": 1.8274,
      "step": 101800
    },
    {
      "epoch": 1.4990506748303725,
      "grad_norm": 2.1428868770599365,
      "learning_rate": 1.4697421987862192e-05,
      "loss": 1.7594,
      "step": 101850
    },
    {
      "epoch": 1.4997865858145798,
      "grad_norm": 2.1410536766052246,
      "learning_rate": 1.4656515105094294e-05,
      "loss": 1.8077,
      "step": 101900
    },
    {
      "epoch": 1.5005224967987871,
      "grad_norm": 2.3549935817718506,
      "learning_rate": 1.4615655450336252e-05,
      "loss": 1.8153,
      "step": 101950
    },
    {
      "epoch": 1.5012584077829945,
      "grad_norm": 2.522749185562134,
      "learning_rate": 1.4574843078187107e-05,
      "loss": 1.7883,
      "step": 102000
    },
    {
      "epoch": 1.5019943187672018,
      "grad_norm": 2.1034610271453857,
      "learning_rate": 1.4534078043182702e-05,
      "loss": 1.8205,
      "step": 102050
    },
    {
      "epoch": 1.5027302297514091,
      "grad_norm": 2.310681104660034,
      "learning_rate": 1.4493360399795658e-05,
      "loss": 1.7127,
      "step": 102100
    },
    {
      "epoch": 1.5034661407356165,
      "grad_norm": 2.644136667251587,
      "learning_rate": 1.4452690202435242e-05,
      "loss": 1.8207,
      "step": 102150
    },
    {
      "epoch": 1.504202051719824,
      "grad_norm": 2.2006418704986572,
      "learning_rate": 1.4412067505447313e-05,
      "loss": 1.8172,
      "step": 102200
    },
    {
      "epoch": 1.5049379627040314,
      "grad_norm": 2.113210678100586,
      "learning_rate": 1.437149236311428e-05,
      "loss": 1.8605,
      "step": 102250
    },
    {
      "epoch": 1.5056738736882387,
      "grad_norm": 2.5259714126586914,
      "learning_rate": 1.4330964829655003e-05,
      "loss": 1.7948,
      "step": 102300
    },
    {
      "epoch": 1.506409784672446,
      "grad_norm": 2.1832480430603027,
      "learning_rate": 1.4290484959224693e-05,
      "loss": 1.7578,
      "step": 102350
    },
    {
      "epoch": 1.5071456956566534,
      "grad_norm": 2.4081289768218994,
      "learning_rate": 1.425005280591491e-05,
      "loss": 1.7914,
      "step": 102400
    },
    {
      "epoch": 1.5078816066408607,
      "grad_norm": 2.5253047943115234,
      "learning_rate": 1.420966842375343e-05,
      "loss": 1.7889,
      "step": 102450
    },
    {
      "epoch": 1.508617517625068,
      "grad_norm": 2.219339370727539,
      "learning_rate": 1.4169331866704217e-05,
      "loss": 1.8039,
      "step": 102500
    },
    {
      "epoch": 1.5093534286092756,
      "grad_norm": 2.09053373336792,
      "learning_rate": 1.4129043188667296e-05,
      "loss": 1.7209,
      "step": 102550
    },
    {
      "epoch": 1.510089339593483,
      "grad_norm": 2.4117133617401123,
      "learning_rate": 1.4088802443478738e-05,
      "loss": 1.782,
      "step": 102600
    },
    {
      "epoch": 1.5108252505776902,
      "grad_norm": 2.019484043121338,
      "learning_rate": 1.4048609684910557e-05,
      "loss": 1.8303,
      "step": 102650
    },
    {
      "epoch": 1.5115611615618976,
      "grad_norm": 1.9430197477340698,
      "learning_rate": 1.4008464966670626e-05,
      "loss": 1.8087,
      "step": 102700
    },
    {
      "epoch": 1.512297072546105,
      "grad_norm": 2.753406047821045,
      "learning_rate": 1.3968368342402666e-05,
      "loss": 1.7959,
      "step": 102750
    },
    {
      "epoch": 1.5130329835303122,
      "grad_norm": 2.2068023681640625,
      "learning_rate": 1.39283198656861e-05,
      "loss": 1.7585,
      "step": 102800
    },
    {
      "epoch": 1.5137688945145196,
      "grad_norm": 2.338949680328369,
      "learning_rate": 1.388831959003602e-05,
      "loss": 1.8019,
      "step": 102850
    },
    {
      "epoch": 1.514504805498727,
      "grad_norm": 2.117623805999756,
      "learning_rate": 1.3848367568903103e-05,
      "loss": 1.7705,
      "step": 102900
    },
    {
      "epoch": 1.5152407164829342,
      "grad_norm": 2.1188442707061768,
      "learning_rate": 1.3808463855673548e-05,
      "loss": 1.7758,
      "step": 102950
    },
    {
      "epoch": 1.5159766274671416,
      "grad_norm": 2.225586175918579,
      "learning_rate": 1.3768608503669017e-05,
      "loss": 1.835,
      "step": 103000
    },
    {
      "epoch": 1.516712538451349,
      "grad_norm": 2.38004469871521,
      "learning_rate": 1.3728801566146538e-05,
      "loss": 1.8078,
      "step": 103050
    },
    {
      "epoch": 1.5174484494355562,
      "grad_norm": 2.2212085723876953,
      "learning_rate": 1.3689043096298432e-05,
      "loss": 1.791,
      "step": 103100
    },
    {
      "epoch": 1.5181843604197636,
      "grad_norm": 2.1138622760772705,
      "learning_rate": 1.3649333147252275e-05,
      "loss": 1.7841,
      "step": 103150
    },
    {
      "epoch": 1.518920271403971,
      "grad_norm": 2.153122901916504,
      "learning_rate": 1.3609671772070803e-05,
      "loss": 1.8405,
      "step": 103200
    },
    {
      "epoch": 1.5196561823881782,
      "grad_norm": 1.8724421262741089,
      "learning_rate": 1.3570059023751836e-05,
      "loss": 1.7923,
      "step": 103250
    },
    {
      "epoch": 1.5203920933723856,
      "grad_norm": 2.336324691772461,
      "learning_rate": 1.3530494955228234e-05,
      "loss": 1.828,
      "step": 103300
    },
    {
      "epoch": 1.521128004356593,
      "grad_norm": 2.168790102005005,
      "learning_rate": 1.3490979619367755e-05,
      "loss": 1.8262,
      "step": 103350
    },
    {
      "epoch": 1.5218639153408002,
      "grad_norm": 2.152435541152954,
      "learning_rate": 1.3451513068973116e-05,
      "loss": 1.8211,
      "step": 103400
    },
    {
      "epoch": 1.5225998263250076,
      "grad_norm": 2.1580164432525635,
      "learning_rate": 1.3412095356781795e-05,
      "loss": 1.7531,
      "step": 103450
    },
    {
      "epoch": 1.5233357373092151,
      "grad_norm": 2.201268434524536,
      "learning_rate": 1.3372726535466024e-05,
      "loss": 1.7571,
      "step": 103500
    },
    {
      "epoch": 1.5240716482934225,
      "grad_norm": 1.9334681034088135,
      "learning_rate": 1.333340665763269e-05,
      "loss": 1.8115,
      "step": 103550
    },
    {
      "epoch": 1.5248075592776298,
      "grad_norm": 2.4451797008514404,
      "learning_rate": 1.3294135775823302e-05,
      "loss": 1.764,
      "step": 103600
    },
    {
      "epoch": 1.5255434702618371,
      "grad_norm": 2.4585084915161133,
      "learning_rate": 1.3254913942513869e-05,
      "loss": 1.7825,
      "step": 103650
    },
    {
      "epoch": 1.5262793812460445,
      "grad_norm": 2.212367296218872,
      "learning_rate": 1.321574121011489e-05,
      "loss": 1.7888,
      "step": 103700
    },
    {
      "epoch": 1.5270152922302518,
      "grad_norm": 1.9448680877685547,
      "learning_rate": 1.3176617630971227e-05,
      "loss": 1.7545,
      "step": 103750
    },
    {
      "epoch": 1.5277512032144593,
      "grad_norm": 2.366884469985962,
      "learning_rate": 1.3137543257362077e-05,
      "loss": 1.747,
      "step": 103800
    },
    {
      "epoch": 1.5284871141986667,
      "grad_norm": 2.312410831451416,
      "learning_rate": 1.3098518141500877e-05,
      "loss": 1.8166,
      "step": 103850
    },
    {
      "epoch": 1.529223025182874,
      "grad_norm": 2.21000075340271,
      "learning_rate": 1.305954233553524e-05,
      "loss": 1.7929,
      "step": 103900
    },
    {
      "epoch": 1.5299589361670813,
      "grad_norm": 2.3192851543426514,
      "learning_rate": 1.3020615891546883e-05,
      "loss": 1.786,
      "step": 103950
    },
    {
      "epoch": 1.5306948471512887,
      "grad_norm": 2.057295560836792,
      "learning_rate": 1.2981738861551617e-05,
      "loss": 1.7854,
      "step": 104000
    },
    {
      "epoch": 1.531430758135496,
      "grad_norm": 2.5552642345428467,
      "learning_rate": 1.2942911297499138e-05,
      "loss": 1.7944,
      "step": 104050
    },
    {
      "epoch": 1.5321666691197033,
      "grad_norm": 2.0106201171875,
      "learning_rate": 1.2904133251273099e-05,
      "loss": 1.7337,
      "step": 104100
    },
    {
      "epoch": 1.5329025801039107,
      "grad_norm": 1.6197917461395264,
      "learning_rate": 1.2865404774690965e-05,
      "loss": 1.8315,
      "step": 104150
    },
    {
      "epoch": 1.533638491088118,
      "grad_norm": 2.1305668354034424,
      "learning_rate": 1.2826725919503962e-05,
      "loss": 1.7963,
      "step": 104200
    },
    {
      "epoch": 1.5343744020723253,
      "grad_norm": 2.1476023197174072,
      "learning_rate": 1.2788096737397032e-05,
      "loss": 1.7679,
      "step": 104250
    },
    {
      "epoch": 1.5351103130565327,
      "grad_norm": 1.803229570388794,
      "learning_rate": 1.274951727998871e-05,
      "loss": 1.7617,
      "step": 104300
    },
    {
      "epoch": 1.53584622404074,
      "grad_norm": 2.1533315181732178,
      "learning_rate": 1.2710987598831103e-05,
      "loss": 1.8383,
      "step": 104350
    },
    {
      "epoch": 1.5365821350249473,
      "grad_norm": 2.2872707843780518,
      "learning_rate": 1.26725077454098e-05,
      "loss": 1.7559,
      "step": 104400
    },
    {
      "epoch": 1.5373180460091547,
      "grad_norm": 2.239163398742676,
      "learning_rate": 1.2634077771143815e-05,
      "loss": 1.8104,
      "step": 104450
    },
    {
      "epoch": 1.538053956993362,
      "grad_norm": 2.366239309310913,
      "learning_rate": 1.2595697727385492e-05,
      "loss": 1.8373,
      "step": 104500
    },
    {
      "epoch": 1.5387898679775693,
      "grad_norm": 2.472731351852417,
      "learning_rate": 1.2557367665420472e-05,
      "loss": 1.7591,
      "step": 104550
    },
    {
      "epoch": 1.5395257789617767,
      "grad_norm": 2.1121184825897217,
      "learning_rate": 1.2519087636467592e-05,
      "loss": 1.7621,
      "step": 104600
    },
    {
      "epoch": 1.540261689945984,
      "grad_norm": 2.4475343227386475,
      "learning_rate": 1.2480857691678865e-05,
      "loss": 1.705,
      "step": 104650
    },
    {
      "epoch": 1.5409976009301913,
      "grad_norm": 2.300971031188965,
      "learning_rate": 1.2442677882139341e-05,
      "loss": 1.8302,
      "step": 104700
    },
    {
      "epoch": 1.5417335119143987,
      "grad_norm": 2.2483925819396973,
      "learning_rate": 1.2404548258867093e-05,
      "loss": 1.7495,
      "step": 104750
    },
    {
      "epoch": 1.5424694228986062,
      "grad_norm": 2.223705291748047,
      "learning_rate": 1.236646887281313e-05,
      "loss": 1.7698,
      "step": 104800
    },
    {
      "epoch": 1.5432053338828136,
      "grad_norm": 2.3495090007781982,
      "learning_rate": 1.2328439774861333e-05,
      "loss": 1.81,
      "step": 104850
    },
    {
      "epoch": 1.543941244867021,
      "grad_norm": 2.3382201194763184,
      "learning_rate": 1.2290461015828381e-05,
      "loss": 1.8128,
      "step": 104900
    },
    {
      "epoch": 1.5446771558512282,
      "grad_norm": 2.3126144409179688,
      "learning_rate": 1.2252532646463693e-05,
      "loss": 1.7075,
      "step": 104950
    },
    {
      "epoch": 1.5454130668354356,
      "grad_norm": 1.960781455039978,
      "learning_rate": 1.2214654717449359e-05,
      "loss": 1.7853,
      "step": 105000
    },
    {
      "epoch": 1.546148977819643,
      "grad_norm": 2.1677684783935547,
      "learning_rate": 1.2176827279400032e-05,
      "loss": 1.7381,
      "step": 105050
    },
    {
      "epoch": 1.5468848888038504,
      "grad_norm": 2.1644065380096436,
      "learning_rate": 1.2139050382862948e-05,
      "loss": 1.8316,
      "step": 105100
    },
    {
      "epoch": 1.5476207997880578,
      "grad_norm": 2.21366024017334,
      "learning_rate": 1.2101324078317772e-05,
      "loss": 1.7375,
      "step": 105150
    },
    {
      "epoch": 1.548356710772265,
      "grad_norm": 2.402785539627075,
      "learning_rate": 1.2063648416176581e-05,
      "loss": 1.7533,
      "step": 105200
    },
    {
      "epoch": 1.5490926217564724,
      "grad_norm": 2.261585235595703,
      "learning_rate": 1.2026023446783758e-05,
      "loss": 1.7914,
      "step": 105250
    },
    {
      "epoch": 1.5498285327406798,
      "grad_norm": 2.2309274673461914,
      "learning_rate": 1.1988449220415992e-05,
      "loss": 1.8072,
      "step": 105300
    },
    {
      "epoch": 1.550564443724887,
      "grad_norm": 2.251885414123535,
      "learning_rate": 1.1950925787282124e-05,
      "loss": 1.8313,
      "step": 105350
    },
    {
      "epoch": 1.5513003547090944,
      "grad_norm": 2.294393539428711,
      "learning_rate": 1.1913453197523138e-05,
      "loss": 1.8057,
      "step": 105400
    },
    {
      "epoch": 1.5520362656933018,
      "grad_norm": 2.239673376083374,
      "learning_rate": 1.1876031501212081e-05,
      "loss": 1.7845,
      "step": 105450
    },
    {
      "epoch": 1.552772176677509,
      "grad_norm": 2.3175461292266846,
      "learning_rate": 1.1838660748353986e-05,
      "loss": 1.7741,
      "step": 105500
    },
    {
      "epoch": 1.5535080876617164,
      "grad_norm": 2.6987061500549316,
      "learning_rate": 1.1801340988885817e-05,
      "loss": 1.7911,
      "step": 105550
    },
    {
      "epoch": 1.5542439986459238,
      "grad_norm": 2.304337501525879,
      "learning_rate": 1.1764072272676396e-05,
      "loss": 1.7511,
      "step": 105600
    },
    {
      "epoch": 1.554979909630131,
      "grad_norm": 1.9503384828567505,
      "learning_rate": 1.1726854649526337e-05,
      "loss": 1.7678,
      "step": 105650
    },
    {
      "epoch": 1.5557158206143384,
      "grad_norm": 2.359882116317749,
      "learning_rate": 1.1689688169167978e-05,
      "loss": 1.7427,
      "step": 105700
    },
    {
      "epoch": 1.5564517315985458,
      "grad_norm": 2.3202927112579346,
      "learning_rate": 1.1652572881265327e-05,
      "loss": 1.7948,
      "step": 105750
    },
    {
      "epoch": 1.557187642582753,
      "grad_norm": 2.257821798324585,
      "learning_rate": 1.1615508835413979e-05,
      "loss": 1.8209,
      "step": 105800
    },
    {
      "epoch": 1.5579235535669604,
      "grad_norm": 2.3336873054504395,
      "learning_rate": 1.1578496081141055e-05,
      "loss": 1.7969,
      "step": 105850
    },
    {
      "epoch": 1.5586594645511678,
      "grad_norm": 2.354144334793091,
      "learning_rate": 1.1541534667905141e-05,
      "loss": 1.7957,
      "step": 105900
    },
    {
      "epoch": 1.559395375535375,
      "grad_norm": 3.0371077060699463,
      "learning_rate": 1.1504624645096212e-05,
      "loss": 1.7168,
      "step": 105950
    },
    {
      "epoch": 1.5601312865195824,
      "grad_norm": 2.130979537963867,
      "learning_rate": 1.1467766062035584e-05,
      "loss": 1.8082,
      "step": 106000
    },
    {
      "epoch": 1.56086719750379,
      "grad_norm": 2.3317341804504395,
      "learning_rate": 1.143095896797582e-05,
      "loss": 1.8327,
      "step": 106050
    },
    {
      "epoch": 1.5616031084879973,
      "grad_norm": 2.3779842853546143,
      "learning_rate": 1.1394203412100695e-05,
      "loss": 1.7694,
      "step": 106100
    },
    {
      "epoch": 1.5623390194722047,
      "grad_norm": 2.1353676319122314,
      "learning_rate": 1.135749944352511e-05,
      "loss": 1.8142,
      "step": 106150
    },
    {
      "epoch": 1.563074930456412,
      "grad_norm": 2.210723876953125,
      "learning_rate": 1.132084711129503e-05,
      "loss": 1.8158,
      "step": 106200
    },
    {
      "epoch": 1.5638108414406193,
      "grad_norm": 1.9914350509643555,
      "learning_rate": 1.1284246464387421e-05,
      "loss": 1.8028,
      "step": 106250
    },
    {
      "epoch": 1.5645467524248267,
      "grad_norm": 2.234234571456909,
      "learning_rate": 1.124769755171019e-05,
      "loss": 1.7439,
      "step": 106300
    },
    {
      "epoch": 1.565282663409034,
      "grad_norm": 2.039853811264038,
      "learning_rate": 1.1211200422102103e-05,
      "loss": 1.7371,
      "step": 106350
    },
    {
      "epoch": 1.5660185743932415,
      "grad_norm": 2.563143491744995,
      "learning_rate": 1.1174755124332741e-05,
      "loss": 1.7576,
      "step": 106400
    },
    {
      "epoch": 1.5667544853774489,
      "grad_norm": 2.2502331733703613,
      "learning_rate": 1.1138361707102423e-05,
      "loss": 1.774,
      "step": 106450
    },
    {
      "epoch": 1.5674903963616562,
      "grad_norm": 2.335035800933838,
      "learning_rate": 1.110202021904213e-05,
      "loss": 1.7682,
      "step": 106500
    },
    {
      "epoch": 1.5682263073458635,
      "grad_norm": 2.175370454788208,
      "learning_rate": 1.1065730708713456e-05,
      "loss": 1.7683,
      "step": 106550
    },
    {
      "epoch": 1.5689622183300709,
      "grad_norm": 2.0105104446411133,
      "learning_rate": 1.1029493224608568e-05,
      "loss": 1.8162,
      "step": 106600
    },
    {
      "epoch": 1.5696981293142782,
      "grad_norm": 2.1604244709014893,
      "learning_rate": 1.0993307815150078e-05,
      "loss": 1.7438,
      "step": 106650
    },
    {
      "epoch": 1.5704340402984855,
      "grad_norm": 2.2576286792755127,
      "learning_rate": 1.0957174528691038e-05,
      "loss": 1.7716,
      "step": 106700
    },
    {
      "epoch": 1.5711699512826929,
      "grad_norm": 2.1379215717315674,
      "learning_rate": 1.0921093413514804e-05,
      "loss": 1.7998,
      "step": 106750
    },
    {
      "epoch": 1.5719058622669002,
      "grad_norm": 2.0676472187042236,
      "learning_rate": 1.088506451783507e-05,
      "loss": 1.7522,
      "step": 106800
    },
    {
      "epoch": 1.5726417732511075,
      "grad_norm": 2.4088432788848877,
      "learning_rate": 1.0849087889795728e-05,
      "loss": 1.7858,
      "step": 106850
    },
    {
      "epoch": 1.5733776842353149,
      "grad_norm": 2.675837278366089,
      "learning_rate": 1.0813163577470831e-05,
      "loss": 1.8664,
      "step": 106900
    },
    {
      "epoch": 1.5741135952195222,
      "grad_norm": 2.015521764755249,
      "learning_rate": 1.0777291628864527e-05,
      "loss": 1.7945,
      "step": 106950
    },
    {
      "epoch": 1.5748495062037295,
      "grad_norm": 2.225689649581909,
      "learning_rate": 1.0741472091910987e-05,
      "loss": 1.7529,
      "step": 107000
    },
    {
      "epoch": 1.5755854171879369,
      "grad_norm": 2.1193439960479736,
      "learning_rate": 1.0705705014474348e-05,
      "loss": 1.7776,
      "step": 107050
    },
    {
      "epoch": 1.5763213281721442,
      "grad_norm": 2.188204050064087,
      "learning_rate": 1.0669990444348648e-05,
      "loss": 1.8093,
      "step": 107100
    },
    {
      "epoch": 1.5770572391563515,
      "grad_norm": 2.2706964015960693,
      "learning_rate": 1.0634328429257767e-05,
      "loss": 1.8024,
      "step": 107150
    },
    {
      "epoch": 1.5777931501405589,
      "grad_norm": 2.2084476947784424,
      "learning_rate": 1.059871901685534e-05,
      "loss": 1.747,
      "step": 107200
    },
    {
      "epoch": 1.5785290611247662,
      "grad_norm": 2.3197057247161865,
      "learning_rate": 1.0563162254724746e-05,
      "loss": 1.7645,
      "step": 107250
    },
    {
      "epoch": 1.5792649721089735,
      "grad_norm": 1.9086889028549194,
      "learning_rate": 1.0527658190378975e-05,
      "loss": 1.7444,
      "step": 107300
    },
    {
      "epoch": 1.580000883093181,
      "grad_norm": 2.2790372371673584,
      "learning_rate": 1.0492206871260618e-05,
      "loss": 1.7923,
      "step": 107350
    },
    {
      "epoch": 1.5807367940773884,
      "grad_norm": 2.215975522994995,
      "learning_rate": 1.0456808344741776e-05,
      "loss": 1.7719,
      "step": 107400
    },
    {
      "epoch": 1.5814727050615958,
      "grad_norm": 2.56327486038208,
      "learning_rate": 1.042146265812401e-05,
      "loss": 1.7658,
      "step": 107450
    },
    {
      "epoch": 1.582208616045803,
      "grad_norm": 2.4551095962524414,
      "learning_rate": 1.0386169858638267e-05,
      "loss": 1.7546,
      "step": 107500
    },
    {
      "epoch": 1.5829445270300104,
      "grad_norm": 2.560284376144409,
      "learning_rate": 1.0350929993444836e-05,
      "loss": 1.7749,
      "step": 107550
    },
    {
      "epoch": 1.5836804380142178,
      "grad_norm": 2.11360764503479,
      "learning_rate": 1.0315743109633258e-05,
      "loss": 1.7078,
      "step": 107600
    },
    {
      "epoch": 1.5844163489984253,
      "grad_norm": 2.5742244720458984,
      "learning_rate": 1.0280609254222301e-05,
      "loss": 1.8071,
      "step": 107650
    },
    {
      "epoch": 1.5851522599826326,
      "grad_norm": 2.4428911209106445,
      "learning_rate": 1.0245528474159827e-05,
      "loss": 1.828,
      "step": 107700
    },
    {
      "epoch": 1.58588817096684,
      "grad_norm": 2.19114351272583,
      "learning_rate": 1.0210500816322816e-05,
      "loss": 1.7322,
      "step": 107750
    },
    {
      "epoch": 1.5866240819510473,
      "grad_norm": 2.03132700920105,
      "learning_rate": 1.0175526327517255e-05,
      "loss": 1.7687,
      "step": 107800
    },
    {
      "epoch": 1.5873599929352546,
      "grad_norm": 2.4390058517456055,
      "learning_rate": 1.0140605054478064e-05,
      "loss": 1.8269,
      "step": 107850
    },
    {
      "epoch": 1.588095903919462,
      "grad_norm": 2.325225591659546,
      "learning_rate": 1.0105737043869095e-05,
      "loss": 1.8331,
      "step": 107900
    },
    {
      "epoch": 1.5888318149036693,
      "grad_norm": 2.005565881729126,
      "learning_rate": 1.0070922342282996e-05,
      "loss": 1.7918,
      "step": 107950
    },
    {
      "epoch": 1.5895677258878766,
      "grad_norm": 2.4187560081481934,
      "learning_rate": 1.0036160996241173e-05,
      "loss": 1.7901,
      "step": 108000
    },
    {
      "epoch": 1.590303636872084,
      "grad_norm": 2.305659294128418,
      "learning_rate": 1.000145305219376e-05,
      "loss": 1.7716,
      "step": 108050
    },
    {
      "epoch": 1.5910395478562913,
      "grad_norm": 2.3494341373443604,
      "learning_rate": 9.966798556519508e-06,
      "loss": 1.7458,
      "step": 108100
    },
    {
      "epoch": 1.5917754588404986,
      "grad_norm": 1.979434847831726,
      "learning_rate": 9.932197555525774e-06,
      "loss": 1.8019,
      "step": 108150
    },
    {
      "epoch": 1.592511369824706,
      "grad_norm": 2.1897149085998535,
      "learning_rate": 9.897650095448407e-06,
      "loss": 1.8072,
      "step": 108200
    },
    {
      "epoch": 1.5932472808089133,
      "grad_norm": 2.3322367668151855,
      "learning_rate": 9.86315622245173e-06,
      "loss": 1.755,
      "step": 108250
    },
    {
      "epoch": 1.5939831917931206,
      "grad_norm": 2.1564712524414062,
      "learning_rate": 9.82871598262845e-06,
      "loss": 1.8214,
      "step": 108300
    },
    {
      "epoch": 1.594719102777328,
      "grad_norm": 2.7955918312072754,
      "learning_rate": 9.794329421999603e-06,
      "loss": 1.8432,
      "step": 108350
    },
    {
      "epoch": 1.5954550137615353,
      "grad_norm": 2.4387035369873047,
      "learning_rate": 9.759996586514509e-06,
      "loss": 1.8016,
      "step": 108400
    },
    {
      "epoch": 1.5961909247457426,
      "grad_norm": 2.213984489440918,
      "learning_rate": 9.725717522050682e-06,
      "loss": 1.7276,
      "step": 108450
    },
    {
      "epoch": 1.59692683572995,
      "grad_norm": 2.405027389526367,
      "learning_rate": 9.6914922744138e-06,
      "loss": 1.7608,
      "step": 108500
    },
    {
      "epoch": 1.5976627467141573,
      "grad_norm": 2.1508874893188477,
      "learning_rate": 9.657320889337613e-06,
      "loss": 1.7727,
      "step": 108550
    },
    {
      "epoch": 1.5983986576983646,
      "grad_norm": 2.4150519371032715,
      "learning_rate": 9.623203412483906e-06,
      "loss": 1.7569,
      "step": 108600
    },
    {
      "epoch": 1.5991345686825722,
      "grad_norm": 2.358990430831909,
      "learning_rate": 9.589139889442423e-06,
      "loss": 1.7572,
      "step": 108650
    },
    {
      "epoch": 1.5998704796667795,
      "grad_norm": 2.1995842456817627,
      "learning_rate": 9.555130365730818e-06,
      "loss": 1.7308,
      "step": 108700
    },
    {
      "epoch": 1.6006063906509869,
      "grad_norm": 2.444714069366455,
      "learning_rate": 9.521174886794587e-06,
      "loss": 1.77,
      "step": 108750
    },
    {
      "epoch": 1.6013423016351942,
      "grad_norm": 2.1612420082092285,
      "learning_rate": 9.487273498007005e-06,
      "loss": 1.7855,
      "step": 108800
    },
    {
      "epoch": 1.6020782126194015,
      "grad_norm": 2.2398855686187744,
      "learning_rate": 9.453426244669061e-06,
      "loss": 1.8227,
      "step": 108850
    },
    {
      "epoch": 1.6028141236036089,
      "grad_norm": 2.042602062225342,
      "learning_rate": 9.41963317200943e-06,
      "loss": 1.8222,
      "step": 108900
    },
    {
      "epoch": 1.6035500345878164,
      "grad_norm": 2.339484930038452,
      "learning_rate": 9.385894325184352e-06,
      "loss": 1.8444,
      "step": 108950
    },
    {
      "epoch": 1.6042859455720238,
      "grad_norm": 2.3573410511016846,
      "learning_rate": 9.35220974927764e-06,
      "loss": 1.8233,
      "step": 109000
    },
    {
      "epoch": 1.605021856556231,
      "grad_norm": 2.2712645530700684,
      "learning_rate": 9.318579489300571e-06,
      "loss": 1.7593,
      "step": 109050
    },
    {
      "epoch": 1.6057577675404384,
      "grad_norm": 2.3727777004241943,
      "learning_rate": 9.285003590191826e-06,
      "loss": 1.8052,
      "step": 109100
    },
    {
      "epoch": 1.6064936785246458,
      "grad_norm": 2.188936948776245,
      "learning_rate": 9.2514820968175e-06,
      "loss": 1.8058,
      "step": 109150
    },
    {
      "epoch": 1.607229589508853,
      "grad_norm": 2.278134346008301,
      "learning_rate": 9.21801505397093e-06,
      "loss": 1.8053,
      "step": 109200
    },
    {
      "epoch": 1.6079655004930604,
      "grad_norm": 2.2450850009918213,
      "learning_rate": 9.184602506372726e-06,
      "loss": 1.762,
      "step": 109250
    },
    {
      "epoch": 1.6087014114772678,
      "grad_norm": 2.241905689239502,
      "learning_rate": 9.151244498670658e-06,
      "loss": 1.7461,
      "step": 109300
    },
    {
      "epoch": 1.609437322461475,
      "grad_norm": 2.449503183364868,
      "learning_rate": 9.11794107543963e-06,
      "loss": 1.7144,
      "step": 109350
    },
    {
      "epoch": 1.6101732334456824,
      "grad_norm": 2.660688638687134,
      "learning_rate": 9.084692281181612e-06,
      "loss": 1.7609,
      "step": 109400
    },
    {
      "epoch": 1.6109091444298897,
      "grad_norm": 2.5731489658355713,
      "learning_rate": 9.05149816032555e-06,
      "loss": 1.7851,
      "step": 109450
    },
    {
      "epoch": 1.611645055414097,
      "grad_norm": 2.2279856204986572,
      "learning_rate": 9.018358757227351e-06,
      "loss": 1.7506,
      "step": 109500
    },
    {
      "epoch": 1.6123809663983044,
      "grad_norm": 2.1615092754364014,
      "learning_rate": 8.985274116169817e-06,
      "loss": 1.8222,
      "step": 109550
    },
    {
      "epoch": 1.6131168773825117,
      "grad_norm": 2.1240971088409424,
      "learning_rate": 8.952244281362548e-06,
      "loss": 1.7715,
      "step": 109600
    },
    {
      "epoch": 1.613852788366719,
      "grad_norm": 2.2597808837890625,
      "learning_rate": 8.91926929694193e-06,
      "loss": 1.7747,
      "step": 109650
    },
    {
      "epoch": 1.6145886993509264,
      "grad_norm": 2.488084316253662,
      "learning_rate": 8.88634920697104e-06,
      "loss": 1.7895,
      "step": 109700
    },
    {
      "epoch": 1.6153246103351337,
      "grad_norm": 2.258857488632202,
      "learning_rate": 8.853484055439593e-06,
      "loss": 1.7948,
      "step": 109750
    },
    {
      "epoch": 1.616060521319341,
      "grad_norm": 2.2500290870666504,
      "learning_rate": 8.82067388626393e-06,
      "loss": 1.7075,
      "step": 109800
    },
    {
      "epoch": 1.6167964323035484,
      "grad_norm": 2.0894651412963867,
      "learning_rate": 8.787918743286899e-06,
      "loss": 1.7331,
      "step": 109850
    },
    {
      "epoch": 1.6175323432877557,
      "grad_norm": 2.407026529312134,
      "learning_rate": 8.755218670277805e-06,
      "loss": 1.7971,
      "step": 109900
    },
    {
      "epoch": 1.6182682542719633,
      "grad_norm": 2.1176414489746094,
      "learning_rate": 8.722573710932374e-06,
      "loss": 1.798,
      "step": 109950
    },
    {
      "epoch": 1.6190041652561706,
      "grad_norm": 2.2965996265411377,
      "learning_rate": 8.6899839088727e-06,
      "loss": 1.7631,
      "step": 110000
    },
    {
      "epoch": 1.619740076240378,
      "grad_norm": 1.5322188138961792,
      "learning_rate": 8.657449307647164e-06,
      "loss": 1.8089,
      "step": 110050
    },
    {
      "epoch": 1.6204759872245853,
      "grad_norm": 2.0976781845092773,
      "learning_rate": 8.624969950730378e-06,
      "loss": 1.76,
      "step": 110100
    },
    {
      "epoch": 1.6212118982087926,
      "grad_norm": 2.3106484413146973,
      "learning_rate": 8.592545881523134e-06,
      "loss": 1.7368,
      "step": 110150
    },
    {
      "epoch": 1.621947809193,
      "grad_norm": 2.0978124141693115,
      "learning_rate": 8.560177143352354e-06,
      "loss": 1.7736,
      "step": 110200
    },
    {
      "epoch": 1.6226837201772075,
      "grad_norm": 2.317735433578491,
      "learning_rate": 8.527863779471018e-06,
      "loss": 1.7713,
      "step": 110250
    },
    {
      "epoch": 1.6234196311614149,
      "grad_norm": 2.3802058696746826,
      "learning_rate": 8.495605833058117e-06,
      "loss": 1.7868,
      "step": 110300
    },
    {
      "epoch": 1.6241555421456222,
      "grad_norm": 2.511495590209961,
      "learning_rate": 8.463403347218596e-06,
      "loss": 1.7909,
      "step": 110350
    },
    {
      "epoch": 1.6248914531298295,
      "grad_norm": 2.264512300491333,
      "learning_rate": 8.431256364983242e-06,
      "loss": 1.7756,
      "step": 110400
    },
    {
      "epoch": 1.6256273641140369,
      "grad_norm": 2.4006786346435547,
      "learning_rate": 8.399164929308751e-06,
      "loss": 1.7858,
      "step": 110450
    },
    {
      "epoch": 1.6263632750982442,
      "grad_norm": 2.0100371837615967,
      "learning_rate": 8.367129083077541e-06,
      "loss": 1.7565,
      "step": 110500
    },
    {
      "epoch": 1.6270991860824515,
      "grad_norm": 2.2675678730010986,
      "learning_rate": 8.33514886909777e-06,
      "loss": 1.7987,
      "step": 110550
    },
    {
      "epoch": 1.6278350970666589,
      "grad_norm": 2.195115804672241,
      "learning_rate": 8.303224330103243e-06,
      "loss": 1.7835,
      "step": 110600
    },
    {
      "epoch": 1.6285710080508662,
      "grad_norm": 2.3952560424804688,
      "learning_rate": 8.27135550875338e-06,
      "loss": 1.7522,
      "step": 110650
    },
    {
      "epoch": 1.6293069190350735,
      "grad_norm": 1.860835075378418,
      "learning_rate": 8.239542447633148e-06,
      "loss": 1.7906,
      "step": 110700
    },
    {
      "epoch": 1.6300428300192809,
      "grad_norm": 2.6059632301330566,
      "learning_rate": 8.207785189252999e-06,
      "loss": 1.8062,
      "step": 110750
    },
    {
      "epoch": 1.6307787410034882,
      "grad_norm": 1.921858310699463,
      "learning_rate": 8.17608377604881e-06,
      "loss": 1.7822,
      "step": 110800
    },
    {
      "epoch": 1.6315146519876955,
      "grad_norm": 2.2287778854370117,
      "learning_rate": 8.144438250381858e-06,
      "loss": 1.7768,
      "step": 110850
    },
    {
      "epoch": 1.6322505629719029,
      "grad_norm": 2.4370007514953613,
      "learning_rate": 8.112848654538712e-06,
      "loss": 1.7555,
      "step": 110900
    },
    {
      "epoch": 1.6329864739561102,
      "grad_norm": 1.9800219535827637,
      "learning_rate": 8.081315030731234e-06,
      "loss": 1.7501,
      "step": 110950
    },
    {
      "epoch": 1.6337223849403175,
      "grad_norm": 2.4354238510131836,
      "learning_rate": 8.049837421096461e-06,
      "loss": 1.8215,
      "step": 111000
    },
    {
      "epoch": 1.6344582959245249,
      "grad_norm": 2.0318620204925537,
      "learning_rate": 8.018415867696594e-06,
      "loss": 1.8085,
      "step": 111050
    },
    {
      "epoch": 1.6351942069087322,
      "grad_norm": 2.0732614994049072,
      "learning_rate": 7.987050412518965e-06,
      "loss": 1.7741,
      "step": 111100
    },
    {
      "epoch": 1.6359301178929395,
      "grad_norm": 2.2713441848754883,
      "learning_rate": 7.955741097475877e-06,
      "loss": 1.7773,
      "step": 111150
    },
    {
      "epoch": 1.636666028877147,
      "grad_norm": 2.604771375656128,
      "learning_rate": 7.924487964404654e-06,
      "loss": 1.7762,
      "step": 111200
    },
    {
      "epoch": 1.6374019398613544,
      "grad_norm": 2.52708101272583,
      "learning_rate": 7.893291055067548e-06,
      "loss": 1.8079,
      "step": 111250
    },
    {
      "epoch": 1.6381378508455617,
      "grad_norm": 1.9152170419692993,
      "learning_rate": 7.86215041115167e-06,
      "loss": 1.7846,
      "step": 111300
    },
    {
      "epoch": 1.638873761829769,
      "grad_norm": 2.18369197845459,
      "learning_rate": 7.83106607426895e-06,
      "loss": 1.7915,
      "step": 111350
    },
    {
      "epoch": 1.6396096728139764,
      "grad_norm": 1.921303153038025,
      "learning_rate": 7.800038085956079e-06,
      "loss": 1.7813,
      "step": 111400
    },
    {
      "epoch": 1.6403455837981837,
      "grad_norm": 1.7775894403457642,
      "learning_rate": 7.769066487674454e-06,
      "loss": 1.7299,
      "step": 111450
    },
    {
      "epoch": 1.641081494782391,
      "grad_norm": 2.1752331256866455,
      "learning_rate": 7.738151320810116e-06,
      "loss": 1.7966,
      "step": 111500
    },
    {
      "epoch": 1.6418174057665986,
      "grad_norm": 2.1358277797698975,
      "learning_rate": 7.707292626673695e-06,
      "loss": 1.81,
      "step": 111550
    },
    {
      "epoch": 1.642553316750806,
      "grad_norm": 2.172633409500122,
      "learning_rate": 7.676490446500366e-06,
      "loss": 1.7866,
      "step": 111600
    },
    {
      "epoch": 1.6432892277350133,
      "grad_norm": 2.7264270782470703,
      "learning_rate": 7.64574482144979e-06,
      "loss": 1.7798,
      "step": 111650
    },
    {
      "epoch": 1.6440251387192206,
      "grad_norm": 1.991101622581482,
      "learning_rate": 7.615055792606035e-06,
      "loss": 1.7646,
      "step": 111700
    },
    {
      "epoch": 1.644761049703428,
      "grad_norm": 2.6522774696350098,
      "learning_rate": 7.58442340097758e-06,
      "loss": 1.7263,
      "step": 111750
    },
    {
      "epoch": 1.6454969606876353,
      "grad_norm": 2.9784185886383057,
      "learning_rate": 7.55384768749719e-06,
      "loss": 1.7735,
      "step": 111800
    },
    {
      "epoch": 1.6462328716718426,
      "grad_norm": 2.3468799591064453,
      "learning_rate": 7.523328693021897e-06,
      "loss": 1.772,
      "step": 111850
    },
    {
      "epoch": 1.64696878265605,
      "grad_norm": 2.1033263206481934,
      "learning_rate": 7.492866458332953e-06,
      "loss": 1.7728,
      "step": 111900
    },
    {
      "epoch": 1.6477046936402573,
      "grad_norm": 2.206108808517456,
      "learning_rate": 7.462461024135758e-06,
      "loss": 1.7521,
      "step": 111950
    },
    {
      "epoch": 1.6484406046244646,
      "grad_norm": 2.318317413330078,
      "learning_rate": 7.432112431059813e-06,
      "loss": 1.8343,
      "step": 112000
    },
    {
      "epoch": 1.649176515608672,
      "grad_norm": 2.26784348487854,
      "learning_rate": 7.40182071965867e-06,
      "loss": 1.7741,
      "step": 112050
    },
    {
      "epoch": 1.6499124265928793,
      "grad_norm": 2.660928964614868,
      "learning_rate": 7.371585930409852e-06,
      "loss": 1.7454,
      "step": 112100
    },
    {
      "epoch": 1.6506483375770866,
      "grad_norm": 2.2227392196655273,
      "learning_rate": 7.3414081037148475e-06,
      "loss": 1.7623,
      "step": 112150
    },
    {
      "epoch": 1.651384248561294,
      "grad_norm": 2.0060043334960938,
      "learning_rate": 7.311287279899004e-06,
      "loss": 1.7901,
      "step": 112200
    },
    {
      "epoch": 1.6521201595455013,
      "grad_norm": 2.3830225467681885,
      "learning_rate": 7.281223499211515e-06,
      "loss": 1.8418,
      "step": 112250
    },
    {
      "epoch": 1.6528560705297086,
      "grad_norm": 2.3778107166290283,
      "learning_rate": 7.251216801825344e-06,
      "loss": 1.8104,
      "step": 112300
    },
    {
      "epoch": 1.653591981513916,
      "grad_norm": 2.451408863067627,
      "learning_rate": 7.22126722783717e-06,
      "loss": 1.7744,
      "step": 112350
    },
    {
      "epoch": 1.6543278924981233,
      "grad_norm": 1.8244348764419556,
      "learning_rate": 7.19137481726736e-06,
      "loss": 1.7306,
      "step": 112400
    },
    {
      "epoch": 1.6550638034823306,
      "grad_norm": 2.396059036254883,
      "learning_rate": 7.1615396100598804e-06,
      "loss": 1.7692,
      "step": 112450
    },
    {
      "epoch": 1.6557997144665382,
      "grad_norm": 2.090606451034546,
      "learning_rate": 7.131761646082258e-06,
      "loss": 1.7646,
      "step": 112500
    },
    {
      "epoch": 1.6565356254507455,
      "grad_norm": 2.126282215118408,
      "learning_rate": 7.102040965125528e-06,
      "loss": 1.7614,
      "step": 112550
    },
    {
      "epoch": 1.6572715364349528,
      "grad_norm": 2.255783796310425,
      "learning_rate": 7.0723776069041904e-06,
      "loss": 1.7456,
      "step": 112600
    },
    {
      "epoch": 1.6580074474191602,
      "grad_norm": 2.124501943588257,
      "learning_rate": 7.042771611056142e-06,
      "loss": 1.7929,
      "step": 112650
    },
    {
      "epoch": 1.6587433584033675,
      "grad_norm": 2.4486024379730225,
      "learning_rate": 7.0132230171426175e-06,
      "loss": 1.7785,
      "step": 112700
    },
    {
      "epoch": 1.6594792693875748,
      "grad_norm": 2.263484477996826,
      "learning_rate": 6.983731864648168e-06,
      "loss": 1.7424,
      "step": 112750
    },
    {
      "epoch": 1.6602151803717824,
      "grad_norm": 2.065589189529419,
      "learning_rate": 6.954298192980574e-06,
      "loss": 1.7464,
      "step": 112800
    },
    {
      "epoch": 1.6609510913559897,
      "grad_norm": 1.907192587852478,
      "learning_rate": 6.924922041470811e-06,
      "loss": 1.789,
      "step": 112850
    },
    {
      "epoch": 1.661687002340197,
      "grad_norm": 2.1476192474365234,
      "learning_rate": 6.895603449372995e-06,
      "loss": 1.7977,
      "step": 112900
    },
    {
      "epoch": 1.6624229133244044,
      "grad_norm": 2.5356690883636475,
      "learning_rate": 6.866342455864322e-06,
      "loss": 1.7579,
      "step": 112950
    },
    {
      "epoch": 1.6631588243086117,
      "grad_norm": 2.4123599529266357,
      "learning_rate": 6.837139100045031e-06,
      "loss": 1.8009,
      "step": 113000
    },
    {
      "epoch": 1.663894735292819,
      "grad_norm": 2.1895506381988525,
      "learning_rate": 6.80799342093833e-06,
      "loss": 1.8234,
      "step": 113050
    },
    {
      "epoch": 1.6646306462770264,
      "grad_norm": 2.02416729927063,
      "learning_rate": 6.7789054574903645e-06,
      "loss": 1.7698,
      "step": 113100
    },
    {
      "epoch": 1.6653665572612337,
      "grad_norm": 2.176029682159424,
      "learning_rate": 6.7498752485701476e-06,
      "loss": 1.7756,
      "step": 113150
    },
    {
      "epoch": 1.666102468245441,
      "grad_norm": 2.182708501815796,
      "learning_rate": 6.720902832969539e-06,
      "loss": 1.7609,
      "step": 113200
    },
    {
      "epoch": 1.6668383792296484,
      "grad_norm": 2.2714431285858154,
      "learning_rate": 6.6919882494031475e-06,
      "loss": 1.7613,
      "step": 113250
    },
    {
      "epoch": 1.6675742902138557,
      "grad_norm": 2.0649149417877197,
      "learning_rate": 6.66313153650831e-06,
      "loss": 1.7741,
      "step": 113300
    },
    {
      "epoch": 1.668310201198063,
      "grad_norm": 2.189445734024048,
      "learning_rate": 6.634332732845044e-06,
      "loss": 1.7594,
      "step": 113350
    },
    {
      "epoch": 1.6690461121822704,
      "grad_norm": 2.435586452484131,
      "learning_rate": 6.6055918768959725e-06,
      "loss": 1.8342,
      "step": 113400
    },
    {
      "epoch": 1.6697820231664777,
      "grad_norm": 2.3358519077301025,
      "learning_rate": 6.576909007066295e-06,
      "loss": 1.7881,
      "step": 113450
    },
    {
      "epoch": 1.670517934150685,
      "grad_norm": 2.422194242477417,
      "learning_rate": 6.548284161683716e-06,
      "loss": 1.7855,
      "step": 113500
    },
    {
      "epoch": 1.6712538451348924,
      "grad_norm": 2.176426649093628,
      "learning_rate": 6.519717378998419e-06,
      "loss": 1.7361,
      "step": 113550
    },
    {
      "epoch": 1.6719897561190997,
      "grad_norm": 2.2875216007232666,
      "learning_rate": 6.491208697182982e-06,
      "loss": 1.8212,
      "step": 113600
    },
    {
      "epoch": 1.672725667103307,
      "grad_norm": 2.168644428253174,
      "learning_rate": 6.462758154332349e-06,
      "loss": 1.8395,
      "step": 113650
    },
    {
      "epoch": 1.6734615780875144,
      "grad_norm": 2.2261440753936768,
      "learning_rate": 6.434365788463809e-06,
      "loss": 1.7897,
      "step": 113700
    },
    {
      "epoch": 1.6741974890717217,
      "grad_norm": 2.3276684284210205,
      "learning_rate": 6.4060316375168735e-06,
      "loss": 1.8355,
      "step": 113750
    },
    {
      "epoch": 1.6749334000559293,
      "grad_norm": 2.3055942058563232,
      "learning_rate": 6.377755739353264e-06,
      "loss": 1.8117,
      "step": 113800
    },
    {
      "epoch": 1.6756693110401366,
      "grad_norm": 2.1551015377044678,
      "learning_rate": 6.349538131756872e-06,
      "loss": 1.7736,
      "step": 113850
    },
    {
      "epoch": 1.676405222024344,
      "grad_norm": 2.448272943496704,
      "learning_rate": 6.321378852433707e-06,
      "loss": 1.8351,
      "step": 113900
    },
    {
      "epoch": 1.6771411330085513,
      "grad_norm": 2.109067440032959,
      "learning_rate": 6.293277939011827e-06,
      "loss": 1.7682,
      "step": 113950
    },
    {
      "epoch": 1.6778770439927586,
      "grad_norm": 1.9862828254699707,
      "learning_rate": 6.265235429041289e-06,
      "loss": 1.7195,
      "step": 114000
    },
    {
      "epoch": 1.678612954976966,
      "grad_norm": 2.4240188598632812,
      "learning_rate": 6.2372513599941285e-06,
      "loss": 1.7306,
      "step": 114050
    },
    {
      "epoch": 1.6793488659611735,
      "grad_norm": 1.8193401098251343,
      "learning_rate": 6.209325769264274e-06,
      "loss": 1.7645,
      "step": 114100
    },
    {
      "epoch": 1.6800847769453808,
      "grad_norm": 1.9773834943771362,
      "learning_rate": 6.181458694167514e-06,
      "loss": 1.7832,
      "step": 114150
    },
    {
      "epoch": 1.6808206879295882,
      "grad_norm": 2.212968349456787,
      "learning_rate": 6.15365017194145e-06,
      "loss": 1.7772,
      "step": 114200
    },
    {
      "epoch": 1.6815565989137955,
      "grad_norm": 2.6163077354431152,
      "learning_rate": 6.125900239745424e-06,
      "loss": 1.7188,
      "step": 114250
    },
    {
      "epoch": 1.6822925098980028,
      "grad_norm": 2.4256138801574707,
      "learning_rate": 6.098208934660532e-06,
      "loss": 1.755,
      "step": 114300
    },
    {
      "epoch": 1.6830284208822102,
      "grad_norm": 2.1749019622802734,
      "learning_rate": 6.070576293689473e-06,
      "loss": 1.7171,
      "step": 114350
    },
    {
      "epoch": 1.6837643318664175,
      "grad_norm": 2.1789298057556152,
      "learning_rate": 6.0430023537565935e-06,
      "loss": 1.7761,
      "step": 114400
    },
    {
      "epoch": 1.6845002428506248,
      "grad_norm": 2.111287832260132,
      "learning_rate": 6.015487151707782e-06,
      "loss": 1.8096,
      "step": 114450
    },
    {
      "epoch": 1.6852361538348322,
      "grad_norm": 2.4233908653259277,
      "learning_rate": 5.9880307243104504e-06,
      "loss": 1.7624,
      "step": 114500
    },
    {
      "epoch": 1.6859720648190395,
      "grad_norm": 2.5797176361083984,
      "learning_rate": 5.960633108253455e-06,
      "loss": 1.7445,
      "step": 114550
    },
    {
      "epoch": 1.6867079758032468,
      "grad_norm": 2.3730709552764893,
      "learning_rate": 5.933294340147083e-06,
      "loss": 1.7403,
      "step": 114600
    },
    {
      "epoch": 1.6874438867874542,
      "grad_norm": 2.4356613159179688,
      "learning_rate": 5.906014456522979e-06,
      "loss": 1.7076,
      "step": 114650
    },
    {
      "epoch": 1.6881797977716615,
      "grad_norm": 2.1204936504364014,
      "learning_rate": 5.878793493834095e-06,
      "loss": 1.7656,
      "step": 114700
    },
    {
      "epoch": 1.6889157087558688,
      "grad_norm": 2.135021209716797,
      "learning_rate": 5.851631488454679e-06,
      "loss": 1.742,
      "step": 114750
    },
    {
      "epoch": 1.6896516197400762,
      "grad_norm": 1.9417526721954346,
      "learning_rate": 5.8245284766801415e-06,
      "loss": 1.743,
      "step": 114800
    },
    {
      "epoch": 1.6903875307242835,
      "grad_norm": 2.200833559036255,
      "learning_rate": 5.797484494727112e-06,
      "loss": 1.7348,
      "step": 114850
    },
    {
      "epoch": 1.6911234417084908,
      "grad_norm": 2.1245827674865723,
      "learning_rate": 5.770499578733313e-06,
      "loss": 1.7202,
      "step": 114900
    },
    {
      "epoch": 1.6918593526926982,
      "grad_norm": 2.5906100273132324,
      "learning_rate": 5.743573764757576e-06,
      "loss": 1.805,
      "step": 114950
    },
    {
      "epoch": 1.6925952636769055,
      "grad_norm": 2.160580635070801,
      "learning_rate": 5.71670708877971e-06,
      "loss": 1.8225,
      "step": 115000
    },
    {
      "epoch": 1.6933311746611128,
      "grad_norm": 2.4557743072509766,
      "learning_rate": 5.689899586700536e-06,
      "loss": 1.8152,
      "step": 115050
    },
    {
      "epoch": 1.6940670856453204,
      "grad_norm": 2.1637966632843018,
      "learning_rate": 5.663151294341779e-06,
      "loss": 1.7853,
      "step": 115100
    },
    {
      "epoch": 1.6948029966295277,
      "grad_norm": 2.7130467891693115,
      "learning_rate": 5.636462247446061e-06,
      "loss": 1.784,
      "step": 115150
    },
    {
      "epoch": 1.695538907613735,
      "grad_norm": 2.412307024002075,
      "learning_rate": 5.609832481676825e-06,
      "loss": 1.7507,
      "step": 115200
    },
    {
      "epoch": 1.6962748185979424,
      "grad_norm": 2.3760147094726562,
      "learning_rate": 5.583262032618314e-06,
      "loss": 1.7797,
      "step": 115250
    },
    {
      "epoch": 1.6970107295821497,
      "grad_norm": 2.3007538318634033,
      "learning_rate": 5.5567509357754895e-06,
      "loss": 1.8057,
      "step": 115300
    },
    {
      "epoch": 1.697746640566357,
      "grad_norm": 1.9570515155792236,
      "learning_rate": 5.530299226574026e-06,
      "loss": 1.7911,
      "step": 115350
    },
    {
      "epoch": 1.6984825515505646,
      "grad_norm": 2.3788089752197266,
      "learning_rate": 5.503906940360215e-06,
      "loss": 1.7581,
      "step": 115400
    },
    {
      "epoch": 1.699218462534772,
      "grad_norm": 2.3610405921936035,
      "learning_rate": 5.4775741124009684e-06,
      "loss": 1.7576,
      "step": 115450
    },
    {
      "epoch": 1.6999543735189793,
      "grad_norm": 2.496661901473999,
      "learning_rate": 5.4513007778837226e-06,
      "loss": 1.7994,
      "step": 115500
    },
    {
      "epoch": 1.7006902845031866,
      "grad_norm": 2.2027430534362793,
      "learning_rate": 5.425086971916437e-06,
      "loss": 1.8171,
      "step": 115550
    },
    {
      "epoch": 1.701426195487394,
      "grad_norm": 2.500206470489502,
      "learning_rate": 5.3989327295275115e-06,
      "loss": 1.7875,
      "step": 115600
    },
    {
      "epoch": 1.7021621064716013,
      "grad_norm": 2.065647840499878,
      "learning_rate": 5.37283808566576e-06,
      "loss": 1.7933,
      "step": 115650
    },
    {
      "epoch": 1.7028980174558086,
      "grad_norm": 2.343946933746338,
      "learning_rate": 5.346803075200358e-06,
      "loss": 1.7353,
      "step": 115700
    },
    {
      "epoch": 1.703633928440016,
      "grad_norm": 2.4123008251190186,
      "learning_rate": 5.320827732920791e-06,
      "loss": 1.7546,
      "step": 115750
    },
    {
      "epoch": 1.7043698394242233,
      "grad_norm": 2.175985097885132,
      "learning_rate": 5.294912093536808e-06,
      "loss": 1.7659,
      "step": 115800
    },
    {
      "epoch": 1.7051057504084306,
      "grad_norm": 2.193997621536255,
      "learning_rate": 5.269056191678401e-06,
      "loss": 1.7909,
      "step": 115850
    },
    {
      "epoch": 1.705841661392638,
      "grad_norm": 2.1581878662109375,
      "learning_rate": 5.24326006189571e-06,
      "loss": 1.7776,
      "step": 115900
    },
    {
      "epoch": 1.7065775723768453,
      "grad_norm": 2.1132895946502686,
      "learning_rate": 5.2175237386590195e-06,
      "loss": 1.8158,
      "step": 115950
    },
    {
      "epoch": 1.7073134833610526,
      "grad_norm": 2.1608359813690186,
      "learning_rate": 5.191847256358695e-06,
      "loss": 1.7829,
      "step": 116000
    },
    {
      "epoch": 1.70804939434526,
      "grad_norm": 2.528313636779785,
      "learning_rate": 5.166230649305137e-06,
      "loss": 1.7465,
      "step": 116050
    },
    {
      "epoch": 1.7087853053294673,
      "grad_norm": 2.3380959033966064,
      "learning_rate": 5.140673951728742e-06,
      "loss": 1.8346,
      "step": 116100
    },
    {
      "epoch": 1.7095212163136746,
      "grad_norm": 2.262970447540283,
      "learning_rate": 5.115177197779841e-06,
      "loss": 1.8064,
      "step": 116150
    },
    {
      "epoch": 1.710257127297882,
      "grad_norm": 1.9883826971054077,
      "learning_rate": 5.089740421528671e-06,
      "loss": 1.7367,
      "step": 116200
    },
    {
      "epoch": 1.7109930382820893,
      "grad_norm": 2.3322954177856445,
      "learning_rate": 5.064363656965338e-06,
      "loss": 1.8226,
      "step": 116250
    },
    {
      "epoch": 1.7117289492662966,
      "grad_norm": 2.188042640686035,
      "learning_rate": 5.039046937999736e-06,
      "loss": 1.7173,
      "step": 116300
    },
    {
      "epoch": 1.7124648602505042,
      "grad_norm": 2.316118001937866,
      "learning_rate": 5.013790298461529e-06,
      "loss": 1.7618,
      "step": 116350
    },
    {
      "epoch": 1.7132007712347115,
      "grad_norm": 2.264914035797119,
      "learning_rate": 4.988593772100097e-06,
      "loss": 1.7734,
      "step": 116400
    },
    {
      "epoch": 1.7139366822189188,
      "grad_norm": 2.547031879425049,
      "learning_rate": 4.963457392584514e-06,
      "loss": 1.8044,
      "step": 116450
    },
    {
      "epoch": 1.7146725932031261,
      "grad_norm": 2.3236641883850098,
      "learning_rate": 4.938381193503444e-06,
      "loss": 1.7433,
      "step": 116500
    },
    {
      "epoch": 1.7154085041873335,
      "grad_norm": 2.08059024810791,
      "learning_rate": 4.913365208365156e-06,
      "loss": 1.7665,
      "step": 116550
    },
    {
      "epoch": 1.7161444151715408,
      "grad_norm": 2.2079124450683594,
      "learning_rate": 4.888409470597471e-06,
      "loss": 1.8575,
      "step": 116600
    },
    {
      "epoch": 1.7168803261557481,
      "grad_norm": 2.400381565093994,
      "learning_rate": 4.863514013547676e-06,
      "loss": 1.7735,
      "step": 116650
    },
    {
      "epoch": 1.7176162371399557,
      "grad_norm": 2.2475764751434326,
      "learning_rate": 4.838678870482532e-06,
      "loss": 1.7781,
      "step": 116700
    },
    {
      "epoch": 1.718352148124163,
      "grad_norm": 2.2591238021850586,
      "learning_rate": 4.813904074588188e-06,
      "loss": 1.7475,
      "step": 116750
    },
    {
      "epoch": 1.7190880591083704,
      "grad_norm": 2.146427869796753,
      "learning_rate": 4.789189658970167e-06,
      "loss": 1.7854,
      "step": 116800
    },
    {
      "epoch": 1.7198239700925777,
      "grad_norm": 2.1228644847869873,
      "learning_rate": 4.764535656653285e-06,
      "loss": 1.7796,
      "step": 116850
    },
    {
      "epoch": 1.720559881076785,
      "grad_norm": 2.0058271884918213,
      "learning_rate": 4.739942100581668e-06,
      "loss": 1.793,
      "step": 116900
    },
    {
      "epoch": 1.7212957920609924,
      "grad_norm": 2.7935791015625,
      "learning_rate": 4.715409023618639e-06,
      "loss": 1.7544,
      "step": 116950
    },
    {
      "epoch": 1.7220317030451997,
      "grad_norm": 2.29106068611145,
      "learning_rate": 4.690936458546713e-06,
      "loss": 1.7627,
      "step": 117000
    },
    {
      "epoch": 1.722767614029407,
      "grad_norm": 2.2967207431793213,
      "learning_rate": 4.666524438067554e-06,
      "loss": 1.7355,
      "step": 117050
    },
    {
      "epoch": 1.7235035250136144,
      "grad_norm": 2.1859402656555176,
      "learning_rate": 4.6421729948019135e-06,
      "loss": 1.7609,
      "step": 117100
    },
    {
      "epoch": 1.7242394359978217,
      "grad_norm": 2.418722152709961,
      "learning_rate": 4.617882161289599e-06,
      "loss": 1.7793,
      "step": 117150
    },
    {
      "epoch": 1.724975346982029,
      "grad_norm": 2.174807548522949,
      "learning_rate": 4.593651969989426e-06,
      "loss": 1.772,
      "step": 117200
    },
    {
      "epoch": 1.7257112579662364,
      "grad_norm": 2.334059715270996,
      "learning_rate": 4.569482453279178e-06,
      "loss": 1.7576,
      "step": 117250
    },
    {
      "epoch": 1.7264471689504437,
      "grad_norm": 2.0886495113372803,
      "learning_rate": 4.545373643455564e-06,
      "loss": 1.7797,
      "step": 117300
    },
    {
      "epoch": 1.727183079934651,
      "grad_norm": 2.254161834716797,
      "learning_rate": 4.5213255727341694e-06,
      "loss": 1.8005,
      "step": 117350
    },
    {
      "epoch": 1.7279189909188584,
      "grad_norm": 2.3931844234466553,
      "learning_rate": 4.497338273249424e-06,
      "loss": 1.823,
      "step": 117400
    },
    {
      "epoch": 1.7286549019030657,
      "grad_norm": 2.967583656311035,
      "learning_rate": 4.473411777054548e-06,
      "loss": 1.8355,
      "step": 117450
    },
    {
      "epoch": 1.729390812887273,
      "grad_norm": 2.3573572635650635,
      "learning_rate": 4.4495461161214835e-06,
      "loss": 1.7489,
      "step": 117500
    },
    {
      "epoch": 1.7301267238714804,
      "grad_norm": 2.3522098064422607,
      "learning_rate": 4.425741322340937e-06,
      "loss": 1.7715,
      "step": 117550
    },
    {
      "epoch": 1.7308626348556877,
      "grad_norm": 2.466651201248169,
      "learning_rate": 4.401997427522242e-06,
      "loss": 1.745,
      "step": 117600
    },
    {
      "epoch": 1.7315985458398953,
      "grad_norm": 2.079901695251465,
      "learning_rate": 4.378314463393363e-06,
      "loss": 1.7888,
      "step": 117650
    },
    {
      "epoch": 1.7323344568241026,
      "grad_norm": 2.2725651264190674,
      "learning_rate": 4.354692461600851e-06,
      "loss": 1.8051,
      "step": 117700
    },
    {
      "epoch": 1.73307036780831,
      "grad_norm": 2.769122838973999,
      "learning_rate": 4.331131453709786e-06,
      "loss": 1.7585,
      "step": 117750
    },
    {
      "epoch": 1.7338062787925173,
      "grad_norm": 2.139819383621216,
      "learning_rate": 4.30763147120376e-06,
      "loss": 1.7685,
      "step": 117800
    },
    {
      "epoch": 1.7345421897767246,
      "grad_norm": 2.078585386276245,
      "learning_rate": 4.2841925454848e-06,
      "loss": 1.7444,
      "step": 117850
    },
    {
      "epoch": 1.735278100760932,
      "grad_norm": 2.1446526050567627,
      "learning_rate": 4.260814707873356e-06,
      "loss": 1.7956,
      "step": 117900
    },
    {
      "epoch": 1.7360140117451395,
      "grad_norm": 2.3603720664978027,
      "learning_rate": 4.2374979896082526e-06,
      "loss": 1.7831,
      "step": 117950
    },
    {
      "epoch": 1.7367499227293468,
      "grad_norm": 2.637820243835449,
      "learning_rate": 4.214242421846637e-06,
      "loss": 1.7331,
      "step": 118000
    },
    {
      "epoch": 1.7374858337135541,
      "grad_norm": 2.3744988441467285,
      "learning_rate": 4.191048035663942e-06,
      "loss": 1.7993,
      "step": 118050
    },
    {
      "epoch": 1.7382217446977615,
      "grad_norm": 2.2321457862854004,
      "learning_rate": 4.167914862053845e-06,
      "loss": 1.7355,
      "step": 118100
    },
    {
      "epoch": 1.7389576556819688,
      "grad_norm": 2.1924846172332764,
      "learning_rate": 4.1448429319282444e-06,
      "loss": 1.7603,
      "step": 118150
    },
    {
      "epoch": 1.7396935666661761,
      "grad_norm": 2.1554157733917236,
      "learning_rate": 4.121832276117172e-06,
      "loss": 1.8244,
      "step": 118200
    },
    {
      "epoch": 1.7404294776503835,
      "grad_norm": 1.9921163320541382,
      "learning_rate": 4.098882925368813e-06,
      "loss": 1.7722,
      "step": 118250
    },
    {
      "epoch": 1.7411653886345908,
      "grad_norm": 2.473363161087036,
      "learning_rate": 4.075994910349407e-06,
      "loss": 1.7537,
      "step": 118300
    },
    {
      "epoch": 1.7419012996187981,
      "grad_norm": 2.02957820892334,
      "learning_rate": 4.053168261643247e-06,
      "loss": 1.7838,
      "step": 118350
    },
    {
      "epoch": 1.7426372106030055,
      "grad_norm": 2.4578449726104736,
      "learning_rate": 4.030403009752626e-06,
      "loss": 1.7873,
      "step": 118400
    },
    {
      "epoch": 1.7433731215872128,
      "grad_norm": 2.4829485416412354,
      "learning_rate": 4.007699185097785e-06,
      "loss": 1.7994,
      "step": 118450
    },
    {
      "epoch": 1.7441090325714201,
      "grad_norm": 2.166261672973633,
      "learning_rate": 3.9850568180168916e-06,
      "loss": 1.816,
      "step": 118500
    },
    {
      "epoch": 1.7448449435556275,
      "grad_norm": 2.2632102966308594,
      "learning_rate": 3.962475938765986e-06,
      "loss": 1.7951,
      "step": 118550
    },
    {
      "epoch": 1.7455808545398348,
      "grad_norm": 2.3894479274749756,
      "learning_rate": 3.93995657751895e-06,
      "loss": 1.757,
      "step": 118600
    },
    {
      "epoch": 1.7463167655240421,
      "grad_norm": 2.6652402877807617,
      "learning_rate": 3.917498764367455e-06,
      "loss": 1.7454,
      "step": 118650
    },
    {
      "epoch": 1.7470526765082495,
      "grad_norm": 2.276970624923706,
      "learning_rate": 3.895102529320927e-06,
      "loss": 1.7842,
      "step": 118700
    },
    {
      "epoch": 1.7477885874924568,
      "grad_norm": 2.432985305786133,
      "learning_rate": 3.872767902306512e-06,
      "loss": 1.7887,
      "step": 118750
    },
    {
      "epoch": 1.7485244984766641,
      "grad_norm": 2.383711814880371,
      "learning_rate": 3.850494913169028e-06,
      "loss": 1.7843,
      "step": 118800
    },
    {
      "epoch": 1.7492604094608715,
      "grad_norm": 2.1600072383880615,
      "learning_rate": 3.828283591670945e-06,
      "loss": 1.815,
      "step": 118850
    },
    {
      "epoch": 1.7499963204450788,
      "grad_norm": 2.3162176609039307,
      "learning_rate": 3.8061339674923035e-06,
      "loss": 1.7316,
      "step": 118900
    },
    {
      "epoch": 1.7507322314292864,
      "grad_norm": 2.490938186645508,
      "learning_rate": 3.7840460702307235e-06,
      "loss": 1.7528,
      "step": 118950
    },
    {
      "epoch": 1.7514681424134937,
      "grad_norm": 2.354717969894409,
      "learning_rate": 3.7620199294013224e-06,
      "loss": 1.7841,
      "step": 119000
    },
    {
      "epoch": 1.752204053397701,
      "grad_norm": 2.2744340896606445,
      "learning_rate": 3.7400555744367073e-06,
      "loss": 1.7509,
      "step": 119050
    },
    {
      "epoch": 1.7529399643819084,
      "grad_norm": 2.2106173038482666,
      "learning_rate": 3.718153034686922e-06,
      "loss": 1.7965,
      "step": 119100
    },
    {
      "epoch": 1.7536758753661157,
      "grad_norm": 2.398499011993408,
      "learning_rate": 3.6963123394194165e-06,
      "loss": 1.7403,
      "step": 119150
    },
    {
      "epoch": 1.754411786350323,
      "grad_norm": 2.752967596054077,
      "learning_rate": 3.6745335178189734e-06,
      "loss": 1.8022,
      "step": 119200
    },
    {
      "epoch": 1.7551476973345306,
      "grad_norm": 1.9087733030319214,
      "learning_rate": 3.652816598987718e-06,
      "loss": 1.8118,
      "step": 119250
    },
    {
      "epoch": 1.755883608318738,
      "grad_norm": 2.3877408504486084,
      "learning_rate": 3.6311616119450662e-06,
      "loss": 1.7922,
      "step": 119300
    },
    {
      "epoch": 1.7566195193029452,
      "grad_norm": 2.435734272003174,
      "learning_rate": 3.609568585627654e-06,
      "loss": 1.7726,
      "step": 119350
    },
    {
      "epoch": 1.7573554302871526,
      "grad_norm": 2.3044703006744385,
      "learning_rate": 3.588037548889328e-06,
      "loss": 1.7483,
      "step": 119400
    },
    {
      "epoch": 1.75809134127136,
      "grad_norm": 1.9814395904541016,
      "learning_rate": 3.5665685305011185e-06,
      "loss": 1.7988,
      "step": 119450
    },
    {
      "epoch": 1.7588272522555672,
      "grad_norm": 2.0026557445526123,
      "learning_rate": 3.5451615591511665e-06,
      "loss": 1.7873,
      "step": 119500
    },
    {
      "epoch": 1.7595631632397746,
      "grad_norm": 1.9303219318389893,
      "learning_rate": 3.5238166634447122e-06,
      "loss": 1.7575,
      "step": 119550
    },
    {
      "epoch": 1.760299074223982,
      "grad_norm": 3.013559103012085,
      "learning_rate": 3.5025338719040235e-06,
      "loss": 1.8293,
      "step": 119600
    },
    {
      "epoch": 1.7610349852081892,
      "grad_norm": 2.0312323570251465,
      "learning_rate": 3.4813132129684124e-06,
      "loss": 1.7581,
      "step": 119650
    },
    {
      "epoch": 1.7617708961923966,
      "grad_norm": 2.241323947906494,
      "learning_rate": 3.460154714994146e-06,
      "loss": 1.8275,
      "step": 119700
    },
    {
      "epoch": 1.762506807176604,
      "grad_norm": 2.277564764022827,
      "learning_rate": 3.4390584062544363e-06,
      "loss": 1.7409,
      "step": 119750
    },
    {
      "epoch": 1.7632427181608112,
      "grad_norm": 2.0643258094787598,
      "learning_rate": 3.418024314939389e-06,
      "loss": 1.847,
      "step": 119800
    },
    {
      "epoch": 1.7639786291450186,
      "grad_norm": 2.133992910385132,
      "learning_rate": 3.397052469155976e-06,
      "loss": 1.7749,
      "step": 119850
    },
    {
      "epoch": 1.764714540129226,
      "grad_norm": 2.3073744773864746,
      "learning_rate": 3.376142896927992e-06,
      "loss": 1.7773,
      "step": 119900
    },
    {
      "epoch": 1.7654504511134332,
      "grad_norm": 2.1804211139678955,
      "learning_rate": 3.3552956261960143e-06,
      "loss": 1.7703,
      "step": 119950
    },
    {
      "epoch": 1.7661863620976406,
      "grad_norm": 2.3426477909088135,
      "learning_rate": 3.334510684817377e-06,
      "loss": 1.6948,
      "step": 120000
    },
    {
      "epoch": 1.766922273081848,
      "grad_norm": 2.386449098587036,
      "learning_rate": 3.313788100566112e-06,
      "loss": 1.756,
      "step": 120050
    },
    {
      "epoch": 1.7676581840660552,
      "grad_norm": 2.879857063293457,
      "learning_rate": 3.2931279011329475e-06,
      "loss": 1.7774,
      "step": 120100
    },
    {
      "epoch": 1.7683940950502626,
      "grad_norm": 2.087005376815796,
      "learning_rate": 3.2725301141252327e-06,
      "loss": 1.699,
      "step": 120150
    },
    {
      "epoch": 1.76913000603447,
      "grad_norm": 2.651066780090332,
      "learning_rate": 3.2519947670669238e-06,
      "loss": 1.7694,
      "step": 120200
    },
    {
      "epoch": 1.7698659170186775,
      "grad_norm": 2.522465705871582,
      "learning_rate": 3.2315218873985364e-06,
      "loss": 1.7884,
      "step": 120250
    },
    {
      "epoch": 1.7706018280028848,
      "grad_norm": 2.5040221214294434,
      "learning_rate": 3.211111502477121e-06,
      "loss": 1.7662,
      "step": 120300
    },
    {
      "epoch": 1.7713377389870921,
      "grad_norm": 2.1946661472320557,
      "learning_rate": 3.190763639576211e-06,
      "loss": 1.8011,
      "step": 120350
    },
    {
      "epoch": 1.7720736499712995,
      "grad_norm": 2.2270336151123047,
      "learning_rate": 3.170478325885801e-06,
      "loss": 1.7043,
      "step": 120400
    },
    {
      "epoch": 1.7728095609555068,
      "grad_norm": 2.4993057250976562,
      "learning_rate": 3.150255588512302e-06,
      "loss": 1.7709,
      "step": 120450
    },
    {
      "epoch": 1.7735454719397141,
      "grad_norm": 2.166330575942993,
      "learning_rate": 3.1300954544785043e-06,
      "loss": 1.7656,
      "step": 120500
    },
    {
      "epoch": 1.7742813829239217,
      "grad_norm": 2.0554730892181396,
      "learning_rate": 3.1099979507235477e-06,
      "loss": 1.7726,
      "step": 120550
    },
    {
      "epoch": 1.775017293908129,
      "grad_norm": 2.2632124423980713,
      "learning_rate": 3.0899631041028777e-06,
      "loss": 1.7245,
      "step": 120600
    },
    {
      "epoch": 1.7757532048923363,
      "grad_norm": 2.3284478187561035,
      "learning_rate": 3.0699909413882175e-06,
      "loss": 1.7921,
      "step": 120650
    },
    {
      "epoch": 1.7764891158765437,
      "grad_norm": 2.9044551849365234,
      "learning_rate": 3.0500814892675145e-06,
      "loss": 1.7879,
      "step": 120700
    },
    {
      "epoch": 1.777225026860751,
      "grad_norm": 2.345916509628296,
      "learning_rate": 3.0302347743449533e-06,
      "loss": 1.7301,
      "step": 120750
    },
    {
      "epoch": 1.7779609378449583,
      "grad_norm": 2.6856164932250977,
      "learning_rate": 3.010450823140848e-06,
      "loss": 1.8188,
      "step": 120800
    },
    {
      "epoch": 1.7786968488291657,
      "grad_norm": 2.2120606899261475,
      "learning_rate": 2.9907296620916738e-06,
      "loss": 1.7901,
      "step": 120850
    },
    {
      "epoch": 1.779432759813373,
      "grad_norm": 2.118725299835205,
      "learning_rate": 2.9710713175499614e-06,
      "loss": 1.7498,
      "step": 120900
    },
    {
      "epoch": 1.7801686707975803,
      "grad_norm": 2.511535406112671,
      "learning_rate": 2.951475815784349e-06,
      "loss": 1.8615,
      "step": 120950
    },
    {
      "epoch": 1.7809045817817877,
      "grad_norm": 2.5875654220581055,
      "learning_rate": 2.931943182979474e-06,
      "loss": 1.7513,
      "step": 121000
    },
    {
      "epoch": 1.781640492765995,
      "grad_norm": 2.506084442138672,
      "learning_rate": 2.912473445235969e-06,
      "loss": 1.8093,
      "step": 121050
    },
    {
      "epoch": 1.7823764037502023,
      "grad_norm": 2.5940377712249756,
      "learning_rate": 2.8930666285704288e-06,
      "loss": 1.7741,
      "step": 121100
    },
    {
      "epoch": 1.7831123147344097,
      "grad_norm": 2.32954740524292,
      "learning_rate": 2.8737227589153647e-06,
      "loss": 1.77,
      "step": 121150
    },
    {
      "epoch": 1.783848225718617,
      "grad_norm": 2.4162521362304688,
      "learning_rate": 2.854441862119167e-06,
      "loss": 1.795,
      "step": 121200
    },
    {
      "epoch": 1.7845841367028243,
      "grad_norm": 2.3597421646118164,
      "learning_rate": 2.8352239639460932e-06,
      "loss": 1.7742,
      "step": 121250
    },
    {
      "epoch": 1.7853200476870317,
      "grad_norm": 2.4655773639678955,
      "learning_rate": 2.816069090076212e-06,
      "loss": 1.7663,
      "step": 121300
    },
    {
      "epoch": 1.786055958671239,
      "grad_norm": 2.1631436347961426,
      "learning_rate": 2.7969772661053607e-06,
      "loss": 1.7424,
      "step": 121350
    },
    {
      "epoch": 1.7867918696554463,
      "grad_norm": 1.991513729095459,
      "learning_rate": 2.7779485175451536e-06,
      "loss": 1.8214,
      "step": 121400
    },
    {
      "epoch": 1.7875277806396537,
      "grad_norm": 2.0230634212493896,
      "learning_rate": 2.758982869822907e-06,
      "loss": 1.8099,
      "step": 121450
    },
    {
      "epoch": 1.7882636916238612,
      "grad_norm": 2.141850709915161,
      "learning_rate": 2.7400803482816153e-06,
      "loss": 1.7343,
      "step": 121500
    },
    {
      "epoch": 1.7889996026080686,
      "grad_norm": 2.0310769081115723,
      "learning_rate": 2.721240978179912e-06,
      "loss": 1.7935,
      "step": 121550
    },
    {
      "epoch": 1.789735513592276,
      "grad_norm": 2.392174243927002,
      "learning_rate": 2.702464784692066e-06,
      "loss": 1.7648,
      "step": 121600
    },
    {
      "epoch": 1.7904714245764832,
      "grad_norm": 2.5591869354248047,
      "learning_rate": 2.6837517929079017e-06,
      "loss": 1.7855,
      "step": 121650
    },
    {
      "epoch": 1.7912073355606906,
      "grad_norm": 2.4274377822875977,
      "learning_rate": 2.665102027832811e-06,
      "loss": 1.8186,
      "step": 121700
    },
    {
      "epoch": 1.791943246544898,
      "grad_norm": 2.4514334201812744,
      "learning_rate": 2.6465155143876876e-06,
      "loss": 1.7598,
      "step": 121750
    },
    {
      "epoch": 1.7926791575291052,
      "grad_norm": 2.230759859085083,
      "learning_rate": 2.627992277408903e-06,
      "loss": 1.7649,
      "step": 121800
    },
    {
      "epoch": 1.7934150685133128,
      "grad_norm": 2.2573113441467285,
      "learning_rate": 2.609532341648285e-06,
      "loss": 1.7349,
      "step": 121850
    },
    {
      "epoch": 1.7941509794975201,
      "grad_norm": 2.286817789077759,
      "learning_rate": 2.5911357317730588e-06,
      "loss": 1.7616,
      "step": 121900
    },
    {
      "epoch": 1.7948868904817274,
      "grad_norm": 2.066535711288452,
      "learning_rate": 2.5728024723658427e-06,
      "loss": 1.7529,
      "step": 121950
    },
    {
      "epoch": 1.7956228014659348,
      "grad_norm": 1.9743796586990356,
      "learning_rate": 2.554532587924596e-06,
      "loss": 1.7608,
      "step": 122000
    },
    {
      "epoch": 1.7963587124501421,
      "grad_norm": 2.4136509895324707,
      "learning_rate": 2.5363261028626016e-06,
      "loss": 1.8037,
      "step": 122050
    },
    {
      "epoch": 1.7970946234343494,
      "grad_norm": 2.6316781044006348,
      "learning_rate": 2.5181830415084206e-06,
      "loss": 1.7179,
      "step": 122100
    },
    {
      "epoch": 1.7978305344185568,
      "grad_norm": 2.3553106784820557,
      "learning_rate": 2.5001034281058543e-06,
      "loss": 1.7825,
      "step": 122150
    },
    {
      "epoch": 1.7985664454027641,
      "grad_norm": 2.058558225631714,
      "learning_rate": 2.482087286813933e-06,
      "loss": 1.7943,
      "step": 122200
    },
    {
      "epoch": 1.7993023563869714,
      "grad_norm": 2.3046884536743164,
      "learning_rate": 2.4641346417068613e-06,
      "loss": 1.7201,
      "step": 122250
    },
    {
      "epoch": 1.8000382673711788,
      "grad_norm": 2.2828361988067627,
      "learning_rate": 2.4462455167740093e-06,
      "loss": 1.7402,
      "step": 122300
    },
    {
      "epoch": 1.8007741783553861,
      "grad_norm": 2.1160202026367188,
      "learning_rate": 2.4284199359198624e-06,
      "loss": 1.7482,
      "step": 122350
    },
    {
      "epoch": 1.8015100893395934,
      "grad_norm": 2.125262975692749,
      "learning_rate": 2.4106579229639825e-06,
      "loss": 1.7947,
      "step": 122400
    },
    {
      "epoch": 1.8022460003238008,
      "grad_norm": 2.092250347137451,
      "learning_rate": 2.3929595016410122e-06,
      "loss": 1.8158,
      "step": 122450
    },
    {
      "epoch": 1.8029819113080081,
      "grad_norm": 2.2006688117980957,
      "learning_rate": 2.375324695600595e-06,
      "loss": 1.7794,
      "step": 122500
    },
    {
      "epoch": 1.8037178222922154,
      "grad_norm": 2.3154897689819336,
      "learning_rate": 2.357753528407386e-06,
      "loss": 1.7254,
      "step": 122550
    },
    {
      "epoch": 1.8044537332764228,
      "grad_norm": 2.381221294403076,
      "learning_rate": 2.3402460235409985e-06,
      "loss": 1.7782,
      "step": 122600
    },
    {
      "epoch": 1.80518964426063,
      "grad_norm": 1.9980336427688599,
      "learning_rate": 2.322802204395963e-06,
      "loss": 1.792,
      "step": 122650
    },
    {
      "epoch": 1.8059255552448374,
      "grad_norm": 2.2008678913116455,
      "learning_rate": 2.3054220942817285e-06,
      "loss": 1.7237,
      "step": 122700
    },
    {
      "epoch": 1.8066614662290448,
      "grad_norm": 2.0579211711883545,
      "learning_rate": 2.2881057164226006e-06,
      "loss": 1.7588,
      "step": 122750
    },
    {
      "epoch": 1.8073973772132523,
      "grad_norm": 2.0284945964813232,
      "learning_rate": 2.270853093957731e-06,
      "loss": 1.6909,
      "step": 122800
    },
    {
      "epoch": 1.8081332881974597,
      "grad_norm": 1.937091588973999,
      "learning_rate": 2.2536642499410675e-06,
      "loss": 1.7662,
      "step": 122850
    },
    {
      "epoch": 1.808869199181667,
      "grad_norm": 2.2043304443359375,
      "learning_rate": 2.2365392073413315e-06,
      "loss": 1.7474,
      "step": 122900
    },
    {
      "epoch": 1.8096051101658743,
      "grad_norm": 1.960273265838623,
      "learning_rate": 2.2194779890420126e-06,
      "loss": 1.735,
      "step": 122950
    },
    {
      "epoch": 1.8103410211500817,
      "grad_norm": 2.3197340965270996,
      "learning_rate": 2.2024806178412795e-06,
      "loss": 1.756,
      "step": 123000
    },
    {
      "epoch": 1.811076932134289,
      "grad_norm": 2.421396493911743,
      "learning_rate": 2.185547116452019e-06,
      "loss": 1.7159,
      "step": 123050
    },
    {
      "epoch": 1.8118128431184966,
      "grad_norm": 2.2725536823272705,
      "learning_rate": 2.1686775075017484e-06,
      "loss": 1.7619,
      "step": 123100
    },
    {
      "epoch": 1.8125487541027039,
      "grad_norm": 2.3995823860168457,
      "learning_rate": 2.1518718135326133e-06,
      "loss": 1.6933,
      "step": 123150
    },
    {
      "epoch": 1.8132846650869112,
      "grad_norm": 2.1221418380737305,
      "learning_rate": 2.1351300570013554e-06,
      "loss": 1.8086,
      "step": 123200
    },
    {
      "epoch": 1.8140205760711186,
      "grad_norm": 2.2196192741394043,
      "learning_rate": 2.1184522602792856e-06,
      "loss": 1.7633,
      "step": 123250
    },
    {
      "epoch": 1.8147564870553259,
      "grad_norm": 2.270289421081543,
      "learning_rate": 2.1018384456522268e-06,
      "loss": 1.8054,
      "step": 123300
    },
    {
      "epoch": 1.8154923980395332,
      "grad_norm": 2.191800355911255,
      "learning_rate": 2.0852886353205314e-06,
      "loss": 1.7662,
      "step": 123350
    },
    {
      "epoch": 1.8162283090237406,
      "grad_norm": 2.220365524291992,
      "learning_rate": 2.0688028513990155e-06,
      "loss": 1.762,
      "step": 123400
    },
    {
      "epoch": 1.8169642200079479,
      "grad_norm": 2.0418319702148438,
      "learning_rate": 2.052381115916935e-06,
      "loss": 1.8252,
      "step": 123450
    },
    {
      "epoch": 1.8177001309921552,
      "grad_norm": 1.9533576965332031,
      "learning_rate": 2.0360234508179588e-06,
      "loss": 1.7955,
      "step": 123500
    },
    {
      "epoch": 1.8184360419763625,
      "grad_norm": 2.2830379009246826,
      "learning_rate": 2.0197298779601527e-06,
      "loss": 1.8104,
      "step": 123550
    },
    {
      "epoch": 1.8191719529605699,
      "grad_norm": 2.082028388977051,
      "learning_rate": 2.0035004191159223e-06,
      "loss": 1.7432,
      "step": 123600
    },
    {
      "epoch": 1.8199078639447772,
      "grad_norm": 2.5037600994110107,
      "learning_rate": 1.9873350959720092e-06,
      "loss": 1.8118,
      "step": 123650
    },
    {
      "epoch": 1.8206437749289845,
      "grad_norm": 2.1317262649536133,
      "learning_rate": 1.971233930129468e-06,
      "loss": 1.8113,
      "step": 123700
    },
    {
      "epoch": 1.8213796859131919,
      "grad_norm": 2.2487401962280273,
      "learning_rate": 1.9551969431035934e-06,
      "loss": 1.7936,
      "step": 123750
    },
    {
      "epoch": 1.8221155968973992,
      "grad_norm": 2.339062213897705,
      "learning_rate": 1.9392241563239435e-06,
      "loss": 1.8088,
      "step": 123800
    },
    {
      "epoch": 1.8228515078816065,
      "grad_norm": 2.0493741035461426,
      "learning_rate": 1.9233155911342727e-06,
      "loss": 1.7352,
      "step": 123850
    },
    {
      "epoch": 1.8235874188658139,
      "grad_norm": 2.148470640182495,
      "learning_rate": 1.9074712687925377e-06,
      "loss": 1.7393,
      "step": 123900
    },
    {
      "epoch": 1.8243233298500212,
      "grad_norm": 2.0401973724365234,
      "learning_rate": 1.891691210470825e-06,
      "loss": 1.7499,
      "step": 123950
    },
    {
      "epoch": 1.8250592408342285,
      "grad_norm": 2.8521149158477783,
      "learning_rate": 1.8759754372553784e-06,
      "loss": 1.7601,
      "step": 124000
    },
    {
      "epoch": 1.8257951518184359,
      "grad_norm": 2.4526760578155518,
      "learning_rate": 1.8603239701465159e-06,
      "loss": 1.8007,
      "step": 124050
    },
    {
      "epoch": 1.8265310628026434,
      "grad_norm": 2.3227810859680176,
      "learning_rate": 1.8447368300586298e-06,
      "loss": 1.8009,
      "step": 124100
    },
    {
      "epoch": 1.8272669737868508,
      "grad_norm": 2.0484542846679688,
      "learning_rate": 1.8292140378201593e-06,
      "loss": 1.718,
      "step": 124150
    },
    {
      "epoch": 1.828002884771058,
      "grad_norm": 2.2209513187408447,
      "learning_rate": 1.8137556141735624e-06,
      "loss": 1.7608,
      "step": 124200
    },
    {
      "epoch": 1.8287387957552654,
      "grad_norm": 2.4064559936523438,
      "learning_rate": 1.7983615797752762e-06,
      "loss": 1.7757,
      "step": 124250
    },
    {
      "epoch": 1.8294747067394728,
      "grad_norm": 1.966854214668274,
      "learning_rate": 1.783031955195702e-06,
      "loss": 1.7578,
      "step": 124300
    },
    {
      "epoch": 1.83021061772368,
      "grad_norm": 2.200782299041748,
      "learning_rate": 1.7677667609191762e-06,
      "loss": 1.7586,
      "step": 124350
    },
    {
      "epoch": 1.8309465287078877,
      "grad_norm": 2.085653066635132,
      "learning_rate": 1.7525660173439206e-06,
      "loss": 1.7953,
      "step": 124400
    },
    {
      "epoch": 1.831682439692095,
      "grad_norm": 2.471140146255493,
      "learning_rate": 1.7374297447820598e-06,
      "loss": 1.7393,
      "step": 124450
    },
    {
      "epoch": 1.8324183506763023,
      "grad_norm": 2.057629346847534,
      "learning_rate": 1.7223579634595532e-06,
      "loss": 1.7387,
      "step": 124500
    },
    {
      "epoch": 1.8331542616605097,
      "grad_norm": 2.3362462520599365,
      "learning_rate": 1.707350693516191e-06,
      "loss": 1.7894,
      "step": 124550
    },
    {
      "epoch": 1.833890172644717,
      "grad_norm": 2.3042044639587402,
      "learning_rate": 1.6924079550055483e-06,
      "loss": 1.7353,
      "step": 124600
    },
    {
      "epoch": 1.8346260836289243,
      "grad_norm": 2.213399887084961,
      "learning_rate": 1.6775297678949864e-06,
      "loss": 1.7749,
      "step": 124650
    },
    {
      "epoch": 1.8353619946131317,
      "grad_norm": 2.6433639526367188,
      "learning_rate": 1.6627161520655965e-06,
      "loss": 1.778,
      "step": 124700
    },
    {
      "epoch": 1.836097905597339,
      "grad_norm": 2.602588415145874,
      "learning_rate": 1.6479671273121833e-06,
      "loss": 1.7963,
      "step": 124750
    },
    {
      "epoch": 1.8368338165815463,
      "grad_norm": 2.2204387187957764,
      "learning_rate": 1.6332827133432537e-06,
      "loss": 1.774,
      "step": 124800
    },
    {
      "epoch": 1.8375697275657537,
      "grad_norm": 2.0066869258880615,
      "learning_rate": 1.6186629297809675e-06,
      "loss": 1.7608,
      "step": 124850
    },
    {
      "epoch": 1.838305638549961,
      "grad_norm": 2.253741979598999,
      "learning_rate": 1.6041077961611306e-06,
      "loss": 1.8354,
      "step": 124900
    },
    {
      "epoch": 1.8390415495341683,
      "grad_norm": 2.2304978370666504,
      "learning_rate": 1.589617331933152e-06,
      "loss": 1.7447,
      "step": 124950
    },
    {
      "epoch": 1.8397774605183757,
      "grad_norm": 2.3561012744903564,
      "learning_rate": 1.5751915564600317e-06,
      "loss": 1.7575,
      "step": 125000
    },
    {
      "epoch": 1.840513371502583,
      "grad_norm": 2.006401777267456,
      "learning_rate": 1.5608304890183168e-06,
      "loss": 1.8105,
      "step": 125050
    },
    {
      "epoch": 1.8412492824867903,
      "grad_norm": 2.641080379486084,
      "learning_rate": 1.5465341487981066e-06,
      "loss": 1.7736,
      "step": 125100
    },
    {
      "epoch": 1.8419851934709977,
      "grad_norm": 2.3545989990234375,
      "learning_rate": 1.5323025549029923e-06,
      "loss": 1.7233,
      "step": 125150
    },
    {
      "epoch": 1.842721104455205,
      "grad_norm": 2.251387596130371,
      "learning_rate": 1.5181357263500507e-06,
      "loss": 1.7785,
      "step": 125200
    },
    {
      "epoch": 1.8434570154394123,
      "grad_norm": 2.1205475330352783,
      "learning_rate": 1.5040336820698331e-06,
      "loss": 1.811,
      "step": 125250
    },
    {
      "epoch": 1.8441929264236196,
      "grad_norm": 2.489388942718506,
      "learning_rate": 1.4899964409062883e-06,
      "loss": 1.848,
      "step": 125300
    },
    {
      "epoch": 1.8449288374078272,
      "grad_norm": 2.178412914276123,
      "learning_rate": 1.4760240216168008e-06,
      "loss": 1.7714,
      "step": 125350
    },
    {
      "epoch": 1.8456647483920345,
      "grad_norm": 2.2519948482513428,
      "learning_rate": 1.4621164428721245e-06,
      "loss": 1.785,
      "step": 125400
    },
    {
      "epoch": 1.8464006593762419,
      "grad_norm": 2.0470941066741943,
      "learning_rate": 1.4482737232563715e-06,
      "loss": 1.7788,
      "step": 125450
    },
    {
      "epoch": 1.8471365703604492,
      "grad_norm": 1.811463475227356,
      "learning_rate": 1.4344958812669784e-06,
      "loss": 1.7579,
      "step": 125500
    },
    {
      "epoch": 1.8478724813446565,
      "grad_norm": 2.4118590354919434,
      "learning_rate": 1.4207829353147018e-06,
      "loss": 1.7433,
      "step": 125550
    },
    {
      "epoch": 1.8486083923288639,
      "grad_norm": 2.2653748989105225,
      "learning_rate": 1.4071349037235725e-06,
      "loss": 1.7796,
      "step": 125600
    },
    {
      "epoch": 1.8493443033130712,
      "grad_norm": 2.6262829303741455,
      "learning_rate": 1.3935518047308803e-06,
      "loss": 1.7676,
      "step": 125650
    },
    {
      "epoch": 1.8500802142972788,
      "grad_norm": 2.2944483757019043,
      "learning_rate": 1.3800336564871452e-06,
      "loss": 1.7559,
      "step": 125700
    },
    {
      "epoch": 1.850816125281486,
      "grad_norm": 2.4205596446990967,
      "learning_rate": 1.3665804770561009e-06,
      "loss": 1.7732,
      "step": 125750
    },
    {
      "epoch": 1.8515520362656934,
      "grad_norm": 2.548593759536743,
      "learning_rate": 1.353192284414667e-06,
      "loss": 1.7091,
      "step": 125800
    },
    {
      "epoch": 1.8522879472499008,
      "grad_norm": 2.0265305042266846,
      "learning_rate": 1.3398690964529115e-06,
      "loss": 1.7524,
      "step": 125850
    },
    {
      "epoch": 1.853023858234108,
      "grad_norm": 2.2988290786743164,
      "learning_rate": 1.3266109309740593e-06,
      "loss": 1.7451,
      "step": 125900
    },
    {
      "epoch": 1.8537597692183154,
      "grad_norm": 2.3256680965423584,
      "learning_rate": 1.313417805694439e-06,
      "loss": 1.7037,
      "step": 125950
    },
    {
      "epoch": 1.8544956802025228,
      "grad_norm": 2.136590003967285,
      "learning_rate": 1.3002897382434653e-06,
      "loss": 1.8038,
      "step": 126000
    },
    {
      "epoch": 1.85523159118673,
      "grad_norm": 2.231081962585449,
      "learning_rate": 1.2872267461636222e-06,
      "loss": 1.7581,
      "step": 126050
    },
    {
      "epoch": 1.8559675021709374,
      "grad_norm": 2.41430926322937,
      "learning_rate": 1.2742288469104302e-06,
      "loss": 1.7912,
      "step": 126100
    },
    {
      "epoch": 1.8567034131551448,
      "grad_norm": 2.2371981143951416,
      "learning_rate": 1.2612960578524403e-06,
      "loss": 1.747,
      "step": 126150
    },
    {
      "epoch": 1.857439324139352,
      "grad_norm": 2.166753053665161,
      "learning_rate": 1.248428396271195e-06,
      "loss": 1.7711,
      "step": 126200
    },
    {
      "epoch": 1.8581752351235594,
      "grad_norm": 2.3537888526916504,
      "learning_rate": 1.2356258793612075e-06,
      "loss": 1.8012,
      "step": 126250
    },
    {
      "epoch": 1.8589111461077668,
      "grad_norm": 2.158641815185547,
      "learning_rate": 1.222888524229937e-06,
      "loss": 1.7477,
      "step": 126300
    },
    {
      "epoch": 1.859647057091974,
      "grad_norm": 2.4107441902160645,
      "learning_rate": 1.2102163478977746e-06,
      "loss": 1.7599,
      "step": 126350
    },
    {
      "epoch": 1.8603829680761814,
      "grad_norm": 2.1983890533447266,
      "learning_rate": 1.1976093672980194e-06,
      "loss": 1.7901,
      "step": 126400
    },
    {
      "epoch": 1.8611188790603888,
      "grad_norm": 1.8982175588607788,
      "learning_rate": 1.1850675992768512e-06,
      "loss": 1.7437,
      "step": 126450
    },
    {
      "epoch": 1.861854790044596,
      "grad_norm": 1.8586426973342896,
      "learning_rate": 1.1725910605932921e-06,
      "loss": 1.7902,
      "step": 126500
    },
    {
      "epoch": 1.8625907010288034,
      "grad_norm": 2.4804916381835938,
      "learning_rate": 1.160179767919234e-06,
      "loss": 1.7861,
      "step": 126550
    },
    {
      "epoch": 1.8633266120130108,
      "grad_norm": 2.19956374168396,
      "learning_rate": 1.1478337378393545e-06,
      "loss": 1.8117,
      "step": 126600
    },
    {
      "epoch": 1.8640625229972183,
      "grad_norm": 1.9615497589111328,
      "learning_rate": 1.135552986851135e-06,
      "loss": 1.77,
      "step": 126650
    },
    {
      "epoch": 1.8647984339814256,
      "grad_norm": 2.4959349632263184,
      "learning_rate": 1.1233375313648264e-06,
      "loss": 1.7525,
      "step": 126700
    },
    {
      "epoch": 1.865534344965633,
      "grad_norm": 2.4253947734832764,
      "learning_rate": 1.1111873877034273e-06,
      "loss": 1.7728,
      "step": 126750
    },
    {
      "epoch": 1.8662702559498403,
      "grad_norm": 2.1899054050445557,
      "learning_rate": 1.0991025721026615e-06,
      "loss": 1.8076,
      "step": 126800
    },
    {
      "epoch": 1.8670061669340476,
      "grad_norm": 2.0798981189727783,
      "learning_rate": 1.087083100710956e-06,
      "loss": 1.7825,
      "step": 126850
    },
    {
      "epoch": 1.867742077918255,
      "grad_norm": 2.1389975547790527,
      "learning_rate": 1.07512898958943e-06,
      "loss": 1.7618,
      "step": 126900
    },
    {
      "epoch": 1.8684779889024623,
      "grad_norm": 2.2996888160705566,
      "learning_rate": 1.0632402547118558e-06,
      "loss": 1.776,
      "step": 126950
    },
    {
      "epoch": 1.8692138998866699,
      "grad_norm": 2.1500585079193115,
      "learning_rate": 1.0514169119646478e-06,
      "loss": 1.7321,
      "step": 127000
    },
    {
      "epoch": 1.8699498108708772,
      "grad_norm": 2.1335811614990234,
      "learning_rate": 1.0396589771468456e-06,
      "loss": 1.7719,
      "step": 127050
    },
    {
      "epoch": 1.8706857218550845,
      "grad_norm": 2.3387844562530518,
      "learning_rate": 1.027966465970076e-06,
      "loss": 1.7655,
      "step": 127100
    },
    {
      "epoch": 1.8714216328392919,
      "grad_norm": 2.2632365226745605,
      "learning_rate": 1.0163393940585519e-06,
      "loss": 1.7563,
      "step": 127150
    },
    {
      "epoch": 1.8721575438234992,
      "grad_norm": 2.5711233615875244,
      "learning_rate": 1.0047777769490396e-06,
      "loss": 1.7731,
      "step": 127200
    },
    {
      "epoch": 1.8728934548077065,
      "grad_norm": 2.171142816543579,
      "learning_rate": 9.932816300908421e-07,
      "loss": 1.7889,
      "step": 127250
    },
    {
      "epoch": 1.8736293657919139,
      "grad_norm": 2.303528070449829,
      "learning_rate": 9.818509688457767e-07,
      "loss": 1.7813,
      "step": 127300
    },
    {
      "epoch": 1.8743652767761212,
      "grad_norm": 2.496973991394043,
      "learning_rate": 9.704858084881475e-07,
      "loss": 1.7771,
      "step": 127350
    },
    {
      "epoch": 1.8751011877603285,
      "grad_norm": 2.226715087890625,
      "learning_rate": 9.59186164204745e-07,
      "loss": 1.769,
      "step": 127400
    },
    {
      "epoch": 1.8758370987445359,
      "grad_norm": 2.213888168334961,
      "learning_rate": 9.479520510948137e-07,
      "loss": 1.7504,
      "step": 127450
    },
    {
      "epoch": 1.8765730097287432,
      "grad_norm": 2.391833782196045,
      "learning_rate": 9.367834841700118e-07,
      "loss": 1.7526,
      "step": 127500
    },
    {
      "epoch": 1.8773089207129505,
      "grad_norm": 1.9000099897384644,
      "learning_rate": 9.256804783544348e-07,
      "loss": 1.8275,
      "step": 127550
    },
    {
      "epoch": 1.8780448316971579,
      "grad_norm": 2.2165474891662598,
      "learning_rate": 9.146430484845648e-07,
      "loss": 1.7817,
      "step": 127600
    },
    {
      "epoch": 1.8787807426813652,
      "grad_norm": 2.257349729537964,
      "learning_rate": 9.036712093092425e-07,
      "loss": 1.7823,
      "step": 127650
    },
    {
      "epoch": 1.8795166536655725,
      "grad_norm": 2.3907623291015625,
      "learning_rate": 8.927649754896794e-07,
      "loss": 1.739,
      "step": 127700
    },
    {
      "epoch": 1.8802525646497799,
      "grad_norm": 2.3296732902526855,
      "learning_rate": 8.819243615994232e-07,
      "loss": 1.7673,
      "step": 127750
    },
    {
      "epoch": 1.8809884756339872,
      "grad_norm": 2.2431087493896484,
      "learning_rate": 8.711493821243144e-07,
      "loss": 1.7879,
      "step": 127800
    },
    {
      "epoch": 1.8817243866181945,
      "grad_norm": 2.304739236831665,
      "learning_rate": 8.604400514625188e-07,
      "loss": 1.7794,
      "step": 127850
    },
    {
      "epoch": 1.8824602976024019,
      "grad_norm": 2.1245687007904053,
      "learning_rate": 8.497963839244616e-07,
      "loss": 1.7552,
      "step": 127900
    },
    {
      "epoch": 1.8831962085866094,
      "grad_norm": 2.095824718475342,
      "learning_rate": 8.392183937328268e-07,
      "loss": 1.7371,
      "step": 127950
    },
    {
      "epoch": 1.8839321195708167,
      "grad_norm": 2.6256752014160156,
      "learning_rate": 8.2870609502253e-07,
      "loss": 1.7667,
      "step": 128000
    },
    {
      "epoch": 1.884668030555024,
      "grad_norm": 2.550142288208008,
      "learning_rate": 8.18259501840718e-07,
      "loss": 1.8291,
      "step": 128050
    },
    {
      "epoch": 1.8854039415392314,
      "grad_norm": 1.837401032447815,
      "learning_rate": 8.078786281467298e-07,
      "loss": 1.7642,
      "step": 128100
    },
    {
      "epoch": 1.8861398525234387,
      "grad_norm": 2.3204219341278076,
      "learning_rate": 7.975634878120974e-07,
      "loss": 1.8047,
      "step": 128150
    },
    {
      "epoch": 1.886875763507646,
      "grad_norm": 2.5692522525787354,
      "learning_rate": 7.873140946205004e-07,
      "loss": 1.8342,
      "step": 128200
    },
    {
      "epoch": 1.8876116744918536,
      "grad_norm": 2.5129125118255615,
      "learning_rate": 7.771304622677722e-07,
      "loss": 1.7324,
      "step": 128250
    },
    {
      "epoch": 1.888347585476061,
      "grad_norm": 1.6890455484390259,
      "learning_rate": 7.670126043618775e-07,
      "loss": 1.7098,
      "step": 128300
    },
    {
      "epoch": 1.8890834964602683,
      "grad_norm": 2.4200003147125244,
      "learning_rate": 7.569605344228736e-07,
      "loss": 1.7642,
      "step": 128350
    },
    {
      "epoch": 1.8898194074444756,
      "grad_norm": 2.0938801765441895,
      "learning_rate": 7.469742658829326e-07,
      "loss": 1.7722,
      "step": 128400
    },
    {
      "epoch": 1.890555318428683,
      "grad_norm": 2.0382044315338135,
      "learning_rate": 7.370538120862636e-07,
      "loss": 1.784,
      "step": 128450
    },
    {
      "epoch": 1.8912912294128903,
      "grad_norm": 2.4424986839294434,
      "learning_rate": 7.271991862891736e-07,
      "loss": 1.763,
      "step": 128500
    },
    {
      "epoch": 1.8920271403970976,
      "grad_norm": 1.983446478843689,
      "learning_rate": 7.174104016599681e-07,
      "loss": 1.7781,
      "step": 128550
    },
    {
      "epoch": 1.892763051381305,
      "grad_norm": 2.159977912902832,
      "learning_rate": 7.076874712790005e-07,
      "loss": 1.7741,
      "step": 128600
    },
    {
      "epoch": 1.8934989623655123,
      "grad_norm": 2.087874174118042,
      "learning_rate": 6.980304081386002e-07,
      "loss": 1.7123,
      "step": 128650
    },
    {
      "epoch": 1.8942348733497196,
      "grad_norm": 2.1566317081451416,
      "learning_rate": 6.884392251431004e-07,
      "loss": 1.7801,
      "step": 128700
    },
    {
      "epoch": 1.894970784333927,
      "grad_norm": 2.0700557231903076,
      "learning_rate": 6.789139351087937e-07,
      "loss": 1.7534,
      "step": 128750
    },
    {
      "epoch": 1.8957066953181343,
      "grad_norm": 2.605377674102783,
      "learning_rate": 6.694545507639205e-07,
      "loss": 1.7318,
      "step": 128800
    },
    {
      "epoch": 1.8964426063023416,
      "grad_norm": 2.677241325378418,
      "learning_rate": 6.600610847486644e-07,
      "loss": 1.7459,
      "step": 128850
    },
    {
      "epoch": 1.897178517286549,
      "grad_norm": 2.398167610168457,
      "learning_rate": 6.507335496151179e-07,
      "loss": 1.7494,
      "step": 128900
    },
    {
      "epoch": 1.8979144282707563,
      "grad_norm": 2.230142593383789,
      "learning_rate": 6.414719578272721e-07,
      "loss": 1.7295,
      "step": 128950
    },
    {
      "epoch": 1.8986503392549636,
      "grad_norm": 2.1609089374542236,
      "learning_rate": 6.322763217609939e-07,
      "loss": 1.7877,
      "step": 129000
    },
    {
      "epoch": 1.899386250239171,
      "grad_norm": 2.3201396465301514,
      "learning_rate": 6.23146653704032e-07,
      "loss": 1.7249,
      "step": 129050
    },
    {
      "epoch": 1.9001221612233783,
      "grad_norm": 2.1408755779266357,
      "learning_rate": 6.140829658559721e-07,
      "loss": 1.7209,
      "step": 129100
    },
    {
      "epoch": 1.9008580722075856,
      "grad_norm": 2.396766424179077,
      "learning_rate": 6.050852703282483e-07,
      "loss": 1.8694,
      "step": 129150
    },
    {
      "epoch": 1.901593983191793,
      "grad_norm": 2.3995766639709473,
      "learning_rate": 5.961535791440931e-07,
      "loss": 1.7769,
      "step": 129200
    },
    {
      "epoch": 1.9023298941760005,
      "grad_norm": 2.22873592376709,
      "learning_rate": 5.872879042385537e-07,
      "loss": 1.7668,
      "step": 129250
    },
    {
      "epoch": 1.9030658051602078,
      "grad_norm": 2.3565969467163086,
      "learning_rate": 5.784882574584538e-07,
      "loss": 1.7312,
      "step": 129300
    },
    {
      "epoch": 1.9038017161444152,
      "grad_norm": 2.265261650085449,
      "learning_rate": 5.69754650562393e-07,
      "loss": 1.7628,
      "step": 129350
    },
    {
      "epoch": 1.9045376271286225,
      "grad_norm": 2.214123487472534,
      "learning_rate": 5.610870952207248e-07,
      "loss": 1.8258,
      "step": 129400
    },
    {
      "epoch": 1.9052735381128298,
      "grad_norm": 2.3197290897369385,
      "learning_rate": 5.52485603015529e-07,
      "loss": 1.78,
      "step": 129450
    },
    {
      "epoch": 1.9060094490970372,
      "grad_norm": 1.9557710886001587,
      "learning_rate": 5.439501854406337e-07,
      "loss": 1.8236,
      "step": 129500
    },
    {
      "epoch": 1.9067453600812447,
      "grad_norm": 2.4925155639648438,
      "learning_rate": 5.354808539015432e-07,
      "loss": 1.7755,
      "step": 129550
    },
    {
      "epoch": 1.907481271065452,
      "grad_norm": 2.380858898162842,
      "learning_rate": 5.270776197154714e-07,
      "loss": 1.7619,
      "step": 129600
    },
    {
      "epoch": 1.9082171820496594,
      "grad_norm": 2.375393867492676,
      "learning_rate": 5.187404941113083e-07,
      "loss": 1.7895,
      "step": 129650
    },
    {
      "epoch": 1.9089530930338667,
      "grad_norm": 1.9529967308044434,
      "learning_rate": 5.10469488229609e-07,
      "loss": 1.7827,
      "step": 129700
    },
    {
      "epoch": 1.909689004018074,
      "grad_norm": 2.6756057739257812,
      "learning_rate": 5.022646131225606e-07,
      "loss": 1.7051,
      "step": 129750
    },
    {
      "epoch": 1.9104249150022814,
      "grad_norm": 2.007843494415283,
      "learning_rate": 4.941258797539928e-07,
      "loss": 1.7811,
      "step": 129800
    },
    {
      "epoch": 1.9111608259864887,
      "grad_norm": 2.171790599822998,
      "learning_rate": 4.860532989993615e-07,
      "loss": 1.756,
      "step": 129850
    },
    {
      "epoch": 1.911896736970696,
      "grad_norm": 2.3072118759155273,
      "learning_rate": 4.780468816457051e-07,
      "loss": 1.7472,
      "step": 129900
    },
    {
      "epoch": 1.9126326479549034,
      "grad_norm": 2.6895968914031982,
      "learning_rate": 4.701066383916708e-07,
      "loss": 1.728,
      "step": 129950
    },
    {
      "epoch": 1.9133685589391107,
      "grad_norm": 1.9830970764160156,
      "learning_rate": 4.622325798474658e-07,
      "loss": 1.7551,
      "step": 130000
    },
    {
      "epoch": 1.914104469923318,
      "grad_norm": 1.9919036626815796,
      "learning_rate": 4.544247165348625e-07,
      "loss": 1.75,
      "step": 130050
    },
    {
      "epoch": 1.9148403809075254,
      "grad_norm": 2.3335494995117188,
      "learning_rate": 4.466830588871818e-07,
      "loss": 1.7497,
      "step": 130100
    },
    {
      "epoch": 1.9155762918917327,
      "grad_norm": 2.290863275527954,
      "learning_rate": 4.390076172492763e-07,
      "loss": 1.7751,
      "step": 130150
    },
    {
      "epoch": 1.91631220287594,
      "grad_norm": 2.126711845397949,
      "learning_rate": 4.313984018775086e-07,
      "loss": 1.8027,
      "step": 130200
    },
    {
      "epoch": 1.9170481138601474,
      "grad_norm": 2.0420446395874023,
      "learning_rate": 4.238554229397618e-07,
      "loss": 1.7619,
      "step": 130250
    },
    {
      "epoch": 1.9177840248443547,
      "grad_norm": 2.3712077140808105,
      "learning_rate": 4.1637869051539545e-07,
      "loss": 1.799,
      "step": 130300
    },
    {
      "epoch": 1.918519935828562,
      "grad_norm": 2.5409393310546875,
      "learning_rate": 4.0896821459525115e-07,
      "loss": 1.7405,
      "step": 130350
    },
    {
      "epoch": 1.9192558468127694,
      "grad_norm": 2.41115665435791,
      "learning_rate": 4.0162400508163554e-07,
      "loss": 1.81,
      "step": 130400
    },
    {
      "epoch": 1.9199917577969767,
      "grad_norm": 2.349879026412964,
      "learning_rate": 3.943460717883096e-07,
      "loss": 1.7414,
      "step": 130450
    },
    {
      "epoch": 1.9207276687811843,
      "grad_norm": 2.4811289310455322,
      "learning_rate": 3.8713442444047175e-07,
      "loss": 1.7802,
      "step": 130500
    },
    {
      "epoch": 1.9214635797653916,
      "grad_norm": 2.446537494659424,
      "learning_rate": 3.799890726747357e-07,
      "loss": 1.7959,
      "step": 130550
    },
    {
      "epoch": 1.922199490749599,
      "grad_norm": 1.9831478595733643,
      "learning_rate": 3.7291002603913606e-07,
      "loss": 1.6854,
      "step": 130600
    },
    {
      "epoch": 1.9229354017338063,
      "grad_norm": 2.2498202323913574,
      "learning_rate": 3.6589729399311155e-07,
      "loss": 1.7668,
      "step": 130650
    },
    {
      "epoch": 1.9236713127180136,
      "grad_norm": 2.7258756160736084,
      "learning_rate": 3.5895088590746637e-07,
      "loss": 1.7466,
      "step": 130700
    },
    {
      "epoch": 1.924407223702221,
      "grad_norm": 2.143921136856079,
      "learning_rate": 3.5207081106440334e-07,
      "loss": 1.7385,
      "step": 130750
    },
    {
      "epoch": 1.9251431346864283,
      "grad_norm": 2.4946420192718506,
      "learning_rate": 3.452570786574738e-07,
      "loss": 1.735,
      "step": 130800
    },
    {
      "epoch": 1.9258790456706358,
      "grad_norm": 2.3198299407958984,
      "learning_rate": 3.3850969779158357e-07,
      "loss": 1.8211,
      "step": 130850
    },
    {
      "epoch": 1.9266149566548432,
      "grad_norm": 2.0205466747283936,
      "learning_rate": 3.318286774829704e-07,
      "loss": 1.7327,
      "step": 130900
    },
    {
      "epoch": 1.9273508676390505,
      "grad_norm": 2.1941773891448975,
      "learning_rate": 3.252140266592041e-07,
      "loss": 1.7274,
      "step": 130950
    },
    {
      "epoch": 1.9280867786232578,
      "grad_norm": 2.572742223739624,
      "learning_rate": 3.1866575415915313e-07,
      "loss": 1.7374,
      "step": 131000
    },
    {
      "epoch": 1.9288226896074652,
      "grad_norm": 2.2311692237854004,
      "learning_rate": 3.1218386873301255e-07,
      "loss": 1.6628,
      "step": 131050
    },
    {
      "epoch": 1.9295586005916725,
      "grad_norm": 2.233487129211426,
      "learning_rate": 3.057683790422483e-07,
      "loss": 1.7664,
      "step": 131100
    },
    {
      "epoch": 1.9302945115758798,
      "grad_norm": 2.236412525177002,
      "learning_rate": 2.9941929365960297e-07,
      "loss": 1.8117,
      "step": 131150
    },
    {
      "epoch": 1.9310304225600872,
      "grad_norm": 2.48228120803833,
      "learning_rate": 2.9313662106909556e-07,
      "loss": 1.7606,
      "step": 131200
    },
    {
      "epoch": 1.9317663335442945,
      "grad_norm": 2.1475276947021484,
      "learning_rate": 2.8692036966599945e-07,
      "loss": 1.7155,
      "step": 131250
    },
    {
      "epoch": 1.9325022445285018,
      "grad_norm": 2.3404922485351562,
      "learning_rate": 2.8077054775682565e-07,
      "loss": 1.7555,
      "step": 131300
    },
    {
      "epoch": 1.9332381555127092,
      "grad_norm": 2.4475302696228027,
      "learning_rate": 2.746871635593284e-07,
      "loss": 1.7751,
      "step": 131350
    },
    {
      "epoch": 1.9339740664969165,
      "grad_norm": 2.238668203353882,
      "learning_rate": 2.6867022520246645e-07,
      "loss": 1.7677,
      "step": 131400
    },
    {
      "epoch": 1.9347099774811238,
      "grad_norm": 2.1817197799682617,
      "learning_rate": 2.627197407264359e-07,
      "loss": 1.7595,
      "step": 131450
    },
    {
      "epoch": 1.9354458884653312,
      "grad_norm": 2.451751708984375,
      "learning_rate": 2.568357180826042e-07,
      "loss": 1.7619,
      "step": 131500
    },
    {
      "epoch": 1.9361817994495385,
      "grad_norm": 2.138014793395996,
      "learning_rate": 2.510181651335486e-07,
      "loss": 1.7614,
      "step": 131550
    },
    {
      "epoch": 1.9369177104337458,
      "grad_norm": 2.335655927658081,
      "learning_rate": 2.452670896530229e-07,
      "loss": 1.7562,
      "step": 131600
    },
    {
      "epoch": 1.9376536214179532,
      "grad_norm": 2.348947048187256,
      "learning_rate": 2.39582499325941e-07,
      "loss": 1.728,
      "step": 131650
    },
    {
      "epoch": 1.9383895324021605,
      "grad_norm": 2.2489571571350098,
      "learning_rate": 2.3396440174838774e-07,
      "loss": 1.7793,
      "step": 131700
    },
    {
      "epoch": 1.9391254433863678,
      "grad_norm": 2.1317050457000732,
      "learning_rate": 2.2841280442758572e-07,
      "loss": 1.7451,
      "step": 131750
    },
    {
      "epoch": 1.9398613543705754,
      "grad_norm": 2.155550241470337,
      "learning_rate": 2.2292771478190643e-07,
      "loss": 1.7631,
      "step": 131800
    },
    {
      "epoch": 1.9405972653547827,
      "grad_norm": 2.098747968673706,
      "learning_rate": 2.175091401408369e-07,
      "loss": 1.7555,
      "step": 131850
    },
    {
      "epoch": 1.94133317633899,
      "grad_norm": 2.4179368019104004,
      "learning_rate": 2.1215708774499077e-07,
      "loss": 1.7914,
      "step": 131900
    },
    {
      "epoch": 1.9420690873231974,
      "grad_norm": 2.7815101146698,
      "learning_rate": 2.0687156474609725e-07,
      "loss": 1.8107,
      "step": 131950
    },
    {
      "epoch": 1.9428049983074047,
      "grad_norm": 2.115248680114746,
      "learning_rate": 2.016525782069678e-07,
      "loss": 1.714,
      "step": 132000
    },
    {
      "epoch": 1.943540909291612,
      "grad_norm": 2.4653773307800293,
      "learning_rate": 1.9650013510152387e-07,
      "loss": 1.769,
      "step": 132050
    },
    {
      "epoch": 1.9442768202758194,
      "grad_norm": 1.9774677753448486,
      "learning_rate": 1.9141424231475247e-07,
      "loss": 1.7803,
      "step": 132100
    },
    {
      "epoch": 1.945012731260027,
      "grad_norm": 2.362884998321533,
      "learning_rate": 1.8639490664272286e-07,
      "loss": 1.8154,
      "step": 132150
    },
    {
      "epoch": 1.9457486422442343,
      "grad_norm": 2.158334493637085,
      "learning_rate": 1.8144213479255322e-07,
      "loss": 1.7333,
      "step": 132200
    },
    {
      "epoch": 1.9464845532284416,
      "grad_norm": 2.5595667362213135,
      "learning_rate": 1.7655593338243292e-07,
      "loss": 1.814,
      "step": 132250
    },
    {
      "epoch": 1.947220464212649,
      "grad_norm": 2.554459571838379,
      "learning_rate": 1.71736308941578e-07,
      "loss": 1.796,
      "step": 132300
    },
    {
      "epoch": 1.9479563751968563,
      "grad_norm": 2.681060314178467,
      "learning_rate": 1.6698326791025898e-07,
      "loss": 1.7695,
      "step": 132350
    },
    {
      "epoch": 1.9486922861810636,
      "grad_norm": 1.930874228477478,
      "learning_rate": 1.6229681663976203e-07,
      "loss": 1.7657,
      "step": 132400
    },
    {
      "epoch": 1.949428197165271,
      "grad_norm": 2.4447104930877686,
      "learning_rate": 1.5767696139238897e-07,
      "loss": 1.7733,
      "step": 132450
    },
    {
      "epoch": 1.9501641081494783,
      "grad_norm": 2.3557703495025635,
      "learning_rate": 1.531237083414627e-07,
      "loss": 1.7877,
      "step": 132500
    },
    {
      "epoch": 1.9509000191336856,
      "grad_norm": 2.4944279193878174,
      "learning_rate": 1.486370635712997e-07,
      "loss": 1.7893,
      "step": 132550
    },
    {
      "epoch": 1.951635930117893,
      "grad_norm": 2.244218111038208,
      "learning_rate": 1.4421703307722078e-07,
      "loss": 1.8049,
      "step": 132600
    },
    {
      "epoch": 1.9523718411021003,
      "grad_norm": 2.5465211868286133,
      "learning_rate": 1.3986362276551812e-07,
      "loss": 1.8504,
      "step": 132650
    },
    {
      "epoch": 1.9531077520863076,
      "grad_norm": 2.516002893447876,
      "learning_rate": 1.3557683845347724e-07,
      "loss": 1.7909,
      "step": 132700
    },
    {
      "epoch": 1.953843663070515,
      "grad_norm": 2.0206682682037354,
      "learning_rate": 1.3135668586934935e-07,
      "loss": 1.7575,
      "step": 132750
    },
    {
      "epoch": 1.9545795740547223,
      "grad_norm": 2.3165173530578613,
      "learning_rate": 1.2720317065233468e-07,
      "loss": 1.8417,
      "step": 132800
    },
    {
      "epoch": 1.9553154850389296,
      "grad_norm": 2.3402676582336426,
      "learning_rate": 1.2311629835261574e-07,
      "loss": 1.7985,
      "step": 132850
    },
    {
      "epoch": 1.956051396023137,
      "grad_norm": 2.1633994579315186,
      "learning_rate": 1.1909607443129634e-07,
      "loss": 1.7969,
      "step": 132900
    },
    {
      "epoch": 1.9567873070073443,
      "grad_norm": 1.9450714588165283,
      "learning_rate": 1.1514250426043483e-07,
      "loss": 1.7897,
      "step": 132950
    },
    {
      "epoch": 1.9575232179915516,
      "grad_norm": 2.4950637817382812,
      "learning_rate": 1.1125559312302192e-07,
      "loss": 1.8145,
      "step": 133000
    },
    {
      "epoch": 1.958259128975759,
      "grad_norm": 2.5793607234954834,
      "learning_rate": 1.0743534621297513e-07,
      "loss": 1.7616,
      "step": 133050
    },
    {
      "epoch": 1.9589950399599665,
      "grad_norm": 2.150949716567993,
      "learning_rate": 1.0368176863512769e-07,
      "loss": 1.7456,
      "step": 133100
    },
    {
      "epoch": 1.9597309509441738,
      "grad_norm": 2.8708436489105225,
      "learning_rate": 9.999486540522296e-08,
      "loss": 1.84,
      "step": 133150
    },
    {
      "epoch": 1.9604668619283812,
      "grad_norm": 2.231954574584961,
      "learning_rate": 9.637464144992558e-08,
      "loss": 1.763,
      "step": 133200
    },
    {
      "epoch": 1.9612027729125885,
      "grad_norm": 2.5181808471679688,
      "learning_rate": 9.282110160677704e-08,
      "loss": 1.7507,
      "step": 133250
    },
    {
      "epoch": 1.9619386838967958,
      "grad_norm": 2.171457529067993,
      "learning_rate": 8.93342506242234e-08,
      "loss": 1.8251,
      "step": 133300
    },
    {
      "epoch": 1.9626745948810032,
      "grad_norm": 2.395413637161255,
      "learning_rate": 8.591409316160981e-08,
      "loss": 1.7889,
      "step": 133350
    },
    {
      "epoch": 1.9634105058652107,
      "grad_norm": 1.8856732845306396,
      "learning_rate": 8.256063378913048e-08,
      "loss": 1.7531,
      "step": 133400
    },
    {
      "epoch": 1.964146416849418,
      "grad_norm": 2.812222480773926,
      "learning_rate": 7.927387698788424e-08,
      "loss": 1.8444,
      "step": 133450
    },
    {
      "epoch": 1.9648823278336254,
      "grad_norm": 2.1330602169036865,
      "learning_rate": 7.605382714982456e-08,
      "loss": 1.8021,
      "step": 133500
    },
    {
      "epoch": 1.9656182388178327,
      "grad_norm": 2.512688398361206,
      "learning_rate": 7.29004885777651e-08,
      "loss": 1.7148,
      "step": 133550
    },
    {
      "epoch": 1.96635414980204,
      "grad_norm": 2.21173095703125,
      "learning_rate": 6.981386548537416e-08,
      "loss": 1.8094,
      "step": 133600
    },
    {
      "epoch": 1.9670900607862474,
      "grad_norm": 2.3395602703094482,
      "learning_rate": 6.679396199718579e-08,
      "loss": 1.8054,
      "step": 133650
    },
    {
      "epoch": 1.9678259717704547,
      "grad_norm": 2.3051347732543945,
      "learning_rate": 6.384078214856648e-08,
      "loss": 1.7795,
      "step": 133700
    },
    {
      "epoch": 1.968561882754662,
      "grad_norm": 2.5715885162353516,
      "learning_rate": 6.095432988572625e-08,
      "loss": 1.7852,
      "step": 133750
    },
    {
      "epoch": 1.9692977937388694,
      "grad_norm": 2.38639497756958,
      "learning_rate": 5.813460906570756e-08,
      "loss": 1.7458,
      "step": 133800
    },
    {
      "epoch": 1.9700337047230767,
      "grad_norm": 2.2747137546539307,
      "learning_rate": 5.538162345639086e-08,
      "loss": 1.7709,
      "step": 133850
    },
    {
      "epoch": 1.970769615707284,
      "grad_norm": 2.252756118774414,
      "learning_rate": 5.269537673646685e-08,
      "loss": 1.781,
      "step": 133900
    },
    {
      "epoch": 1.9715055266914914,
      "grad_norm": 2.1705291271209717,
      "learning_rate": 5.007587249545309e-08,
      "loss": 1.7171,
      "step": 133950
    },
    {
      "epoch": 1.9722414376756987,
      "grad_norm": 2.234229803085327,
      "learning_rate": 4.7523114233694046e-08,
      "loss": 1.7589,
      "step": 134000
    },
    {
      "epoch": 1.972977348659906,
      "grad_norm": 2.0606305599212646,
      "learning_rate": 4.503710536232219e-08,
      "loss": 1.8079,
      "step": 134050
    },
    {
      "epoch": 1.9737132596441134,
      "grad_norm": 1.9096689224243164,
      "learning_rate": 4.261784920329137e-08,
      "loss": 1.8007,
      "step": 134100
    },
    {
      "epoch": 1.9744491706283207,
      "grad_norm": 2.0468358993530273,
      "learning_rate": 4.026534898934897e-08,
      "loss": 1.7325,
      "step": 134150
    },
    {
      "epoch": 1.975185081612528,
      "grad_norm": 2.212059259414673,
      "learning_rate": 3.797960786404153e-08,
      "loss": 1.7502,
      "step": 134200
    },
    {
      "epoch": 1.9759209925967354,
      "grad_norm": 2.124953508377075,
      "learning_rate": 3.576062888170917e-08,
      "loss": 1.8264,
      "step": 134250
    },
    {
      "epoch": 1.9766569035809427,
      "grad_norm": 2.1202800273895264,
      "learning_rate": 3.360841500748557e-08,
      "loss": 1.8002,
      "step": 134300
    },
    {
      "epoch": 1.97739281456515,
      "grad_norm": 1.932835340499878,
      "learning_rate": 3.152296911727581e-08,
      "loss": 1.8323,
      "step": 134350
    },
    {
      "epoch": 1.9781287255493576,
      "grad_norm": 2.281996250152588,
      "learning_rate": 2.9504293997778542e-08,
      "loss": 1.7615,
      "step": 134400
    },
    {
      "epoch": 1.978864636533565,
      "grad_norm": 2.0878825187683105,
      "learning_rate": 2.7552392346463786e-08,
      "loss": 1.846,
      "step": 134450
    },
    {
      "epoch": 1.9796005475177723,
      "grad_norm": 2.2189104557037354,
      "learning_rate": 2.566726677157294e-08,
      "loss": 1.7508,
      "step": 134500
    },
    {
      "epoch": 1.9803364585019796,
      "grad_norm": 2.267162799835205,
      "learning_rate": 2.3848919792118784e-08,
      "loss": 1.7692,
      "step": 134550
    },
    {
      "epoch": 1.981072369486187,
      "grad_norm": 2.897104501724243,
      "learning_rate": 2.209735383788547e-08,
      "loss": 1.7863,
      "step": 134600
    },
    {
      "epoch": 1.9818082804703943,
      "grad_norm": 2.316209316253662,
      "learning_rate": 2.0412571249417422e-08,
      "loss": 1.762,
      "step": 134650
    },
    {
      "epoch": 1.9825441914546018,
      "grad_norm": 2.0917441844940186,
      "learning_rate": 1.8794574278013786e-08,
      "loss": 1.8042,
      "step": 134700
    },
    {
      "epoch": 1.9832801024388091,
      "grad_norm": 2.259781837463379,
      "learning_rate": 1.724336508573954e-08,
      "loss": 1.7673,
      "step": 134750
    },
    {
      "epoch": 1.9840160134230165,
      "grad_norm": 1.9630122184753418,
      "learning_rate": 1.5758945745408814e-08,
      "loss": 1.7254,
      "step": 134800
    },
    {
      "epoch": 1.9847519244072238,
      "grad_norm": 1.6809091567993164,
      "learning_rate": 1.4341318240590485e-08,
      "loss": 1.7196,
      "step": 134850
    },
    {
      "epoch": 1.9854878353914311,
      "grad_norm": 2.2059216499328613,
      "learning_rate": 1.299048446559703e-08,
      "loss": 1.8022,
      "step": 134900
    },
    {
      "epoch": 1.9862237463756385,
      "grad_norm": 2.2861478328704834,
      "learning_rate": 1.1706446225490109e-08,
      "loss": 1.7335,
      "step": 134950
    },
    {
      "epoch": 1.9869596573598458,
      "grad_norm": 1.962343692779541,
      "learning_rate": 1.0489205236080546e-08,
      "loss": 1.7699,
      "step": 135000
    },
    {
      "epoch": 1.9876955683440531,
      "grad_norm": 2.261910915374756,
      "learning_rate": 9.338763123917238e-09,
      "loss": 1.7459,
      "step": 135050
    },
    {
      "epoch": 1.9884314793282605,
      "grad_norm": 2.099538803100586,
      "learning_rate": 8.255121426281598e-09,
      "loss": 1.7649,
      "step": 135100
    },
    {
      "epoch": 1.9891673903124678,
      "grad_norm": 2.1602163314819336,
      "learning_rate": 7.23828159120421e-09,
      "loss": 1.7658,
      "step": 135150
    },
    {
      "epoch": 1.9899033012966751,
      "grad_norm": 2.052767276763916,
      "learning_rate": 6.288244977437075e-09,
      "loss": 1.7141,
      "step": 135200
    },
    {
      "epoch": 1.9906392122808825,
      "grad_norm": 2.095968723297119,
      "learning_rate": 5.405012854481362e-09,
      "loss": 1.7921,
      "step": 135250
    },
    {
      "epoch": 1.9913751232650898,
      "grad_norm": 1.7244510650634766,
      "learning_rate": 4.588586402565209e-09,
      "loss": 1.7868,
      "step": 135300
    },
    {
      "epoch": 1.9921110342492971,
      "grad_norm": 2.270059585571289,
      "learning_rate": 3.8389667126326186e-09,
      "loss": 1.7619,
      "step": 135350
    },
    {
      "epoch": 1.9928469452335045,
      "grad_norm": 2.1343655586242676,
      "learning_rate": 3.156154786382315e-09,
      "loss": 1.8274,
      "step": 135400
    },
    {
      "epoch": 1.9935828562177118,
      "grad_norm": 2.499480962753296,
      "learning_rate": 2.540151536217783e-09,
      "loss": 1.7744,
      "step": 135450
    },
    {
      "epoch": 1.9943187672019191,
      "grad_norm": 2.3609609603881836,
      "learning_rate": 1.9909577852861295e-09,
      "loss": 1.762,
      "step": 135500
    },
    {
      "epoch": 1.9950546781861265,
      "grad_norm": 2.229374647140503,
      "learning_rate": 1.5085742674447734e-09,
      "loss": 1.836,
      "step": 135550
    },
    {
      "epoch": 1.9957905891703338,
      "grad_norm": 2.174675941467285,
      "learning_rate": 1.0930016272836519e-09,
      "loss": 1.7602,
      "step": 135600
    },
    {
      "epoch": 1.9965265001545414,
      "grad_norm": 2.3232438564300537,
      "learning_rate": 7.442404201196684e-10,
      "loss": 1.7394,
      "step": 135650
    },
    {
      "epoch": 1.9972624111387487,
      "grad_norm": 2.2454726696014404,
      "learning_rate": 4.6229111198559107e-10,
      "loss": 1.7581,
      "step": 135700
    },
    {
      "epoch": 1.997998322122956,
      "grad_norm": 2.1486775875091553,
      "learning_rate": 2.471540796356031e-10,
      "loss": 1.7358,
      "step": 135750
    },
    {
      "epoch": 1.9987342331071634,
      "grad_norm": 1.9934806823730469,
      "learning_rate": 9.88296105508546e-11,
      "loss": 1.7599,
      "step": 135800
    },
    {
      "epoch": 1.9994701440913707,
      "grad_norm": 2.2872018814086914,
      "learning_rate": 1.7317902928359887e-11,
      "loss": 1.7888,
      "step": 135850
    }
  ],
  "logging_steps": 50,
  "max_steps": 135886,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1314564034492928e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
